{
  "question" : {
    "question_id" : 79823021,
    "title" : "How to reduce memory consumption when processing PDFs with large embedded images using PDFBox?",
    "body" : "<p>I'm using Apache PDFBox to process PDFs that contain very large, embedded images (e.g., 6538x6570px = ~163MB when decompressed). The service consumes 290-446MB of memory during conversion, causing performance issues.</p>\n<p>The P<strong>roblem:</strong></p>\n<ul>\n<li><p>PDF file size: 356KB (compressed)</p>\n</li>\n<li><p>Contains 3 embedded images: 6538x6570px (~163MB), 6538x1899px (~47MB), 2535x376px (~3MB)</p>\n</li>\n<li><p>When PDFBox processes these, it decompresses images into memory at full size before I can downscale them</p>\n</li>\n<li><p>Memory peak: 290-446MB for a single-page PDF</p>\n</li>\n</ul>\n<p>Is there a way to downscale embedded images in PDFBox without loading the full image into memory first? Or is there a better approach to handle large, embedded images?</p>\n",
    "tags" : [ "java", "pdf" ],
    "owner" : {
      "account_id" : 44716661,
      "reputation" : 1,
      "user_id" : 31898390,
      "user_type" : "registered",
      "profile_image" : "https://www.gravatar.com/avatar/e213de02aa6055a4e74b549080109dc8?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name" : "user31898390",
      "link" : "https://stackoverflow.com/users/31898390/user31898390"
    },
    "is_answered" : false,
    "view_count" : 65,
    "answer_count" : 1,
    "score" : 2,
    "last_activity_date" : 1763477836,
    "creation_date" : 1763445070,
    "link" : "https://stackoverflow.com/questions/79823021/how-to-reduce-memory-consumption-when-processing-pdfs-with-large-embedded-images",
    "content_license" : "CC BY-SA 4.0"
  },
  "answers" : [ {
    "answer_id" : 79823473,
    "question_id" : 79823021,
    "body" : "<p>PDF stores GENERALLY 2 types of image so either embedded (was a jpeg image) or converted (was something else) thus if imported (as DCT) you can dump the compressed image instantly and it retains its metadata. Otherwise, the PDF compression needs to be undone by expansion and the ginormous single memory block reconverted into any other compressed file so often needing high memory volumes OR banding in blocks. So exactly what type of image store was used internally is each image 2 parts or is it DCT compressed.</p>\n<p>The main problem is data handling the many formats and thus the easiest tool to use is run poppler  <code>pdfimages -all mydoc.pdf prefix</code> then depending on exported file type you may have simplified post processing into standard images.</p>\n",
    "score" : 0,
    "is_accepted" : false,
    "owner" : {
      "account_id" : 14313923,
      "reputation" : 12662,
      "user_id" : 10802527,
      "user_type" : "registered",
      "profile_image" : "https://www.gravatar.com/avatar/09ba828ca91501c7202a014fd0388bc4?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name" : "K J",
      "link" : "https://stackoverflow.com/users/10802527/k-j"
    },
    "creation_date" : 1763476548,
    "last_activity_date" : 1763477836,
    "content_license" : "CC BY-SA 4.0"
  } ],
  "question_comments" : [ ],
  "answer_comments" : { }
}