{
  "question" : {
    "question_id" : 79644632,
    "title" : "Drools Performance Capability",
    "body" : "<p>I was asked to help determine if Drools is capable of high volume processing.  So I in return asked what do we mean by high volume.  They gave me the following:</p>\n<hr />\n<p><em><strong>20,000 files</strong></em></p>\n<p><em><strong>5 different rulesets expected</strong></em></p>\n<p><em><strong>80 rules per set (approximated)</strong></em></p>\n<p><em><strong>500GB total mem in data</strong></em></p>\n<hr />\n<p>Drools will be running on an AWS t3.2xlarge  Machine specs are:\n<em><strong>32 GB Ram\n8 CPUs\n120 GB storage</strong></em>\nBecause of the AWS aspect the host machine will be able to <em>grow if required</em>, not currently looking to use AWS Flink because of other considerations.</p>\n<p>Basic rules are one of two types:</p>\n<p>Does this data have a name tag with a value?  Return T if true</p>\n<p>Is the name NOT one of these values:  [a, b, c, d]? Return T if true</p>\n<p>The rules are mainly on the validation side of things but based upon failed validation routing of the data changes and reporting of the failure occurs.</p>\n<p>With this information, can anyone suggest how to theoretically show Drools can or cannot possibly handle the workload?</p>\n<p>Earlier engines using RETE seemed to wilt under large datasets but could not find performance related info on Drools 8.X.X.  Full disclosure, I just started learning about Drools two days ago.</p>\n<p>FWIIW, I did look over existing articles and didn't see any that offered a format or template to follow for evaluation.</p>\n",
    "tags" : [ "java", "spring", "performance", "jboss", "drools" ],
    "owner" : {
      "account_id" : 8703300,
      "reputation" : 477,
      "user_id" : 6512334,
      "user_type" : "registered",
      "profile_image" : "https://i.sstatic.net/uZrya.jpg?s=256",
      "display_name" : "JavaJd",
      "link" : "https://stackoverflow.com/users/6512334/javajd"
    },
    "is_answered" : false,
    "view_count" : 77,
    "answer_count" : 0,
    "score" : 0,
    "last_activity_date" : 1748594458,
    "creation_date" : 1748552840,
    "link" : "https://stackoverflow.com/questions/79644632/drools-performance-capability",
    "content_license" : "CC BY-SA 4.0"
  },
  "answers" : [ ],
  "question_comments" : [ {
    "comment_id" : 140509967,
    "post_id" : 79644632,
    "body" : "Tyvm Roddy, will definitely give that a spin!!!!",
    "score" : 0,
    "owner" : {
      "account_id" : 8703300,
      "reputation" : 477,
      "user_id" : 6512334,
      "user_type" : "registered",
      "profile_image" : "https://i.sstatic.net/uZrya.jpg?s=256",
      "display_name" : "JavaJd",
      "link" : "https://stackoverflow.com/users/6512334/javajd"
    },
    "creation_date" : 1749768204,
    "content_license" : "CC BY-SA 4.0"
  }, {
    "comment_id" : 140509958,
    "post_id" : 79644632,
    "body" : "Refer to this older answer which was about performance: <a href=\"https://stackoverflow.com/questions/65621089/drools-performance-estimation/65631392#65631392\" title=\"drools performance estimation\">stackoverflow.com/questions/65621089/&hellip;</a> . It&#39;s less about rules-in-memory footprint, but the footprint of your execution (inputs, any side effects, etc).",
    "score" : 0,
    "owner" : {
      "account_id" : 183966,
      "reputation" : 15278,
      "user_id" : 419705,
      "user_type" : "registered",
      "accept_rate" : 67,
      "profile_image" : "https://i.sstatic.net/cvuY8.png?s=256",
      "display_name" : "Roddy of the Frozen Peas",
      "link" : "https://stackoverflow.com/users/419705/roddy-of-the-frozen-peas"
    },
    "creation_date" : 1749767733,
    "content_license" : "CC BY-SA 4.0"
  }, {
    "comment_id" : 140509929,
    "post_id" : 79644632,
    "body" : "thank you for your answer, our rules are going to be pretty straightforward in terms of required values, range values, enumerated values without a lot of interaction between those rules.  I was thinking of loading each of the rulesets up to just measure how much mem the actual rules occupy, then from there match the expected flow rate seen in production and monitor performance.  Is that in general a good approach given your experience?",
    "score" : 0,
    "owner" : {
      "account_id" : 8703300,
      "reputation" : 477,
      "user_id" : 6512334,
      "user_type" : "registered",
      "profile_image" : "https://i.sstatic.net/uZrya.jpg?s=256",
      "display_name" : "JavaJd",
      "link" : "https://stackoverflow.com/users/6512334/javajd"
    },
    "creation_date" : 1749766845,
    "content_license" : "CC BY-SA 4.0"
  }, {
    "comment_id" : 140500458,
    "post_id" : 79644632,
    "body" : "This isn&#39;t answerable. It depends on how your rules are designed. About 8 years ago I was maintaining just under half a million rules with sub-second SLAs for thousands of requests per minute using roughly the same hardware as you&#39;re proposing. Another team was supporting around 10,000 rules with &gt; 128Gb ram and 32 CPUs that was constantly deadlocking and falling over and sometimes took minutes to respond and could handle fewer than 100 tx per minute. It&#39;s all in how your rules are designed and how they interact between each other.",
    "score" : 0,
    "owner" : {
      "account_id" : 183966,
      "reputation" : 15278,
      "user_id" : 419705,
      "user_type" : "registered",
      "accept_rate" : 67,
      "profile_image" : "https://i.sstatic.net/cvuY8.png?s=256",
      "display_name" : "Roddy of the Frozen Peas",
      "link" : "https://stackoverflow.com/users/419705/roddy-of-the-frozen-peas"
    },
    "creation_date" : 1749501593,
    "content_license" : "CC BY-SA 4.0"
  } ],
  "answer_comments" : { }
}