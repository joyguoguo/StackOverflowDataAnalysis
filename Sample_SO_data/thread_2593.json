{
  "question" : {
    "question_id" : 79609979,
    "title" : "Spark - ReadStream Kafka-Events and delete entries in my S3 bucket based on attributes in Kafka-Event",
    "body" : "<p>I Want to listen to a Kafka-Topic. Every event on that topic represents that an entry of my S3 Bucket can be deleted.</p>\n<p>I've got the following Code to read the Events from my Topic:</p>\n<pre><code>private static Dataset&lt;Row&gt; readDataFromKafka(SparkSession sparkSession)\n    {\n    return sparkSession.readStream()\n            .format(&quot;kafka&quot;)\n            .option(&quot;kafka.bootstrap.servers&quot;,\n                &quot;&lt;SERVER&gt;&quot;)\n            .option(&quot;subscribe&quot;, &quot;&lt;TOPIC&gt;&quot;)\n            .option(&quot;&lt;FURTHER_OPTIONS&gt;&quot;, &quot;&lt;FURTHER_VALUES&gt;&quot;)\n            .load()\n   ;\n}\n</code></pre>\n<p>Reading the events seems to function like i imagined.</p>\n<p>After reading the event I map the information to my POJO. Assume its just an ID.</p>\n<pre><code>Dataset&lt;Row&gt; deltatable = readDataFromKafka(sparkSession);\nDataset&lt;ID&gt; idDataset = deltatable.as(Encoders.bean(ID.class));\n</code></pre>\n<p>The following Code Snipped is designed to remove the Events from my DeltaTable:</p>\n<pre><code>StreamingQuery deleteQuery = idDataset.writeStream()\n                                .foreach(new DeleteForeachWriter(S3BUCKET))\n                                .outputMode(&quot;append&quot;)\n                                .option(&quot;checkpointLocation&quot;, S3BUCKET + &quot;/checkpoints/delete&quot;)\n                                .start();\ndeleteQuery.awaitTermination(); \n</code></pre>\n<p>My DeleteForeachWriter looks like this:</p>\n<pre><code>public class DeleteForeachWriter extends ForeachWriter&lt;ID&gt; {\n    private DeltaTable deltaTable;\n\n    public DeleteForeachWriter(String deltaTablePath) {\n        this.deltaTable = DeltaTable.forPath(spark, deltaTablePath);\n    }\n\n    @Override\n    public boolean open(long partitionId, long version) {\n        // Open connection (if needed)\n        return true;\n    }\n\n    @Override\n    public void process(ID event) {\n        // Perform the delete operation\n        deltaTable.delete(&quot;id = '&quot; + ID.getId() + &quot;'&quot;);\n    }\n\n    @Override\n    public void close(Throwable errorOrNull) {\n        // Close connection (if needed)\n    }\n}\n</code></pre>\n<p>After Starting the Appication I get the following exception:</p>\n<blockquote>\n<p>ERROR Utils: Aborting task\norg.apache.spark.sql.delta.DeltaIllegalStateException: [DELTA_TABLE_FOUND_IN_EXECUTOR] DeltaTable cannot be used in executors\nat org.apache.spark.sql.delta.DeltaErrorsBase.deltaTableFoundInExecutor(DeltaErrors.scala:2743)\nat org.apache.spark.sql.delta.DeltaErrorsBase.deltaTableFoundInExecutor$(DeltaErrors.scala:2742)\nat org.apache.spark.sql.delta.DeltaErrors$.deltaTableFoundInExecutor(DeltaErrors.scala:3598)\nat io.delta.tables.DeltaTable.df(DeltaTable.scala:61)\nat io.delta.tables.DeltaTable.toDF(DeltaTable.scala:88)\nat io.delta.tables.execution.DeltaTableOperations.sparkSession(DeltaTableOperations.scala:169)\nat io.delta.tables.execution.DeltaTableOperations.sparkSession$(DeltaTableOperations.scala:169)\nat io.delta.tables.DeltaTable.sparkSession(DeltaTable.scala:44)\nat io.delta.tables.execution.DeltaTableOperations.$anonfun$executeDelete$1(DeltaTableOperations.scala:44)\nat org.apache.spark.sql.delta.util.AnalysisHelper.improveUnsupportedOpError(AnalysisHelper.scala:109)\nat org.apache.spark.sql.delta.util.AnalysisHelper.improveUnsupportedOpError$(AnalysisHelper.scala:95)\nat io.delta.tables.DeltaTable.improveUnsupportedOpError(DeltaTable.scala:44)\nat io.delta.tables.execution.DeltaTableOperations.executeDelete(DeltaTableOperations.scala:44)\nat io.delta.tables.execution.DeltaTableOperations.executeDelete$(DeltaTableOperations.scala:43)\nat io.delta.tables.DeltaTable.executeDelete(DeltaTable.scala:44)\nat io.delta.tables.DeltaTable.delete(DeltaTable.scala:184)\nat io.delta.tables.DeltaTable.delete(DeltaTable.scala:173)\nat com.DeleteForeachWriter.process(DeleteForeachWriter.java:27)\nat com.DeleteForeachWriter.process(DeleteForeachWriter.java:1)</p>\n</blockquote>\n<p>I understand, that it is not possible to use the deltaTable within my DeleteForeachWriter Class, but I didn't found a solution how to remove an entry in my S3 Bucket. Does anyone know how to do it?</p>\n<p>FYI: If i use a KafkaListener with Spring Boot i can use DeltaTable.delete(&quot;...&quot;) and it works which seems a bit strange to me.</p>\n",
    "tags" : [ "java", "apache-spark", "spark-structured-streaming", "spark-java" ],
    "owner" : {
      "account_id" : 24830220,
      "reputation" : 35,
      "user_id" : 18709482,
      "user_type" : "registered",
      "profile_image" : "https://lh3.googleusercontent.com/a/AATXAJwcz1TilpdBSWEiCaR2QxTj1OZHNXrIroP0ctj3=k-s256",
      "display_name" : "Kleifker",
      "link" : "https://stackoverflow.com/users/18709482/kleifker"
    },
    "is_answered" : false,
    "view_count" : 34,
    "answer_count" : 0,
    "score" : 1,
    "last_activity_date" : 1746689788,
    "creation_date" : 1746602310,
    "link" : "https://stackoverflow.com/questions/79609979/spark-readstream-kafka-events-and-delete-entries-in-my-s3-bucket-based-on-attr",
    "content_license" : "CC BY-SA 4.0"
  },
  "answers" : [ ],
  "question_comments" : [ ],
  "answer_comments" : { }
}