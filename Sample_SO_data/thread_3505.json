{
  "question" : {
    "question_id" : 79544952,
    "title" : "Getting Error while deserializing message from kafka using Avro Schema",
    "body" : "<p>I am trying to deserialize a kafka record consumed from kafka topic using the below code, even though the schema is correct we are facing the error. can you suggest what else could be wrong.</p>\n<p>Code</p>\n<pre><code>import org.apache.avro.Schema;\nimport org.apache.avro.generic.GenericRecord;\nimport org.apache.avro.specific.SpecificDatumReader;\n\npublic class AvroUtility {\n    \n  private static final org.slf4j.Logger log = org.slf4j.LoggerFactory.getLogger(Replicator.class);\n\n  public SpecificDatumReader&lt;GenericRecord&gt; datumReader() {\n    String valueSchemaString = null;\n    valueSchemaString = &quot;my schema in form of json string&quot;\n    Schema avroValueSchema = new Schema.Parser().parse(valueSchemaString);\n    SpecificDatumReader&lt;GenericRecord&gt; datumReader = new SpecificDatumReader&lt;&gt;(avroValueSchema);\n    return datumReader;\n  }\n}\n</code></pre>\n<p>We used the above AvroUtility class in below code</p>\n<pre><code>ConsumerRecord&lt;String, byte[]&gt; record\n\nBinaryDecoder binaryDecoder = DecoderFactory.get().binaryDecoder(record.value(), null);\nGenericRecord deserializedValue = datumReader.read(null, binaryDecoder);\n</code></pre>\n<p>We are getting below error</p>\n<pre><code>org.apache.avro.AvroRuntimeException: Malformed data. Length is negative: -1\n        at org.apache.avro.io.BinaryDecoder.readString(BinaryDecoder.java:308)\n        at org.apache.avro.io.ResolvingDecoder.readString(ResolvingDecoder.java:208)\n        at org.apache.avro.generic.GenericDatumReader.readString(GenericDatumReader.java:469)\n        at org.apache.avro.generic.GenericDatumReader.readString(GenericDatumReader.java:459)\n        at org.apache.avro.generic.GenericDatumReader.readWithoutConversion(GenericDatumReader.java:191)\n        at org.apache.avro.specific.SpecificDatumReader.readField(SpecificDatumReader.java:136)\n        at org.apache.avro.generic.GenericDatumReader.readRecord(GenericDatumReader.java:247)\n        at org.apache.avro.specific.SpecificDatumReader.readRecord(SpecificDatumReader.java:123)\n        at org.apache.avro.generic.GenericDatumReader.readWithoutConversion(GenericDatumReader.java:179)\n        at org.apache.avro.specific.SpecificDatumReader.readField(SpecificDatumReader.java:136)\n        at org.apache.avro.generic.GenericDatumReader.readRecord(GenericDatumReader.java:247)\n        at org.apache.avro.specific.SpecificDatumReader.readRecord(SpecificDatumReader.java:123)\n        at org.apache.avro.generic.GenericDatumReader.readWithoutConversion(GenericDatumReader.java:179)\n        at org.apache.avro.specific.SpecificDatumReader.readField(SpecificDatumReader.java:136)\n        at org.apache.avro.generic.GenericDatumReader.readRecord(GenericDatumReader.java:247)\n        at org.apache.avro.specific.SpecificDatumReader.readRecord(SpecificDatumReader.java:123)\n        at org.apache.avro.generic.GenericDatumReader.readWithoutConversion(GenericDatumReader.java:179)\n        at org.apache.avro.generic.GenericDatumReader.read(GenericDatumReader.java:160)\n        at org.apache.avro.generic.GenericDatumReader.read(GenericDatumReader.java:153)\n        at com.myapp.controller.Replicator.lambda$replicator$0(Replicator.java:116)\n        at java.lang.Iterable.forEach(Iterable.java:75)\n        at com.myapp.controller.Replicator.replicator(Replicator.java:104)\n        at com.myapp.SpringBootWithKafkaApplication.main(SpringBootWithKafkaApplication.java:21)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:498)\n        at org.springframework.boot.loader.MainMethodRunner.run(MainMethodRunner.java:48)\n        at org.springframework.boot.loader.Launcher.launch(Launcher.java:87)\n        at org.springframework.boot.loader.Launcher.launch(Launcher.java:51)\n        at org.springframework.boot.loader.PropertiesLauncher.main(PropertiesLauncher.java:597)\n</code></pre>\n",
    "tags" : [ "java", "apache-kafka", "avro" ],
    "owner" : {
      "account_id" : 29507302,
      "reputation" : 33,
      "user_id" : 22613048,
      "user_type" : "registered",
      "profile_image" : "https://lh3.googleusercontent.com/a/ACg8ocLlyHOXxunwXrl2Gnni3R50Ilh4K0g36pWI07irUXXo87M=k-s256",
      "display_name" : "Akash Rai",
      "link" : "https://stackoverflow.com/users/22613048/akash-rai"
    },
    "is_answered" : true,
    "view_count" : 128,
    "answer_count" : 1,
    "score" : 2,
    "last_activity_date" : 1743480334,
    "creation_date" : 1743356096,
    "link" : "https://stackoverflow.com/questions/79544952/getting-error-while-deserializing-message-from-kafka-using-avro-schema",
    "content_license" : "CC BY-SA 4.0"
  },
  "answers" : [ {
    "answer_id" : 79545954,
    "question_id" : 79544952,
    "body" : "<p>Here you are forgetting one thing:</p>\n<pre><code>ConsumerRecord&lt;String, byte[]&gt; record;\n\nBinaryDecoder binaryDecoder = DecoderFactory.get().binaryDecoder(record.value(), null);\nGenericRecord deserializedValue = datumReader.read(null, binaryDecoder);\n</code></pre>\n<p>The binary payload that is passing into <code>binaryDecoder</code> includes extra bytes :</p>\n<p><code>[Magic][4-byte_schemaID][Avro bytes]</code></p>\n<p>The <code>datumReader</code> doesn't expect this, thou causing the failure. This would work if you were using raw Avro, but as you said you serialize with the <code>KafkaAvroSerializer</code></p>\n<p>So, in order to read properly the payload, you could:</p>\n<pre><code>byte[] kafkaPayload = record.value();\n\nint schemaRegistryHeaderLength = 5;  //header bytes\nbyte[] avroData = Arrays.copyOfRange(kafkaPayload, schemaRegistryHeaderLength, \nkafkaPayload.length);\n\nBinaryDecoder binaryDecoder = DecoderFactory.get().binaryDecoder(avroData, null);\nGenericRecord deserializedValue = datumReader.read(null, binaryDecoder);\n</code></pre>\n<hr />\n<p>If using Confluent, you could also use the <code>AvroDeserializer</code>:</p>\n<pre><code>import io.confluent.kafka.serializers.KafkaAvroDeserializer;\nimport io.confluent.kafka.serializers.AbstractKafkaSchemaSerDeConfig;\n\nKafkaAvroDeserializer deserializer = new KafkaAvroDeserializer();\nProperties props = new Properties();\nprops.put(AbstractKafkaSchemaSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG, &quot;yourRegistry:port&quot;);\n\ndeserializer.configure(props, false);\n\nGenericRecord deserializedRecord = (GenericRecord) deserializer.deserialize(&quot;topic&quot;, record.value());\n</code></pre>\n",
    "score" : 1,
    "is_accepted" : true,
    "owner" : {
      "account_id" : 2465829,
      "reputation" : 13577,
      "user_id" : 2148953,
      "user_type" : "registered",
      "accept_rate" : 96,
      "profile_image" : "https://i.sstatic.net/DVHoP84E.jpg?s=256",
      "display_name" : "aran",
      "link" : "https://stackoverflow.com/users/2148953/aran"
    },
    "creation_date" : 1743414685,
    "last_activity_date" : 1743415003,
    "content_license" : "CC BY-SA 4.0"
  } ],
  "question_comments" : [ {
    "comment_id" : 140280775,
    "post_id" : 79544952,
    "body" : "Went through producer code and found that kafka producer has been configured with  (&quot;value.serializer&quot;, KafkaAvroSerializer.class.getName());  and our consumer is using ByteArrayDeserializer  Could be issue right?  If we use KafkaAvroDeserializer.class.getName(), can i expect it to be in json?",
    "score" : 0,
    "owner" : {
      "account_id" : 29507302,
      "reputation" : 33,
      "user_id" : 22613048,
      "user_type" : "registered",
      "profile_image" : "https://lh3.googleusercontent.com/a/ACg8ocLlyHOXxunwXrl2Gnni3R50Ilh4K0g36pWI07irUXXo87M=k-s256",
      "display_name" : "Akash Rai",
      "link" : "https://stackoverflow.com/users/22613048/akash-rai"
    },
    "creation_date" : 1743387782,
    "content_license" : "CC BY-SA 4.0"
  } ],
  "answer_comments" : { }
}