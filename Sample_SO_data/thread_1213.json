{
  "question" : {
    "question_id" : 79730727,
    "title" : "Prometheus/Grafana: increase() spikes during Kubernetes deploys with multi-pod counters (dynamic labels)",
    "body" : "<h2>Context</h2>\n<ul>\n<li>Java service in <strong>Kubernetes</strong>, multiple pods.</li>\n<li>Metrics exposed via <strong>Micrometer + Prometheus</strong>.</li>\n<li>Grafana dashboards use <code>increase(...)</code> and <code>sum by (...)</code> to count events in a time range.</li>\n<li>Counters are <strong>incremented directly in code</strong> when certain business events happen.</li>\n</ul>\n<p>I’m having an issue with Prometheus metrics aggregation that has gone through two different phases:</p>\n<h3><strong>Phase 1 – The initial problem (undercounting with dynamic labels)</strong></h3>\n<p>We have counters emitted from a Java service (Micrometer → Prometheus) that look roughly like this:</p>\n<pre class=\"lang-java prettyprint-override\"><code>@Component\npublic class ExampleMetrics {\n\n    private static final String METRIC_SUCCESS = &quot;app_successful_operations&quot;;\n    private static final String TAG_CATEGORY = &quot;category&quot;;\n    private static final String TAG_REGION = &quot;region&quot;;\n\n    private final MeterRegistry registry;\n\n    public ExampleMetrics(MeterRegistry registry) {\n        this.registry = registry;\n    }\n\n    public void incrementSuccess(String category, String region) {\n        registry.counter(METRIC_SUCCESS,\n                TAG_CATEGORY, category,\n                TAG_REGION, region\n        ).increment();\n    }\n}\n</code></pre>\n<p>The raw metric has <strong>extra dynamic labels</strong> added automatically by Micrometer / Prometheus client (e.g. <code>instance</code>, <code>service</code>, <code>pod</code>, <code>app_version</code>).</p>\n<p>Initially, in Grafana we had queries like:</p>\n<pre class=\"lang-none prettyprint-override\"><code>sum by (category, region) (\n  increase(app_successful_operations{category=~&quot;$category&quot;, region=~&quot;$region&quot;}[$__rate_interval])\n)\n</code></pre>\n<p>The problem: if a new time series appears halfway through the range (e.g. because a pod restarted and <code>instance</code> label changed), that series is ignored by <code>increase()</code>, so totals are undercounted.</p>\n<h3><strong>Phase 2 – Attempted solution (recording rule with <code>sum</code>)</strong></h3>\n<p>To solve that, we created recording rules that pre-aggregate <strong>removing the dynamic labels</strong>, so Grafana queries would see stable series:</p>\n<pre class=\"lang-yaml prettyprint-override\"><code>- record: app_successful_operations_normalized_total\n  expr: |\n    sum by (category, region) (\n      app_successful_operations\n    )\n</code></pre>\n<p>Grafana then uses:</p>\n<pre class=\"lang-none prettyprint-override\"><code>sum(increase(app_successful_operations_normalized_total{category=~&quot;$category&quot;, region=~&quot;$region&quot;}[$__rate_interval]))\n</code></pre>\n<p>This solved the undercounting problem — new pod series don’t cause data loss anymore.</p>\n<hr />\n<h3><strong>New problem – Spikes after deploys</strong></h3>\n<p>After adding the recording rules, during a deploy in Kubernetes we see <strong>huge spikes</strong> in Grafana at the exact time of pod restarts.\nExample: a counter might jump by millions temporarily, then normalize.</p>\n<p>From what I can tell, this happens because when pods restart, Prometheus sees a reset in the raw counters, and since the recording rule is summing raw values, the aggregated series also resets. Then <code>increase()</code> interprets the reset as a huge jump.</p>\n<hr />\n<h3><strong>Question</strong></h3>\n<p>Is there a Prometheus-safe way to:</p>\n<ol>\n<li>Aggregate counters <strong>removing volatile labels</strong> (so new pod instances don’t cause missing data in <code>increase()</code>),</li>\n<li>But also avoid <strong>spikes on deploys</strong> when the counter resets?</li>\n</ol>\n<p>I’ve thought about:</p>\n<ul>\n<li>Moving the <code>increase()</code> <strong>inside</strong> the recording rule with a fixed interval (e.g. <code>[5m]</code>), but that fixes the time window and Grafana can’t choose arbitrary ranges.</li>\n<li>Using <code>rate()</code> instead of <code>increase()</code>, but that still suffers if a counter starts mid-range.</li>\n<li>Some combination of <code>ignoring()</code> / <code>unless</code> to filter new pods, but not sure it solves both problems.</li>\n</ul>\n<p>Any best practices for this pattern?</p>\n",
    "tags" : [ "java", "prometheus", "grafana", "promql", "micrometer" ],
    "owner" : {
      "account_id" : 3581672,
      "reputation" : 177,
      "user_id" : 2989745,
      "user_type" : "registered",
      "accept_rate" : 0,
      "profile_image" : "https://www.gravatar.com/avatar/0df99e4021438a94e1550142f52e1736?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name" : "user2989745",
      "link" : "https://stackoverflow.com/users/2989745/user2989745"
    },
    "is_answered" : true,
    "view_count" : 122,
    "answer_count" : 1,
    "score" : 3,
    "last_activity_date" : 1755247564,
    "creation_date" : 1754755648,
    "link" : "https://stackoverflow.com/questions/79730727/prometheus-grafana-increase-spikes-during-kubernetes-deploys-with-multi-pod-c",
    "content_license" : "CC BY-SA 4.0"
  },
  "answers" : [ {
    "answer_id" : 79733342,
    "question_id" : 79730727,
    "body" : "<p>First of all, using <code>increase()</code> on a <code>sum()</code> is not good because any  break of monotonicity  will be interpreted as a reset of value; that is what you observed with the spikes after deploy.</p>\n<p>Your specific &quot;counting&quot; use case is discussed in <a href=\"https://github.com/prometheus/prometheus/issues/1673\" rel=\"nofollow noreferrer\">issue #1673</a>. The best solution IMHO is to sum the increases as you initially described and adding the newly created counters.</p>\n<pre><code>sum by (category, region) (\n  increase(app_successful_operations{category=~&quot;$category&quot;, region=~&quot;$region&quot;}[$__rate_interval])\n  or\n  ((app_successful_operations{category=~&quot;$category&quot;, region=~&quot;$region&quot;} != 0 unless app_successful_operations{category=~&quot;$category&quot;, region=~&quot;$region&quot;} offset $__rate_interval))\n)\n</code></pre>\n<p>Notice the second part where new counters are detected with <code>metric != 0 unless metric offset 1m</code>.</p>\n",
    "score" : 1,
    "is_accepted" : false,
    "owner" : {
      "account_id" : 4254856,
      "reputation" : 7103,
      "user_id" : 3480808,
      "user_type" : "registered",
      "profile_image" : "https://www.gravatar.com/avatar/bbca64f49de22131b4b51fddfa8a7df4?s=256&d=identicon&r=PG",
      "display_name" : "Michael Doubez",
      "link" : "https://stackoverflow.com/users/3480808/michael-doubez"
    },
    "creation_date" : 1755009922,
    "last_activity_date" : 1755247564,
    "content_license" : "CC BY-SA 4.0"
  } ],
  "question_comments" : [ ],
  "answer_comments" : { }
}