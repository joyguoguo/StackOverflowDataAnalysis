{
  "question" : {
    "question_id" : 79561676,
    "title" : "How can I retrieve an object from an S3 bucket by its metadata value(s)?",
    "body" : "<p>I'm trying to check for duplicates in my S3 bucket whilst processing a file in my Java application. I can calculate the SHA-256 checksum and I'll know the file size of my current file. I can also use the prefix of the filename to denote a file type. How can I use these values to look for a duplicate within the objects in my S3 bucket?</p>\n",
    "tags" : [ "java", "amazon-web-services", "amazon-s3" ],
    "owner" : {
      "account_id" : 20919589,
      "reputation" : 19,
      "user_id" : 15368529,
      "user_type" : "registered",
      "profile_image" : "https://www.gravatar.com/avatar/079b09d8ac96f8101874f48e92d6ad28?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name" : "avocet",
      "link" : "https://stackoverflow.com/users/15368529/avocet"
    },
    "is_answered" : false,
    "view_count" : 103,
    "answer_count" : 1,
    "score" : 1,
    "last_activity_date" : 1747224204,
    "creation_date" : 1744104969,
    "link" : "https://stackoverflow.com/questions/79561676/how-can-i-retrieve-an-object-from-an-s3-bucket-by-its-metadata-values",
    "content_license" : "CC BY-SA 4.0"
  },
  "answers" : [ {
    "answer_id" : 79618048,
    "question_id" : 79561676,
    "body" : "<p>AWS has recently released <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/metadata-tables-overview.html\" rel=\"nofollow noreferrer\">S3 Metadata</a>, built-on S3 Tables, that allows you to query on object's metadata. Using <code>e_tag</code>, which is the hash of the object, you can find duplicate files, this is usually MD5 but can change depending on how the object <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/API/API_Object.html#AmazonS3-Type-Object-ETag\" rel=\"nofollow noreferrer\">was uploaded</a>. Assuming each object's duplicate is created and encrypted in the same way as the original the following method will work.</p>\n<p>If you <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/metadata-tables-create-configuration.html\" rel=\"nofollow noreferrer\">enable S3 Metadata</a> on your selected bucket, you can then query the underlying S3 Table for duplicate hashes. You can do this through Amazon Athena or any other library or tool that works with Apache Iceberg tables. You will need to <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-tables-getting-started.html#s3-tables-tutorial-create-table\" rel=\"nofollow noreferrer\">setup permissions</a> for the IAM role that will query the table, through LakeFormation.</p>\n<p>If you plan to use Athena you can test it out with the following query, replacing <code>&lt;table-bucket-name&gt;</code> with the S3 Table you associated with your bucket metadata.:</p>\n<pre class=\"lang-sql prettyprint-override\"><code>-- List duplicate objects\nSELECT key, e_tag\nFROM &quot;aws_s3_metadata&quot;.&quot;&lt;table-bucket-name&gt;&quot;\nWHERE e_tag\nIN(\n    -- List all duplicates \n    SELECT e_tag\n    FROM (\n        -- Get all created objects and remove deleted ones through left exclusive join\n        SELECT all.e_tag, all.key\n        FROM &quot;aws_s3_metadata&quot;.&quot;&lt;table-bucket-name&gt;&quot; all\n        LEFT OUTER JOIN (\n            -- Get all deleted objects \n            SELECT e_tag, key \n            FROM &quot;aws_s3_metadata&quot;.&quot;&lt;table-bucket-name&gt;&quot;\n            WHERE record_type = 'DELETE'\n            GROUP BY e_tag, key \n        ) deleted ON deleted.key = all.key\n        WHERE all.record_type = 'CREATE' AND deleted.key IS NULL\n        GROUP BY all.e_tag, all.key\n    )\n    GROUP BY e_tag HAVING COUNT(e_tag) &gt; 1)\n;\n</code></pre>\n<p>By using the AWS SDK for Java you can then <a href=\"https://docs.aws.amazon.com/athena/latest/ug/start-query-execution.html\" rel=\"nofollow noreferrer\">run the query execution</a>. Here's an example of what that would look like with placeholder values.</p>\n<pre class=\"lang-java prettyprint-override\"><code>\n            // The QueryExecutionContext allows us to set the database.\n            QueryExecutionContext queryExecutionContext = QueryExecutionContext.builder()\n                    .database(&quot;aws_s3_metadata&quot;)\n                    .catalog(&quot;s3tablescatalog/&lt;table-bucket&gt;&quot;)\n                    .build();\n\n            // The result configuration specifies where the results of the query should go.\n            ResultConfiguration resultConfiguration = ResultConfiguration.builder()\n                    .outputLocation(&quot;&lt;s3-output-bucket&quot;)\n                    .build();\n\n            StartQueryExecutionRequest startQueryExecutionRequest = StartQueryExecutionRequest.builder()\n                    .queryString(&quot;&lt;sql-query&gt;&quot;)\n                    .queryExecutionContext(queryExecutionContext)\n                    .resultConfiguration(resultConfiguration)\n                    .build();\n\n            StartQueryExecutionResponse startQueryExecutionResponse = athenaClient\n                    .startQueryExecution(startQueryExecutionRequest);\n\n            executionID  = startQueryExecutionResponse.queryExecutionId();\n</code></pre>\n<p>After the execution is complete you can parse through the results.</p>\n<pre class=\"lang-java prettyprint-override\"><code>\n        GetQueryResultsRequest getQueryResultsRequest = GetQueryResultsRequest.builder()\n                .queryExecutionId(executionID)\n                .build();\n\n        GetQueryResultsIterable getQueryResultsResults = athenaClient\n                .getQueryResultsPaginator(getQueryResultsRequest);\n        for (GetQueryResultsResponse result : getQueryResultsResults) {\n            // TODO: parse results\n        }\n</code></pre>\n<p>You can see a full example of how to use Athena with the SDK for Java in <a href=\"https://docs.aws.amazon.com/athena/latest/ug/start-query-execution.html\" rel=\"nofollow noreferrer\">the AWS documentation</a>.</p>\n",
    "score" : 0,
    "is_accepted" : false,
    "owner" : {
      "account_id" : 8452211,
      "reputation" : 961,
      "user_id" : 6340707,
      "user_type" : "registered",
      "profile_image" : "https://i.sstatic.net/sOLxK.jpg?s=256",
      "display_name" : "PeskyPotato",
      "link" : "https://stackoverflow.com/users/6340707/peskypotato"
    },
    "creation_date" : 1747059815,
    "last_activity_date" : 1747224204,
    "content_license" : "CC BY-SA 4.0"
  } ],
  "question_comments" : [ {
    "comment_id" : 140311628,
    "post_id" : 79561676,
    "body" : "See <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/metadata-tables-overview.html\" rel=\"nofollow noreferrer\">Accelerating data discovery with S3 Metadata</a>.",
    "score" : 2,
    "owner" : {
      "account_id" : 100740,
      "reputation" : 79837,
      "user_id" : 271415,
      "user_type" : "registered",
      "accept_rate" : 91,
      "profile_image" : "https://www.gravatar.com/avatar/8648256cc53b261c5e1d266380a256fc?s=256&d=identicon&r=PG",
      "display_name" : "jarmod",
      "link" : "https://stackoverflow.com/users/271415/jarmod"
    },
    "creation_date" : 1744114175,
    "content_license" : "CC BY-SA 4.0"
  }, {
    "comment_id" : 140311418,
    "post_id" : 79561676,
    "body" : "Not the case, no.",
    "score" : 0,
    "owner" : {
      "account_id" : 2843847,
      "reputation" : 57516,
      "user_id" : 2442804,
      "user_type" : "registered",
      "accept_rate" : 80,
      "profile_image" : "https://i.sstatic.net/51a3aQOH.jpg?s=256",
      "display_name" : "luk2302",
      "link" : "https://stackoverflow.com/users/2442804/luk2302"
    },
    "creation_date" : 1744109797,
    "content_license" : "CC BY-SA 4.0"
  }, {
    "comment_id" : 140311383,
    "post_id" : 79561676,
    "body" : "Thanks for the reply @luk2302. I was under the impression that you could run queries across the metadata of objects stored in S3. Is that not the case?",
    "score" : 0,
    "owner" : {
      "account_id" : 20919589,
      "reputation" : 19,
      "user_id" : 15368529,
      "user_type" : "registered",
      "profile_image" : "https://www.gravatar.com/avatar/079b09d8ac96f8101874f48e92d6ad28?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name" : "avocet",
      "link" : "https://stackoverflow.com/users/15368529/avocet"
    },
    "creation_date" : 1744109223,
    "content_license" : "CC BY-SA 4.0"
  }, {
    "comment_id" : 140311227,
    "post_id" : 79561676,
    "body" : "The only way to access a file is by its key. If you do not have a key you need to come up with a way to derive the key, e.g. make the key a result of the file sha, or have an external mapping table or file or list a subset of keys / all keys whenever you want to look for a file. In this case you should probably just store the sha of your files in a DynamoDB and check if the sha is already there when putting a new file.",
    "score" : 2,
    "owner" : {
      "account_id" : 2843847,
      "reputation" : 57516,
      "user_id" : 2442804,
      "user_type" : "registered",
      "accept_rate" : 80,
      "profile_image" : "https://i.sstatic.net/51a3aQOH.jpg?s=256",
      "display_name" : "luk2302",
      "link" : "https://stackoverflow.com/users/2442804/luk2302"
    },
    "creation_date" : 1744106352,
    "content_license" : "CC BY-SA 4.0"
  } ],
  "answer_comments" : { }
}