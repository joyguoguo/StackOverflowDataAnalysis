{
  "question" : {
    "question_id" : 79580037,
    "title" : "Error (Py4JJavaError) running pyspark notebook in VSC",
    "body" : "<p>as the title says, I am having trouble running a code in VSC with miniforge, a pyspark notebook. What I currently have installed is:</p>\n<ul>\n<li>VSC</li>\n<li>Java 8 + Java SDK11</li>\n<li>Downloaded into c:/spark spark 3.4.4, and created folder c:/hadoop/bin where I added a winutil and hadoop dll file</li>\n<li>Python 3.11.0</li>\n<li>Latest version of miniforge</li>\n</ul>\n<p>The code I am trying to build is:</p>\n<pre><code>import sys\nimport requests\nimport json\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import *\nfrom datetime import datetime, timedelta\nfrom pyspark.sql import DataFrame\nimport urllib3\nurllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\nspark = SparkSession.builder.appName(&quot;SAP&quot;).getOrCreate()\n\ndef get_data_sap(base_url, login_payload, endpoint):\n    # code here that is querying SAP ServiceLayer, it works on AWSGlue and GCollab\n\nfrom_date = &quot;20240101&quot;\ntoday = &quot;20240105&quot;\nskip = 0\n\nendpoint = ( f&quot;sap(P_FROM_DATE='{from_date}',P_TO_DATE='{today}')&quot;\n    f&quot;/sapview?$skip={skip}&quot;\n)\nbase_url = &quot;URL&quot;\nlogin_payload = {\n    &quot;CompanyDB&quot;: &quot;db&quot;,\n    &quot;UserName&quot;: &quot;usr&quot;,\n    &quot;Password&quot;: &quot;pwd&quot;\n}\n\ndf = get_data_sap(base_url, login_payload, endpoint)\n\ndf.filter(col('doc_entry')==8253).orderBy(col('line_num'),ascending=True).show(30,False)\n</code></pre>\n<p>Each section of the previous code is a cell in a ipynib notebook I am running, and they work, but when I get to the last line (df.filter), or I try anything else such as df.head() or df.show, I get an error. The following is the error I have:</p>\n<pre><code>---------------------------------------------------------------------------\nPy4JJavaError                             Traceback (most recent call last)\nCell In[10], line 1\n----&gt; 1 df.filter(col('doc_entry')==8253).orderBy(col('line_num'),ascending=True).show(30,False)\n\nFile c:\\ProgramData\\miniforge3\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:947, in DataFrame.show(self, n, truncate, vertical)\n    887 def show(self, n: int = 20, truncate: Union[bool, int] = True, vertical: bool = False) -&gt; None:\n    888     &quot;&quot;&quot;Prints the first ``n`` rows to the console.\n    889 \n    890     .. versionadded:: 1.3.0\n   (...)    945     name | Bob\n    946     &quot;&quot;&quot;\n--&gt; 947     print(self._show_string(n, truncate, vertical))\n\nFile c:\\ProgramData\\miniforge3\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:978, in DataFrame._show_string(self, n, truncate, vertical)\n    969 except ValueError:\n    970     raise PySparkTypeError(\n    971         error_class=&quot;NOT_BOOL&quot;,\n    972         message_parameters={\n   (...)    975         },\n    976     )\n--&gt; 978 return self._jdf.showString(n, int_truncate, vertical)\n\nFile c:\\ProgramData\\miniforge3\\Lib\\site-packages\\py4j\\java_gateway.py:1322, in JavaMember.__call__(self, *args)\n   1316 command = proto.CALL_COMMAND_NAME +\\\n   1317     self.command_header +\\\n   1318     args_command +\\\n   1319     proto.END_COMMAND_PART\n   1321 answer = self.gateway_client.send_command(command)\n-&gt; 1322 return_value = get_return_value(\n   1323     answer, self.gateway_client, self.target_id, self.name)\n   1325 for temp_arg in temp_args:\n   1326     if hasattr(temp_arg, &quot;_detach&quot;):\n\nFile c:\\ProgramData\\miniforge3\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179, in capture_sql_exception.&lt;locals&gt;.deco(*a, **kw)\n    177 def deco(*a: Any, **kw: Any) -&gt; Any:\n    178     try:\n--&gt; 179         return f(*a, **kw)\n    180     except Py4JJavaError as e:\n    181         converted = convert_exception(e.java_exception)\n\nFile c:\\ProgramData\\miniforge3\\Lib\\site-packages\\py4j\\protocol.py:326, in get_return_value(answer, gateway_client, target_id, name)\n    324 value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)\n    325 if answer[1] == REFERENCE_TYPE:\n--&gt; 326     raise Py4JJavaError(\n    327         &quot;An error occurred while calling {0}{1}{2}.\\n&quot;.\n    328         format(target_id, &quot;.&quot;, name), value)\n    329 else:\n    330     raise Py4JError(\n    331         &quot;An error occurred while calling {0}{1}{2}. Trace:\\n{3}\\n&quot;.\n    332         format(target_id, &quot;.&quot;, name, value))\n\nPy4JJavaError: An error occurred while calling o130.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 0.0 failed 1 times, most recent failure: Lost task 8.0 in stage 0.0 (TID 8) (NFCLBI01 executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\n    at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:192)\n    at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\n    at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\n    at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:166)\n    at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n    at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n    at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n    at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n    at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n    at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n    at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n    at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n    at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n    at org.apache.spark.scheduler.Task.run(Task.scala:139)\n    at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:834)\nCaused by: java.net.SocketTimeoutException: Accept timed out\n    at java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)\n    at java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)\n    at java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)\n    at java.base/java.net.ServerSocket.implAccept(ServerSocket.java:551)\n    at java.base/java.net.ServerSocket.accept(ServerSocket.java:519)\n    at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:179)\n    ... 33 more\n\nDriver stacktrace:\n    at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2790)\n    at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2726)\n    at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2725)\n    at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n    at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n    at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n    at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2725)\n    at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1211)\n    at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1211)\n    at scala.Option.foreach(Option.scala:407)\n    at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1211)\n    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2989)\n    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2928)\n    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2917)\n    at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n    at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:976)\n    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2258)\n    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2353)\n    at org.apache.spark.rdd.RDD.$anonfun$reduce$1(RDD.scala:1112)\n    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n    at org.apache.spark.rdd.RDD.withScope(RDD.scala:408)\n    at org.apache.spark.rdd.RDD.reduce(RDD.scala:1094)\n    at org.apache.spark.rdd.RDD.$anonfun$takeOrdered$1(RDD.scala:1541)\n    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n    at org.apache.spark.rdd.RDD.withScope(RDD.scala:408)\n    at org.apache.spark.rdd.RDD.takeOrdered(RDD.scala:1528)\n    at org.apache.spark.sql.execution.TakeOrderedAndProjectExec.executeCollect(limit.scala:291)\n    at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4218)\n    at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3202)\n    at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4208)\n    at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)\n    at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4206)\n    at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\n    at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n    at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n    at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n    at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n    at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4206)\n    at org.apache.spark.sql.Dataset.head(Dataset.scala:3202)\n    at org.apache.spark.sql.Dataset.take(Dataset.scala:3423)\n    at org.apache.spark.sql.Dataset.getRows(Dataset.scala:283)\n    at org.apache.spark.sql.Dataset.showString(Dataset.scala:322)\n    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n    at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.base/java.lang.reflect.Method.invoke(Method.java:566)\n    at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n    at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n    at py4j.Gateway.invoke(Gateway.java:282)\n    at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n    at py4j.commands.CallCommand.execute(CallCommand.java:79)\n    at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n    at py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n    at java.base/java.lang.Thread.run(Thread.java:834)\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\n    at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:192)\n    at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\n    at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\n    at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:166)\n    at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n    at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n    at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n    at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n    at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n    at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n    at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n    at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n    at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n    at org.apache.spark.scheduler.Task.run(Task.scala:139)\n    at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    ... 1 more\nCaused by: java.net.SocketTimeoutException: Accept timed out\n    at java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)\n    at java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)\n    at java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)\n    at java.base/java.net.ServerSocket.implAccept(ServerSocket.java:551)\n    at java.base/java.net.ServerSocket.accept(ServerSocket.java:519)\n    at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:179)\n    ... 33 more\n</code></pre>\n<p>What can I do to fix this error? Thanks!</p>\n",
    "tags" : [ "python", "java", "visual-studio-code", "pyspark", "jupyter-notebook" ],
    "owner" : {
      "account_id" : 15669236,
      "reputation" : 37,
      "user_id" : 11306228,
      "user_type" : "registered",
      "profile_image" : "https://www.gravatar.com/avatar/44b6d2d8b6a95ad3dbf80e2abdd785bf?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name" : "lecarusin",
      "link" : "https://stackoverflow.com/users/11306228/lecarusin"
    },
    "is_answered" : false,
    "view_count" : 51,
    "answer_count" : 1,
    "score" : 0,
    "last_activity_date" : 1744923474,
    "creation_date" : 1744921607,
    "link" : "https://stackoverflow.com/questions/79580037/error-py4jjavaerror-running-pyspark-notebook-in-vsc",
    "content_license" : "CC BY-SA 4.0"
  },
  "answers" : [ {
    "answer_id" : 79580078,
    "question_id" : 79580037,
    "body" : "<p>You're getting a Py4JJavaError. In this case, it might have to do with the size of the dataframe you're working with, as there is only a limited amount of memory assigned to the spark session by default.</p>\n<p>The solution would be to manually configure the driver memory and the result size. You currently have:</p>\n<pre><code>spark = SparkSession.builder.appName(&quot;SAP&quot;).getOrCreate()\n</code></pre>\n<p>You should do this instead:</p>\n<pre><code>spark = SparkSession.builder.appName(&quot;SAP&quot;).\\\n                                config(&quot;spark.driver.memory&quot;, &quot;4g&quot;).\\ # or 8g\n                                config(&quot;spark.driver.maxResultSize&quot;, &quot;4g&quot;).\\ # or 8g\n                                getOrCreate()\n</code></pre>\n<p>You could also include:</p>\n<pre><code>                                config(&quot;spark.executor.memory&quot;, &quot;4g&quot;).\\ # or 8g\n</code></pre>\n",
    "score" : 0,
    "is_accepted" : false,
    "owner" : {
      "account_id" : 22884708,
      "reputation" : 9,
      "user_id" : 17025772,
      "user_type" : "registered",
      "profile_image" : "https://graph.facebook.com/851678852211539/picture?type=large",
      "display_name" : "Agbeleye Victor Olalekan",
      "link" : "https://stackoverflow.com/users/17025772/agbeleye-victor-olalekan"
    },
    "creation_date" : 1744923474,
    "last_activity_date" : 1744923474,
    "content_license" : "CC BY-SA 4.0"
  } ],
  "question_comments" : [ ],
  "answer_comments" : {
    "79580078" : [ {
      "comment_id" : 140346337,
      "post_id" : 79580078,
      "body" : "I tried adding <code>memory</code>, <code>maxResultSize</code> and <code>executor.memory</code> with 8g, but I still keep getting the same error. For considerations, the dataset for those 5 days is about 36 columns and 6500 lines",
      "score" : 0,
      "owner" : {
        "account_id" : 15669236,
        "reputation" : 37,
        "user_id" : 11306228,
        "user_type" : "registered",
        "profile_image" : "https://www.gravatar.com/avatar/44b6d2d8b6a95ad3dbf80e2abdd785bf?s=256&d=identicon&r=PG&f=y&so-version=2",
        "display_name" : "lecarusin",
        "link" : "https://stackoverflow.com/users/11306228/lecarusin"
      },
      "creation_date" : 1744925459,
      "content_license" : "CC BY-SA 4.0"
    } ]
  }
}