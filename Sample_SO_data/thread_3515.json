{
  "question" : {
    "question_id" : 79544089,
    "title" : "InaccessibleObjectException when using Spark with Java 21 and Scala 2.11.12",
    "body" : "<p>I'm trying to run Apache Spark using Scala 2.11.12 and Java 21.0.6, but I keep running into an error related to accessing internal fields in the java.util.ArrayList class. Specifically, when I try to load a text file using Spark, I get the following error:</p>\n<pre><code>    25/03/29 22:00:19 WARN BlockManager: Putting block broadcast_0 failed due to exception java.lang.reflect.InaccessibleObjectException: Unable to make field transient java.lang.Object[] java.util.ArrayList.elementData accessible: module java.base does not &quot;opens java.util&quot; to unnamed module @2462cb01.\n25/03/29 22:00:19 WARN BlockManager: Block broadcast_0 could not be removed as it was not found on disk or in memory\njava.lang.reflect.InaccessibleObjectException: Unable to make field transient java.lang.Object[] java.util.ArrayList.elementData accessible: module java.base does not &quot;opens java.util&quot; to unnamed module @2462cb01\n  at java.base/java.lang.reflect.AccessibleObject.throwInaccessibleObjectException(AccessibleObject.java:391)\n  at java.base/java.lang.reflect.AccessibleObject.checkCanSetAccessible(AccessibleObject.java:367)\n  at java.base/java.lang.reflect.AccessibleObject.checkCanSetAccessible(AccessibleObject.java:315)\n  at java.base/java.lang.reflect.Field.checkCanSetAccessible(Field.java:183)\n  at java.base/java.lang.reflect.Field.setAccessible(Field.java:177)\n  at org.apache.spark.util.SizeEstimator$$anonfun$getClassInfo$3.apply(SizeEstimator.scala:336)\n  at org.apache.spark.util.SizeEstimator$$anonfun$getClassInfo$3.apply(SizeEstimator.scala:330)\n  at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n  at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n  at org.apache.spark.util.SizeEstimator$.getClassInfo(SizeEstimator.scala:330)\n  at org.apache.spark.util.SizeEstimator$.visitSingleObject(SizeEstimator.scala:222)\n  at org.apache.spark.util.SizeEstimator$.org$apache$spark$util$SizeEstimator$$estimate(SizeEstimator.scala:201)\n  at org.apache.spark.util.SizeEstimator$.estimate(SizeEstimator.scala:69)\n  at org.apache.spark.util.collection.SizeTracker$class.takeSample(SizeTracker.scala:78)\n  at org.apache.spark.util.collection.SizeTracker$class.afterUpdate(SizeTracker.scala:70)\n  at org.apache.spark.util.collection.SizeTrackingVector.$plus$eq(SizeTrackingVector.scala:31)\n  at org.apache.spark.storage.memory.DeserializedValuesHolder.storeValue(MemoryStore.scala:665)\n  at org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:222)\n  at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n  at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1165)\n  at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)\n  at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)\n  at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)\n  at org.apache.spark.storage.BlockManager.putIterator(BlockManager.scala:914)\n  at org.apache.spark.storage.BlockManager.putSingle(BlockManager.scala:1481)\n  at org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:123)\n  at org.apache.spark.broadcast.TorrentBroadcast.&lt;init&gt;(TorrentBroadcast.scala:88)\n  at org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:34)\n  at org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:62)\n  at org.apache.spark.SparkContext.broadcast(SparkContext.scala:1489)\n  at org.apache.spark.SparkContext$$anonfun$hadoopFile$1.apply(SparkContext.scala:1035)\n  at org.apache.spark.SparkContext$$anonfun$hadoopFile$1.apply(SparkContext.scala:1027)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.SparkContext.withScope(SparkContext.scala:699)\n  at org.apache.spark.SparkContext.hadoopFile(SparkContext.scala:1027)\n  at org.apache.spark.SparkContext$$anonfun$textFile$1.apply(SparkContext.scala:830)\n  at org.apache.spark.SparkContext$$anonfun$textFile$1.apply(SparkContext.scala:828)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.SparkContext.withScope(SparkContext.scala:699)\n  at org.apache.spark.SparkContext.textFile(SparkContext.scala:828)\n  ... 45 elided\n</code></pre>\n<p>Here is the code I am using to load a text file:</p>\n<pre><code>val lines = sc.textFile(&quot;/root/input/file1.txt&quot;)\n</code></pre>\n<p>Here is the file1.txt:</p>\n<pre><code>Hello Spark Wordcount!\nHello everybody else!\n</code></pre>\n<p>Environment:</p>\n<blockquote>\n<pre><code>Spark Version: 2.4.5\n\nScala Version: 2.11.12\n\nJava Version: 21.0.6 (OpenJDK 64-Bit Server VM)\n</code></pre>\n</blockquote>\n<p>What I've tried:</p>\n<pre><code>Added JVM Options: I've tried adding the following JVM options to bypass the module system restrictions, but it didn't solve the issue:\n\n--conf &quot;spark.driver.extraJavaOptions=--add-opens java.base/java.util=ALL-UNNAMED&quot; --conf &quot;spark.executor.extraJavaOptions=--add-opens java.base/java.util=ALL-UNNAMED&quot;\n</code></pre>\n",
    "tags" : [ "java", "apache-spark", "mapreduce", "bigdata" ],
    "owner" : {
      "account_id" : 29875133,
      "reputation" : 1,
      "user_id" : 22895541,
      "user_type" : "registered",
      "profile_image" : "https://i.sstatic.net/LfuBk.png?s=256",
      "display_name" : "aymane RIhane",
      "link" : "https://stackoverflow.com/users/22895541/aymane-rihane"
    },
    "is_answered" : false,
    "view_count" : 68,
    "answer_count" : 0,
    "score" : 0,
    "last_activity_date" : 1743328757,
    "creation_date" : 1743297157,
    "link" : "https://stackoverflow.com/questions/79544089/inaccessibleobjectexception-when-using-spark-with-java-21-and-scala-2-11-12",
    "content_license" : "CC BY-SA 4.0"
  },
  "answers" : [ ],
  "question_comments" : [ {
    "comment_id" : 140281872,
    "post_id" : 79544089,
    "body" : "@greg-449 - this should be THE answer",
    "score" : 0,
    "owner" : {
      "account_id" : 1017255,
      "reputation" : 2965,
      "user_id" : 1028537,
      "user_type" : "registered",
      "profile_image" : "https://www.gravatar.com/avatar/f929a8e1ab37c0667f6991744a3f3ca1?s=256&d=identicon&r=PG",
      "display_name" : "Chris",
      "link" : "https://stackoverflow.com/users/1028537/chris"
    },
    "creation_date" : 1743416460,
    "content_license" : "CC BY-SA 4.0"
  }, {
    "comment_id" : 140278790,
    "post_id" : 79544089,
    "body" : "Recent versions of Java don&#39;t allow that sort of access to core code regardless of any <code>add-opens</code> options. You need to use a version of Spark that supports Java 21.",
    "score" : 1,
    "owner" : {
      "account_id" : 3159259,
      "reputation" : 111552,
      "user_id" : 2670892,
      "user_type" : "registered",
      "profile_image" : "https://i.sstatic.net/4iPV5.png?s=256",
      "display_name" : "greg-449",
      "link" : "https://stackoverflow.com/users/2670892/greg-449"
    },
    "creation_date" : 1743326401,
    "content_license" : "CC BY-SA 4.0"
  } ],
  "answer_comments" : { }
}