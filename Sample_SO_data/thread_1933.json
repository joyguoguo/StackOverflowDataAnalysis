{
  "question" : {
    "question_id" : 79662793,
    "title" : "Spark submit error: Hive metastore AlreadyExistsException class not found â€“ what&#39;s the correct way to load Hive jars?",
    "body" : "<p>During the spark-submit job I am getting error:</p>\n<pre><code>spark-submit   --master yarn   --deploy-mode client   --executor-memory 5g   --executor-cores 4    /home/hadoop/spark_data/save_data_db.py\n</code></pre>\n<p>My job file save_data_db.py:</p>\n<pre><code>from pyspark.sql import SparkSession\nspark = SparkSession.builder \\\n    .appName(&quot;InsertIntoHiveYarn&quot;) \\\n    .enableHiveSupport() \\\n    .config(&quot;hive.server2.authentication&quot;, &quot;CUSTOM&quot;) \\\n    .config(&quot;javax.jdo.option.ConnectionUserName&quot;, &quot;hive_user&quot;) \\\n    .config(&quot;javax.jdo.option.ConnectionPassword&quot;, &quot;p&quot;) \\\n    .getOrCreate()\ndata = [(1, &quot;Alice&quot;, &quot;HR&quot;),\n        (2, &quot;Bob&quot;, &quot;Engineering&quot;),\n        (3, &quot;Charlie&quot;, &quot;Marketing&quot;)]\ndf = spark.createDataFrame(data, [&quot;id&quot;, &quot;name&quot;, &quot;department&quot;])\nspark.sql('USE db_test')\ndf.write.insertInto(&quot;employees&quot;, overwrite=False)\nspark.stop()\n</code></pre>\n<p>I have put jars file into hdfs to avoid any jar error in separate directory:</p>\n<pre><code>[hadoop@RA-DN1 spark]$ hdfs dfs -ls /spark-jars/\nFound 2 items\ndrwxr-xr-x   - hadoop supergroup          0 2025-06-12 05:44 /spark-jars/hive\ndrwxr-xr-x   - hadoop supergroup          0 2025-06-12 05:19 /spark-jars/spark\n[hadoop@RA-DN1 spark]$ hdfs dfs -ls /spark-jars/hive/\nFound 1 items\n-rw-r--r--   3 hadoop supergroup  258269977 2025-06-12 05:44 /spark-jars/hive/hive-libs.zip\n[hadoop@RA-DN1 spark]$ hdfs dfs -ls /spark-jars/spark/\nFound 1 items\n-rw-r--r--   3 hadoop supergroup  354049683 2025-06-12 05:19 /spark-jars/spark/spark-libs.zip\n</code></pre>\n<p><code>spark-default.conf</code>:</p>\n<pre><code>spark.master                    yarn\nspark.submit.deployMode         cluster\nspark.eventLog.enabled          true\nspark.eventLog.dir              hdfs:///spark-logs\nspark.history.fs.logDirectory   hdfs:///spark-logs\nspark.yarn.historyServer.address namenode2:18080\nspark.executor.memory           10g\nspark.driver.memory             2g\nspark.executor.instances        4\nspark.executor.cores            4\n\nspark.hadoop.fs.defaultFS        hdfs://mycluster\nspark.hadoop.dfs.nameservices    mycluster\nspark.hadoop.dfs.ha.namenodes.mycluster namenode1,namenode2\nspark.hadoop.dfs.namenode.rpc-address.mycluster.namenode1 namenode1:8020\nspark.hadoop.dfs.namenode.rpc-address.mycluster.namenode2 namenode2:8020\nspark.hadoop.dfs.client.failover.proxy.provider.mycluster org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider\n\nspark.yarn.archive hdfs:///spark-jars/spark/spark-libs.zip\n\n\nspark.kerberos.access.hadoopFileSystems  hdfs://mycluster\n\nspark.hadoop.yarn.resourcemanager.ha.enabled true\nspark.hadoop.yarn.resourcemanager.cluster-id yarn-cluster\nspark.hadoop.yarn.resourcemanager.ha.rm-ids rm1,rm2\nspark.hadoop.yarn.resourcemanager.hostname.rm1 namenode1\nspark.hadoop.yarn.resourcemanager.hostname.rm2 namenode2\nspark.hadoop.yarn.resourcemanager.address.rm1 namenode1:8032\nspark.hadoop.yarn.resourcemanager.address.rm2 namenode2:8032\nspark.hadoop.yarn.resourcemanager.scheduler.address.rm1 namenode1:8030\nspark.hadoop.yarn.resourcemanager.scheduler.address.rm2 namenode2:8030\n\nspark.sql.catalogImplementation=hive\nspark.sql.warehouse.dir=hdfs://mycluster/user/hive/warehouse2\nspark.hadoop.hive.metastore.uris=thrift://10.101.10.20:9083\nspark.sql.hive.metastore.version=3.1.3\nspark.sql.hive.metastore.jars=hdfs:///spark-jars/hive/hive-libs.zip\n</code></pre>\n<p>Spark submission error:</p>\n<pre><code>with ID 4,  ResourceProfileId 0\n2025-06-12 06:05:56 INFO  YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8\n2025-06-12 06:05:56 INFO  BlockManagerMasterEndpoint: Registering block manager datanode2:43323 with 2.5 GiB RAM, BlockManagerId(4, datanode2, 43323, None)\n2025-06-12 06:05:56 INFO  SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n2025-06-12 06:05:56 INFO  SharedState: Warehouse path is 'hdfs://mycluster/user/hive/warehouse2'.\n2025-06-12 06:05:56 INFO  ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n2025-06-12 06:05:56 INFO  ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n2025-06-12 06:05:56 INFO  ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n2025-06-12 06:05:56 INFO  ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n2025-06-12 06:05:56 INFO  ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n2025-06-12 06:05:59 INFO  HiveUtils: Initializing HiveMetastoreConnection version 3.1.3 using file:/home/hadoop/spark_data/hdfs:file:/spark-jars/hive/hive-libs.zip\nTraceback (most recent call last):\n  File &quot;/home/hadoop/spark_data/save_data_db.py&quot;, line 17, in &lt;module&gt;\n    spark.sql('USE db_test')\n  File &quot;/data/hadoop/binary/spark/python/lib/pyspark.zip/pyspark/sql/session.py&quot;, line 1631, in sql\n  File &quot;/data/hadoop/binary/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py&quot;, line 1322, in __call__\n  File &quot;/data/hadoop/binary/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py&quot;, line 179, in deco\n  File &quot;/data/hadoop/binary/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py&quot;, line 326, in get_return_value\npy4j.protocol.Py4JJavaError: An error occurred while calling o62.sql.\n: java.lang.NoClassDefFoundError: org/apache/hadoop/hive/metastore/api/AlreadyExistsException\n        at java.base/java.lang.Class.getDeclaredConstructors0(Native Method)\n        at java.base/java.lang.Class.privateGetDeclaredConstructors(Class.java:3137)\n        at java.base/java.lang.Class.getConstructors(Class.java:1943)\n        at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:317)\n        at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:488)\n        at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:376)\n        at org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:68)\n        at org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:67)\n        at org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$databaseExists$1(HiveExternalCatalog.scala:224)\n        at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n        at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:99)\n        at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:224)\n        at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:146)\n        at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:140)\n        at org.apache.spark.sql.internal.SharedState.globalTempViewManager$lzycompute(SharedState.scala:176)\n        at org.apache.spark.sql.internal.SharedState.globalTempViewManager(SharedState.scala:174)\n        at org.apache.spark.sql.hive.HiveSessionStateBuilder.$anonfun$catalog$2(HiveSessionStateBuilder.scala:70)\n        at org.apache.spark.sql.catalyst.catalog.SessionCatalog.globalTempViewManager$lzycompute(SessionCatalog.scala:124)\n        at org.apache.spark.sql.catalyst.catalog.SessionCatalog.globalTempViewManager(SessionCatalog.scala:124)\n        at org.apache.spark.sql.catalyst.catalog.SessionCatalog.setCurrentDatabaseWithNameCheck(SessionCatalog.scala:340)\n        at org.apache.spark.sql.connector.catalog.CatalogManager.setCurrentNamespace(CatalogManager.scala:119)\n        at org.apache.spark.sql.execution.datasources.v2.SetCatalogAndNamespaceExec.$anonfun$run$2(SetCatalogAndNamespaceExec.scala:36)\n        at org.apache.spark.sql.execution.datasources.v2.SetCatalogAndNamespaceExec.$anonfun$run$2$adapted(SetCatalogAndNamespaceExec.scala:36)\n        at scala.Option.foreach(Option.scala:407)\n        at org.apache.spark.sql.execution.datasources.v2.SetCatalogAndNamespaceExec.run(SetCatalogAndNamespaceExec.scala:36)\n        at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n        at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n        at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n        at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n        at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n        at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n        at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n        at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n        at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n        at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n        at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n        at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n        at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n        at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n        at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n        at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n        at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n        at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n        at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n        at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n        at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n        at org.apache.spark.sql.Dataset.&lt;init&gt;(Dataset.scala:220)\n        at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)\n        at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n        at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n        at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)\n        at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n        at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)\n        at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)\n        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.base/java.lang.reflect.Method.invoke(Method.java:566)\n        at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n        at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n        at py4j.Gateway.invoke(Gateway.java:282)\n        at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n        at py4j.commands.CallCommand.execute(CallCommand.java:79)\n        at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n        at py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n        at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.ClassNotFoundException: org.apache.hadoop.hive.metastore.api.AlreadyExistsException\n        at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)\n        at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)\n        at org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1.doLoadClass(IsolatedClientLoader.scala:274)\n        at org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1.loadClass(IsolatedClientLoader.scala:263)\n        at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)\n        ... 68 more\n\n2025-06-12 06:05:59 INFO  SparkContext: Invoking stop() from shutdown hook\n2025-06-12 06:05:59 INFO  SparkContext: SparkContext is stopping with exitCode 0.\n2025-06-12 06:05:59 INFO  SparkUI: Stopped Spark web UI at http://datanode1:4040\n2025-06-12 06:05:59 INFO  YarnClientSchedulerBackend: Interrupting monitor thread\n2025-06-12 06:05:59 INFO  YarnClientSchedulerBackend: Shutting down all executors\n2025-06-12 06:05:59 INFO  YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down\n2025-06-12 06:05:59 INFO  YarnClientSchedulerBackend: YARN client scheduler backend Stopped\n2025-06-12 06:05:59 INFO  MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n2025-06-12 06:05:59 INFO  MemoryStore: MemoryStore cleared\n2025-06-12 06:05:59 INFO  BlockManager: BlockManager stopped\n2025-06-12 06:05:59 INFO  BlockManagerMaster: BlockManagerMaster stopped\n2025-06-12 06:05:59 INFO  OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n2025-06-12 06:05:59 INFO  SparkContext: Successfully stopped SparkContext\n2025-06-12 06:05:59 INFO  ShutdownHookManager: Shutdown hook called\n2025-06-12 06:05:59 INFO  ShutdownHookManager: Deleting directory /tmp/spark-4c9908be-eea2-4e85-95a4-d20a87f0cb94\n2025-06-12 06:05:59 INFO  ShutdownHookManager: Deleting directory /tmp/spark-4c9908be-eea2-4e85-95a4-d20a87f0cb94/pyspark-1fafb003-619d-4387-8b16-433e237ecceb\n2025-06-12 06:05:59 INFO  ShutdownHookManager: Deleting directory /tmp/spark-8ac364fb-502d-42bd-8808-b261a7d9a693\n</code></pre>\n<p>I am using Hive to store data from Spark into Hive. I have tried both cluster and client mode get same result.</p>\n",
    "tags" : [ "java", "scala", "apache-spark", "pyspark", "hive" ],
    "owner" : {
      "account_id" : 5135409,
      "reputation" : 568,
      "user_id" : 4118670,
      "user_type" : "registered",
      "profile_image" : "https://i.sstatic.net/HRdub.jpg?s=256",
      "display_name" : "Sheikh Wasiu Al Hasib",
      "link" : "https://stackoverflow.com/users/4118670/sheikh-wasiu-al-hasib"
    },
    "is_answered" : true,
    "view_count" : 80,
    "answer_count" : 2,
    "score" : 0,
    "last_activity_date" : 1749712457,
    "creation_date" : 1749687141,
    "link" : "https://stackoverflow.com/questions/79662793/spark-submit-error-hive-metastore-alreadyexistsexception-class-not-found-what",
    "content_license" : "CC BY-SA 4.0"
  },
  "answers" : [ {
    "answer_id" : 79663074,
    "question_id" : 79662793,
    "body" : "<p>Please check if the file hive-metastore-2.x.x.jar exists in your Spark jars path. I found hive-metastore-2.3.9.jar in the spark-3.5.0-bin-hadoop3/jars directory.</p>\n",
    "score" : 1,
    "is_accepted" : false,
    "owner" : {
      "account_id" : 20918905,
      "reputation" : 41,
      "user_id" : 15367984,
      "user_type" : "registered",
      "profile_image" : "https://www.gravatar.com/avatar/574bc7bf5399cd380c11703b805862b4?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name" : "flowerbirds",
      "link" : "https://stackoverflow.com/users/15367984/flowerbirds"
    },
    "creation_date" : 1749712457,
    "last_activity_date" : 1749712457,
    "content_license" : "CC BY-SA 4.0"
  }, {
    "answer_id" : 79662941,
    "question_id" : 79662793,
    "body" : "<blockquote>\n<p>INFO  HiveUtils: Initializing HiveMetastoreConnection version 3.1.3 using file:/home/hadoop/spark_data/hdfs:file:/spark-jars/hive/hive-libs.zip</p>\n</blockquote>\n<p>From this line of logs, I would think that <code>spark.sql.hive.metastore.jars</code> should target a folder containing the JARs, not a ZIP file.</p>\n<p>I believe here it would try consider the ZIP as a single JAR and just ignore its content.</p>\n",
    "score" : 0,
    "is_accepted" : false,
    "owner" : {
      "account_id" : 7034556,
      "reputation" : 15907,
      "user_id" : 5389127,
      "user_type" : "registered",
      "profile_image" : "https://www.gravatar.com/avatar/391c01e15f62cee096b758c4c2815c2c?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name" : "Ga&#235;l J",
      "link" : "https://stackoverflow.com/users/5389127/ga%c3%abl-j"
    },
    "creation_date" : 1749705099,
    "last_activity_date" : 1749705099,
    "content_license" : "CC BY-SA 4.0"
  } ],
  "question_comments" : [ ],
  "answer_comments" : {
    "79662941" : [ {
      "comment_id" : 140508483,
      "post_id" : 79662941,
      "body" : "its create issue at line 17 where I execute spark.sql(&#39;USE db_test&#39;)   File &quot;/home/hadoop/spark_data/save_data_db.py&quot;, line 17, in &lt;module&gt;      spark.sql(&#39;USE db_test&#39;)",
      "score" : 0,
      "owner" : {
        "account_id" : 5135409,
        "reputation" : 568,
        "user_id" : 4118670,
        "user_type" : "registered",
        "profile_image" : "https://i.sstatic.net/HRdub.jpg?s=256",
        "display_name" : "Sheikh Wasiu Al Hasib",
        "link" : "https://stackoverflow.com/users/4118670/sheikh-wasiu-al-hasib"
      },
      "creation_date" : 1749729896,
      "content_license" : "CC BY-SA 4.0"
    }, {
      "comment_id" : 140508477,
      "post_id" : 79662941,
      "body" : "I did not find how this file generated?  ile:/home/hadoop/spark_data/hdfs:file:/spark-jars/hive/hive-&zwnj;&#8203;libs.zip",
      "score" : 0,
      "owner" : {
        "account_id" : 5135409,
        "reputation" : 568,
        "user_id" : 4118670,
        "user_type" : "registered",
        "profile_image" : "https://i.sstatic.net/HRdub.jpg?s=256",
        "display_name" : "Sheikh Wasiu Al Hasib",
        "link" : "https://stackoverflow.com/users/4118670/sheikh-wasiu-al-hasib"
      },
      "creation_date" : 1749729809,
      "content_license" : "CC BY-SA 4.0"
    } ],
    "79663074" : [ {
      "comment_id" : 140565855,
      "post_id" : 79663074,
      "body" : "It was Spark versioning issue, but alsoan  issue with wrong configuration, you will get details here below link. It took many hour to find the actual reason.    <a href=\"https://medium.com/@wasiualhasib/fixing-invalid-method-name-get-table-in-spark-when-connecting-to-hive-metastore-a1be6a618e2c\" rel=\"nofollow noreferrer\">medium.com/@wasiualhasib/&hellip;</a>",
      "score" : 0,
      "owner" : {
        "account_id" : 5135409,
        "reputation" : 568,
        "user_id" : 4118670,
        "user_type" : "registered",
        "profile_image" : "https://i.sstatic.net/HRdub.jpg?s=256",
        "display_name" : "Sheikh Wasiu Al Hasib",
        "link" : "https://stackoverflow.com/users/4118670/sheikh-wasiu-al-hasib"
      },
      "creation_date" : 1751657361,
      "content_license" : "CC BY-SA 4.0"
    }, {
      "comment_id" : 140507864,
      "post_id" : 79663074,
      "body" : "yes you are right i got this. BEcause of that I am getting this exception again and again &quot;java.lang.NoClassDefFoundError: AlreadyExistsException  &quot;. Now what I need to do do I remove those package from spark jars or should I keep it as it is? or replace those with hive 3.1.3 hive packages?    Thsoe are the below package I got at the spark jars which is 2.3.9.  hive-beeline,   hive-jdbc,     hive-service-rpc,  hive-shims-scheduler,  hive-cli,        hive-llap-common,  hive-shims,    hive-storage-api,  hive-common,     hive-metastore,    hive-shims,         spark-hive,  hive-exec  hive-serde",
      "score" : 0,
      "owner" : {
        "account_id" : 5135409,
        "reputation" : 568,
        "user_id" : 4118670,
        "user_type" : "registered",
        "profile_image" : "https://i.sstatic.net/HRdub.jpg?s=256",
        "display_name" : "Sheikh Wasiu Al Hasib",
        "link" : "https://stackoverflow.com/users/4118670/sheikh-wasiu-al-hasib"
      },
      "creation_date" : 1749718183,
      "content_license" : "CC BY-SA 4.0"
    } ]
  }
}