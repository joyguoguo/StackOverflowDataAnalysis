{
  "question" : {
    "question_id" : 79734166,
    "title" : "Spring WebFlux SSE with Google Vertex AI Streaming returns all chunks at once instead of immediately",
    "body" : "<p>I’m integrating Google Vertex AI’s generateContentStream API with Spring WebFlux to stream output chunks from a file analysis request as Server-Sent Events (SSE).</p>\n<p>The idea is:</p>\n<ul>\n<li><p>Upload a file via /v3/stream endpoint</p>\n</li>\n<li><p>Send it to Vertex AI with a prompt</p>\n</li>\n<li><p>Stream chunks to the client as soon as Vertex AI sends them</p>\n</li>\n</ul>\n<p>But the problem:\nI see logs for each chunk being processed inside Flux.create(...) and the client (Postman, browser) only receives all the chunks after the stream is completely finished — instead of getting them one-by-one in real time.</p>\n<p>Here’s my service method:</p>\n<pre><code>    public Flux&lt;String&gt; analyzeFileWithPromptStream(Path filePath, String mimeType, DocumentType documentType, String prompt) {\n        try {\n            byte[] fileBytes = Files.readAllBytes(filePath);\n            ByteString content = ByteString.copyFrom(fileBytes);\n\n            Part filePart = Part.newBuilder()\n                    .setInlineData(Blob.newBuilder()\n                            .setMimeType(mimeType)\n                            .setData(content)\n                            .build())\n                    .build();\n\n            String customPrompt = getCustomPromptForDocumentType(documentType);\n            String promptText = (prompt != null &amp;&amp; !prompt.isEmpty())\n                    ? prompt\n                    : &quot;&quot;&quot;\n                    Analyze this document thoroughly...\n                    &quot;&quot;&quot; + (customPrompt.isEmpty() ? &quot;&quot; : (&quot;\\n&quot; + customPrompt));\n\n            Part promptPart = Part.newBuilder().setText(promptText).build();\n\n            Content contentRequest = Content.newBuilder()\n                    .setRole(&quot;user&quot;)\n                    .addParts(promptPart)\n                    .addParts(filePart)\n                    .build();\n\n//       generativeModel.generateContentStream(\n//           contentRequest)\n//           .stream()\n//           .forEach(System.out::println);\n\n        //       return Flux.interval(Duration.ofMillis(500))\n        //                .map(i -&gt; &quot;Chunk &quot; + i + &quot;\\n&quot;)\n        //                .take(10);\n\n            return Flux.create(sink -&gt; {\n                try {\n                    ResponseStream&lt;GenerateContentResponse&gt; stream =\n                            generativeModel.generateContentStream(contentRequest);\n\n                    for (GenerateContentResponse response : stream) {\n                        for (Candidate candidate : response.getCandidatesList()) {\n                            for (Part part : candidate.getContent().getPartsList()) {\n                                String chunk = part.getText()\n                                        .replaceAll(&quot;```json&quot;, &quot;&quot;)\n                                        .replaceAll(&quot;```&quot;, &quot;&quot;)\n                                        .trim();\n                                if (!chunk.isEmpty()) {\n                                    log.info(&quot;Streaming chunk: {}&quot;, chunk);\n                                    sink.next(chunk);\n                                }\n                            }\n                        }\n                    }\n                    sink.complete();\n                } catch (Exception e) {\n                    log.error(&quot;Stream error&quot;, e);\n                    sink.error(e);\n                }\n            });\n\n        } catch (IOException e) {\n            log.error(&quot;Error reading file: {}&quot;, e.getMessage(), e);\n            return Flux.error(new RuntimeException(&quot;Error reading file&quot;, e));\n        }\n    }\n</code></pre>\n<p>My controller:</p>\n<pre><code>    public Flux&lt;ServerSentEvent&lt;String&gt;&gt; performOcrStream(\n            @RequestPart(&quot;file&quot;) MultipartFile file,\n            @RequestPart(&quot;request&quot;) OcrReqDto request,\n            @RequestPart(value = &quot;prompt&quot;, required = false) String prompt) {\n\n        if (file == null || file.isEmpty()) {\n            return Flux.error(new IllegalArgumentException(&quot;File is required&quot;));\n        }\n        if (request == null || request.getDocumentType() == null) {\n            return Flux.error(new IllegalArgumentException(&quot;Document type is required&quot;));\n        }\n\n        try {\n            Path tempFile = Files.createTempFile(&quot;gemini-&quot;, file.getOriginalFilename());\n            file.transferTo(tempFile.toFile());\n            String mimeType = file.getContentType();\n\n            return Flux.using(\n                    () -&gt; tempFile,\n                    path -&gt; geminiVertexService.analyzeFileWithPromptStream(path, mimeType, request.getDocumentType(), prompt)\n                            .map(chunk -&gt; ServerSentEvent.builder(chunk)\n                                    .event(&quot;message&quot;)\n                                    .build()),\n                    path -&gt; {\n                        try {\n                            Files.deleteIfExists(path);\n                            log.info(&quot;Temporary file deleted: {}&quot;, path);\n                        } catch (IOException e) {\n                            log.warn(&quot;Failed to delete temporary file: {}&quot;, path, e);\n                        }\n                    }\n            ).onErrorResume(ex -&gt; {\n                return Flux.just(ServerSentEvent.builder(&quot;ERROR: &quot; + ex.getMessage())\n                        .event(&quot;error&quot;)\n                        .build());\n            });\n\n        } catch (IOException e) {\n            return Flux.just(ServerSentEvent.builder(&quot;Failed to process file: &quot; + e.getMessage())\n                    .event(&quot;error&quot;)\n                    .build());\n        }\n    }\n</code></pre>\n<p>What I’ve tried:</p>\n<p>Checked google documentation at <a href=\"https://cloud.google.com/vertex-ai/generative-ai/docs/samples/generativeaionvertexai-stream-multimodality-basic#generativeaionvertexai_stream_multimodality_basic-java\" rel=\"nofollow noreferrer\">google docs link</a></p>\n<p>Used MediaType.TEXT_EVENT_STREAM_VALUE</p>\n<p>Tried flushing explicitly using .doOnNext(...) and .log() — no effect</p>\n<p>Tested in Postman waits for final completion before showing data</p>\n<p>Question:\nWhy is Spring WebFlux not sending SSE chunks to the client as soon as they’re received from Google Vertex AI?\nHow can I make chunks flush to the client immediately instead of waiting for the full stream to complete?</p>\n",
    "tags" : [ "java", "spring-webflux", "server-sent-events", "google-cloud-vertex-ai", "google-gemini" ],
    "owner" : {
      "account_id" : 30563100,
      "reputation" : 21,
      "user_id" : 23425956,
      "user_type" : "registered",
      "profile_image" : "https://www.gravatar.com/avatar/c2bc321c665965b8f017505cd8e99485?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name" : "priyam-shah",
      "link" : "https://stackoverflow.com/users/23425956/priyam-shah"
    },
    "is_answered" : false,
    "view_count" : 123,
    "answer_count" : 0,
    "score" : 0,
    "last_activity_date" : 1755084195,
    "creation_date" : 1755080013,
    "link" : "https://stackoverflow.com/questions/79734166/spring-webflux-sse-with-google-vertex-ai-streaming-returns-all-chunks-at-once-in",
    "content_license" : "CC BY-SA 4.0"
  },
  "answers" : [ ],
  "question_comments" : [ {
    "comment_id" : 140680286,
    "post_id" : 79734166,
    "body" : "then i cant help you without a running example",
    "score" : 0,
    "owner" : {
      "account_id" : 2064278,
      "reputation" : 15073,
      "user_id" : 1840146,
      "user_type" : "registered",
      "accept_rate" : 86,
      "profile_image" : "https://i.sstatic.net/JXdxm.png?s=256",
      "display_name" : "Toerktumlare",
      "link" : "https://stackoverflow.com/users/1840146/toerktumlare"
    },
    "creation_date" : 1755720579,
    "content_license" : "CC BY-SA 4.0"
  }, {
    "comment_id" : 140678866,
    "post_id" : 79734166,
    "body" : "@Toerktumlare the same issue persists on terminal when running the curl",
    "score" : 0,
    "owner" : {
      "account_id" : 30563100,
      "reputation" : 21,
      "user_id" : 23425956,
      "user_type" : "registered",
      "profile_image" : "https://www.gravatar.com/avatar/c2bc321c665965b8f017505cd8e99485?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name" : "priyam-shah",
      "link" : "https://stackoverflow.com/users/23425956/priyam-shah"
    },
    "creation_date" : 1755687871,
    "content_license" : "CC BY-SA 4.0"
  }, {
    "comment_id" : 140664045,
    "post_id" : 79734166,
    "body" : "postman as far as i know, cant stream events, it will buffer data and show it all when the connection closes.  can you try to stream the data using curl and disable buffering.  <code>curl -N http:&#47;&#47;localhost:8080&#47;stream-endpoint</code>",
    "score" : 0,
    "owner" : {
      "account_id" : 2064278,
      "reputation" : 15073,
      "user_id" : 1840146,
      "user_type" : "registered",
      "accept_rate" : 86,
      "profile_image" : "https://i.sstatic.net/JXdxm.png?s=256",
      "display_name" : "Toerktumlare",
      "link" : "https://stackoverflow.com/users/1840146/toerktumlare"
    },
    "creation_date" : 1755125870,
    "content_license" : "CC BY-SA 4.0"
  }, {
    "comment_id" : 140662528,
    "post_id" : 79734166,
    "body" : "Try to log time in <code>for (GenerateContentResponse response : stream)</code> loop. Maybe they send response all at once.",
    "score" : 0,
    "owner" : {
      "account_id" : 4497436,
      "reputation" : 20786,
      "user_id" : 3656904,
      "user_type" : "registered",
      "accept_rate" : 67,
      "profile_image" : "https://www.gravatar.com/avatar/8bdbd1f89f8907e8e8b15ab88c084c65?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name" : "talex",
      "link" : "https://stackoverflow.com/users/3656904/talex"
    },
    "creation_date" : 1755086014,
    "content_license" : "CC BY-SA 4.0"
  }, {
    "comment_id" : 140662376,
    "post_id" : 79734166,
    "body" : "@talex thanks for the comment, yes i did tried that and even with that method the response would come with all the cunks after 15ish seconds. i.e. after all the chunks are received",
    "score" : 0,
    "owner" : {
      "account_id" : 30563100,
      "reputation" : 21,
      "user_id" : 23425956,
      "user_type" : "registered",
      "profile_image" : "https://www.gravatar.com/avatar/c2bc321c665965b8f017505cd8e99485?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name" : "priyam-shah",
      "link" : "https://stackoverflow.com/users/23425956/priyam-shah"
    },
    "creation_date" : 1755081455,
    "content_license" : "CC BY-SA 4.0"
  }, {
    "comment_id" : 140662368,
    "post_id" : 79734166,
    "body" : "Did you tried <code>Flux.fromIterable()</code> and then <code>flatMap</code> through <code>getCandidatesList()</code> and <code>getContent().getPartsList()</code>",
    "score" : 0,
    "owner" : {
      "account_id" : 4497436,
      "reputation" : 20786,
      "user_id" : 3656904,
      "user_type" : "registered",
      "accept_rate" : 67,
      "profile_image" : "https://www.gravatar.com/avatar/8bdbd1f89f8907e8e8b15ab88c084c65?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name" : "talex",
      "link" : "https://stackoverflow.com/users/3656904/talex"
    },
    "creation_date" : 1755081037,
    "content_license" : "CC BY-SA 4.0"
  } ],
  "answer_comments" : { }
}