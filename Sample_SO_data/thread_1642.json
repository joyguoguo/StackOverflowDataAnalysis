{
  "question" : {
    "question_id" : 79688622,
    "title" : "How to use streams to process chunks in JPA stream of records",
    "body" : "<p>In Spring Boot 2.7, streaming results back from a database query and processing them in batches using JDBC is very easy:</p>\n<pre class=\"lang-java prettyprint-override\"><code>   try (PreparedStatement ps = conn.prepareStatement(MY_SQL)) {\n            ps.setFetchSize(100);\n              ResultSet rs = ps.executeQuery();\n         do {\n            List&lt;MyEntity&gt; chunk = new ArrayList&lt;&gt;();\n            while (rs.next() &amp;&amp; (rowsReadInThisBatch &lt; myBatchSize)) {\n                    MyEntity entity = new MyEntity();\n                    myEntity.setSomeCol(rs.getLong(&quot;some_col&quot;);\n                    myEntity.setSomeCol2(rs.getLong(&quot;some_col2&quot;);\n                    chunk.add(entity)\n                    rowsReadInThisBatch++;\n                }\n            // lost more code.\n            process(chunk);\n            writeBackToDB(cunk);\n            // lots more code\n        } while (rowsReadInThisBatch == myBatchSize);\n</code></pre>\n<p>The database table has &gt; 1 million rows, so we can't just read them all into a single 1 million row object as our production microservice servers only have 1 GB RAM. In the above example, we only ever have myBatchSize records in RAM, and we only need to do total/myBatchSize batch inserts to the database (not one insert for every record which would result in 1 million round trips to DB).</p>\n<p>To do this with streams in JPA, there is no equivalent of <code>rs.next()</code>. Instead you have to use lambdas and Java streams stuff.</p>\n<p>There is ForEach:</p>\n<pre class=\"lang-java prettyprint-override\"><code>    try(Stream&lt;MyEntity&gt; myStream = postRepository.streamByCreatedOnSince(yesterday)) {   \n        myStream.forEach(\n             ....\n        );\n    }\n</code></pre>\n<p>However, forEach is extremely limited because you can't use non-final variables to count how many have been processed etc.</p>\n<p>How do I convert my plain old <code>do</code> loops and while loops into Java's streams/lambda stuff in order to read in chunks and process chunks, and not process each one by one with no chunking? Ideally not using inline lambda code which is hard to test separately (and hard to understand for non-Java streams experts)?</p>\n<p>The crux is Java streams don't have the concept of a counter or chunking.</p>\n<p>This article: <a href=\"https://www.baeldung.com/java-stream-batch-processing\" rel=\"nofollow noreferrer\">https://www.baeldung.com/java-stream-batch-processing</a> has some very esoteric solutions requiring additional libraries which we want to avoid. Also, it's not clear if their solutions read the entire stream into memory then break it into chunks or batches, something we want to avoid due to the very large data set.</p>\n",
    "tags" : [ "java", "jpa", "java-stream" ],
    "owner" : {
      "account_id" : 1073482,
      "reputation" : 12771,
      "user_id" : 1072187,
      "user_type" : "registered",
      "accept_rate" : 36,
      "profile_image" : "https://www.gravatar.com/avatar/6a15142de5577986384774484ff25fe6?s=256&d=identicon&r=PG",
      "display_name" : "John Little",
      "link" : "https://stackoverflow.com/users/1072187/john-little"
    },
    "is_answered" : true,
    "view_count" : 196,
    "answer_count" : 2,
    "score" : -3,
    "last_activity_date" : 1764364563,
    "creation_date" : 1751535502,
    "link" : "https://stackoverflow.com/questions/79688622/how-to-use-streams-to-process-chunks-in-jpa-stream-of-records",
    "content_license" : "CC BY-SA 4.0"
  },
  "answers" : [ {
    "answer_id" : 79692376,
    "question_id" : 79688622,
    "body" : "<blockquote>\n<p>The crux is java streams don't have the concept of [...] chunking.</p>\n</blockquote>\n<p>Sure they do. It's called Stream <a href=\"https://openjdk.org/jeps/485\" rel=\"nofollow noreferrer\">Gatherers</a>.</p>\n<p>Specifically, you are looking for the window functions. Consider the following example.</p>\n<pre><code>final List&lt;Character&gt; list = \n    List\n        .of\n        (\n            'a','b','c',\n            'd','e','f',\n            'g','h','i',\n            'j','k','l',\n            'm','n','o',\n            'p','q','r',\n            's','t','u',\n            'v','w','x',\n            'y','z'\n        )\n        ;\nlist\n    .stream()\n    .map(String::valueOf)\n    .gather(Gatherers.windowFixed(5))\n    .forEach(System.out::println)\n    ;\n\n//output\n//[a, b, c, d, e]\n//[f, g, h, i, j]\n//[k, l, m, n, o]\n//[p, q, r, s, t]\n//[u, v, w, x, y]\n//[z]\n</code></pre>\n<p>Here is the documentation -- <a href=\"https://docs.oracle.com/en/java/javase/24/docs/api/java.base/java/util/stream/Gatherers.html#windowFixed(int)\" rel=\"nofollow noreferrer\">Gatherers.windowFixed(int batchSize)</a></p>\n",
    "score" : 2,
    "is_accepted" : false,
    "owner" : {
      "account_id" : 14010051,
      "reputation" : 250,
      "user_id" : 10118965,
      "user_type" : "registered",
      "profile_image" : "https://www.gravatar.com/avatar/61afd55894624ae8f6e4189b68db1f30?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name" : "davidalayachew",
      "link" : "https://stackoverflow.com/users/10118965/davidalayachew"
    },
    "creation_date" : 1751868645,
    "last_activity_date" : 1751868645,
    "content_license" : "CC BY-SA 4.0"
  }, {
    "answer_id" : 79689501,
    "question_id" : 79688622,
    "body" : "<p>You can use streams, you just have to batch it:</p>\n<p>I have this method:</p>\n<pre class=\"lang-java prettyprint-override\"><code>public static &lt;T&gt; Stream&lt;List&lt;T&gt;&gt; batchStream(Stream&lt;T&gt; stream, int batchSize) {\n    Iterator&lt;T&gt; iterator = stream.iterator();\n\n    return StreamSupport.stream(new Spliterators.AbstractSpliterator&lt;&gt;(Long.MAX_VALUE, 0) {\n        @Override\n        public boolean tryAdvance(Consumer&lt;? super List&lt;T&gt;&gt; action) {\n            List&lt;T&gt; batch = new ArrayList&lt;&gt;(batchSize);\n            int count = 0;\n            while (count &lt; batchSize &amp;&amp; iterator.hasNext()) {\n                batch.add(iterator.next());\n                count++;\n            }\n            if (batch.isEmpty()) {\n                return false;\n            }\n            action.accept(batch);\n            return true;\n        }\n    }, false);\n}\n</code></pre>\n<p>Now, you could do something like this:</p>\n<pre class=\"lang-java prettyprint-override\"><code>try(Stream&lt;MyEntity&gt; myStream = postRepository.streamByCreatedOnSince(yesterday)) {   \n    batchStream(myStream, 100)\n        .map(this::process)\n        .forEach(postRepository::save);\n}\n</code></pre>\n",
    "score" : 1,
    "is_accepted" : false,
    "owner" : {
      "account_id" : 1601399,
      "reputation" : 638,
      "user_id" : 1482356,
      "user_type" : "registered",
      "profile_image" : "https://www.gravatar.com/avatar/d6160b6e40d230fa92c6da8072b2f7e5?s=256&d=identicon&r=PG",
      "display_name" : "Peter Adrian",
      "link" : "https://stackoverflow.com/users/1482356/peter-adrian"
    },
    "creation_date" : 1751580687,
    "last_activity_date" : 1751580687,
    "content_license" : "CC BY-SA 4.0"
  } ],
  "question_comments" : [ ],
  "answer_comments" : {
    "79689501" : [ {
      "comment_id" : 140590456,
      "post_id" : 79689501,
      "body" : "It would be better to use <code>stream.spliterator()</code>. Then, you can provide a size estimate and the right characteristics for the new stream.",
      "score" : 1,
      "owner" : {
        "account_id" : 3211603,
        "reputation" : 300981,
        "user_id" : 2711488,
        "user_type" : "registered",
        "profile_image" : "https://i.sstatic.net/t2hoD.jpg?s=256",
        "display_name" : "Holger",
        "link" : "https://stackoverflow.com/users/2711488/holger"
      },
      "creation_date" : 1752572845,
      "content_license" : "CC BY-SA 4.0"
    }, {
      "comment_id" : 140570171,
      "post_id" : 79689501,
      "body" : "You should not use parallel streams on DB. Gatherers are available in Java 24. Usually, organisations don&#39;t go to the last oracle version, they stay on the ones they bought support for. So, if you have java 24, then use gather, otherwise, the solution is safe. PS: Even with gather, if you collect to list, then it will collect everything. If you really need to have more control, and do parallel stuff and faster,  go for reactive Flux, that is also supported by spring data, and you would have a lot more power than java stream have, but also more problems to solve.",
      "score" : 0,
      "owner" : {
        "account_id" : 1601399,
        "reputation" : 638,
        "user_id" : 1482356,
        "user_type" : "registered",
        "profile_image" : "https://www.gravatar.com/avatar/d6160b6e40d230fa92c6da8072b2f7e5?s=256&d=identicon&r=PG",
        "display_name" : "Peter Adrian",
        "link" : "https://stackoverflow.com/users/1482356/peter-adrian"
      },
      "creation_date" : 1751880918,
      "content_license" : "CC BY-SA 4.0"
    }, {
      "comment_id" : 140569690,
      "post_id" : 79689501,
      "body" : "This is a pretty dangerous solution. I tried something similar, and ran into all sorts of hidden land mines. Here is one of them -- <a href=\"https://old.reddit.com/r/java/comments/1gukzhb/\" rel=\"nofollow noreferrer\">old.reddit.com/r/java/comments/1gukzhb</a> -- If you really want to batch, I&#39;d use the paginate functionality. Or maybe Stream <a href=\"https://openjdk.org/jeps/485\" rel=\"nofollow noreferrer\">Gatherers</a>.",
      "score" : 0,
      "owner" : {
        "account_id" : 14010051,
        "reputation" : 250,
        "user_id" : 10118965,
        "user_type" : "registered",
        "profile_image" : "https://www.gravatar.com/avatar/61afd55894624ae8f6e4189b68db1f30?s=256&d=identicon&r=PG&f=y&so-version=2",
        "display_name" : "davidalayachew",
        "link" : "https://stackoverflow.com/users/10118965/davidalayachew"
      },
      "creation_date" : 1751867505,
      "content_license" : "CC BY-SA 4.0"
    }, {
      "comment_id" : 140565775,
      "post_id" : 79689501,
      "body" : "It keeps the stream open until everything is processed. It will process 100 items at a time until all the records are processed. The stream api will call the tryAdvance method untill it returns false. The consumer in the method is the downstream subscriber.",
      "score" : 0,
      "owner" : {
        "account_id" : 1601399,
        "reputation" : 638,
        "user_id" : 1482356,
        "user_type" : "registered",
        "profile_image" : "https://www.gravatar.com/avatar/d6160b6e40d230fa92c6da8072b2f7e5?s=256&d=identicon&r=PG",
        "display_name" : "Peter Adrian",
        "link" : "https://stackoverflow.com/users/1482356/peter-adrian"
      },
      "creation_date" : 1751654003,
      "content_license" : "CC BY-SA 4.0"
    }, {
      "comment_id" : 140565332,
      "post_id" : 79689501,
      "body" : "Thanks for the help.  Does this solution iterate through the entire stream (so 200k rows) before breaking into smaller streams, or does it create batches as it goes, so only &quot;reading&quot; a batches worth of records before they are passed on to be processed by .map(this::process) step?",
      "score" : 0,
      "owner" : {
        "account_id" : 1073482,
        "reputation" : 12771,
        "user_id" : 1072187,
        "user_type" : "registered",
        "accept_rate" : 36,
        "profile_image" : "https://www.gravatar.com/avatar/6a15142de5577986384774484ff25fe6?s=256&d=identicon&r=PG",
        "display_name" : "John Little",
        "link" : "https://stackoverflow.com/users/1072187/john-little"
      },
      "creation_date" : 1751641029,
      "content_license" : "CC BY-SA 4.0"
    } ]
  }
}