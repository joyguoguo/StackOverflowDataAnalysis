{
  "question" : {
    "question_id" : 79600594,
    "title" : "IgniteDataStreamer resulting in Partition states validation has failed/Partitions update counters are inconsistent",
    "body" : "<p>What is the significance of this <code>WARN</code> logging in Apache Ignite?</p>\n<pre class=\"lang-none prettyprint-override\"><code>WARN  [sys-#78%IgniteInstance1%] (Log4J2Logger.java:523) Partition states validation has failed for group: Cache1, msg: Partitions update counters are inconsistent for Part 32...\n</code></pre>\n<p>I'm trying to use multiple <code>IgniteDataStreamer</code> instances to preload a cache after my cluster starts up as described <a href=\"https://stackoverflow.com/questions/79573850/multiple-ignitedatastreamer-instances\">here</a>, and I've put together this <a href=\"https://github.com/jvfullam/ignite-data-streamer-example\" rel=\"nofollow noreferrer\">minified example</a> to reproduce what I'm seeing. Each <code>IgniteRunnable/IgniteDataStreamer</code> streams a unique set of keys into the cache.</p>\n<p>Essentially, I have these two classes:</p>\n<h2>IgniteServerMain.java</h2>\n<pre class=\"lang-java prettyprint-override\"><code>public class IgniteServerMain {\n    private static final Logger log = LogManager.getLogger();\n\n    public static final String CACHE_NAME = &quot;Cache1&quot;;\n    public static final String IGNITE_INSTANCE_NAME = &quot;IgniteInstance1&quot;;\n    private static final String NODE_ID = System.getProperty(&quot;NODE_ID&quot;);\n    private static final int SERVER_NODES = 2;\n\n    public static void main(String[] args) {\n        try {\n            Ignition.start(getIgniteConfiguration());\n\n            if (&quot;1&quot;.equals(NODE_ID)) {\n                waitForServerNodesToBeAvailable();\n                loadCache();\n                idleVerifyLoop();\n            }\n\n            while (true) {\n                sleep(10_000);\n            }\n        } catch (Exception e) {\n            log.error(&quot;{}&quot;, e, e);\n            System.exit(1);\n        }\n    }\n\n    private static IgniteConfiguration getIgniteConfiguration() {\n        CacheConfiguration&lt;String, Integer&gt; cacheConfig = new CacheConfiguration&lt;String, Integer&gt;()\n                .setAtomicityMode(CacheAtomicityMode.TRANSACTIONAL)\n                .setBackups(1)\n                .setCacheMode(CacheMode.PARTITIONED)\n                .setName(CACHE_NAME)\n                .setReadThrough(false)\n                .setWriteSynchronizationMode(CacheWriteSynchronizationMode.PRIMARY_SYNC);\n\n        DataRegionConfiguration defaultDataRegionConfiguration = new DataRegionConfiguration()\n                .setName(&quot;Default_Region&quot;)\n                .setMaxSize(1L * 1024 * 1024 * 1024)\n                .setInitialSize(1L * 1024 * 1024 * 1024);\n\n        DataStorageConfiguration dataStorageConfiguration = new DataStorageConfiguration()\n                .setDefaultDataRegionConfiguration(defaultDataRegionConfiguration);\n\n        TcpCommunicationSpi tcpCommunicationSpi = new TcpCommunicationSpi();\n\n        TcpDiscoveryVmIpFinder tcpDiscoveryVmIpFinder = new TcpDiscoveryVmIpFinder()\n                .setAddresses(List.of(&quot;127.0.0.1:47500..47509&quot;));\n\n        TcpDiscoverySpi tcpDiscoverySpi = new TcpDiscoverySpi()\n                .setIpFinder(tcpDiscoveryVmIpFinder);\n\n        return new IgniteConfiguration()\n                .setCacheConfiguration(cacheConfig)\n                .setCommunicationSpi(tcpCommunicationSpi)\n                .setDataStorageConfiguration(dataStorageConfiguration)\n                .setDiscoverySpi(tcpDiscoverySpi)\n                .setIgniteInstanceName(IGNITE_INSTANCE_NAME)\n                .setIncludeEventTypes(EventType.EVT_NODE_FAILED);\n    }\n\n    private static void waitForServerNodesToBeAvailable() {\n        log.info(&quot;waiting for server nodes&quot;);\n        Ignite ignite = Ignition.ignite(IGNITE_INSTANCE_NAME);\n        while (ignite.cluster().forServers().nodes().size() &lt; SERVER_NODES) {\n            sleep(1);\n        }\n\n        log.info(&quot;server nodes are available!&quot;);\n        sleep(100);\n    }\n\n    private static void loadCache() {\n        Instant start = Instant.now();\n        log.info(&quot;starting {}&quot;, CACHE_NAME);\n\n        Ignite ignite = Ignition.ignite(IGNITE_INSTANCE_NAME);\n        IgniteCompute compute = ignite.compute();\n        List&lt;IgniteFuture&lt;Void&gt;&gt; jobs = IntStream.range(0, 100)\n                .mapToObj(PreloadRunnable::new)\n                .map(compute::runAsync)\n                .collect(Collectors.toList());\n\n        jobs.forEach(IgniteFuture::get);\n\n        Duration duration = Duration.between(start, Instant.now());\n        log.info(&quot;finished: duration {}&quot;, duration);\n\n        int size = ignite.cache(CACHE_NAME).size();\n        log.info(&quot;cache {}, size {}&quot;, CACHE_NAME, size);\n    }\n\n    private static boolean idleVerify() {\n        Instant start = Instant.now();\n\n        MBeanServer mbs = ManagementFactory.getPlatformMBeanServer();\n        ObjectName objectName = mbs.queryMBeans(null, null).stream()\n                .filter(objectInstance -&gt; objectInstance.toString().contains(&quot;name=IdleVerify&quot;))\n                .map(objectInstance -&gt; objectInstance.getObjectName())\n                .findFirst()\n                .orElseThrow(() -&gt; new IllegalStateException(&quot;IdleVerify: MBean not found&quot;));\n\n        try {\n            String result = (String) mbs.invoke(objectName, &quot;invoke&quot;,\n                    new Object[] { &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot; },\n                    new String[] {});\n            log.info(&quot;IdleVerify: finished in {}&quot;, Duration.between(start, Instant.now()));\n\n            String[] resultSplit = result.split(&quot;\\\\R&quot;);\n            Stream.of(resultSplit).forEach(s -&gt; log.info(&quot;IdleVerify: {}&quot;, s));\n            return &quot;The check procedure has finished, no conflicts have been found.&quot;\n                    .equals(resultSplit[resultSplit.length - 1]);\n        } catch (Exception e) {\n            log.error(e.toString(), e);\n            return false;\n        }\n    }\n\n    private static void idleVerifyLoop() {\n        Instant start = Instant.now();\n\n        while (!idleVerify()) {\n            if (Duration.between(start, Instant.now()).getSeconds() &gt; 120) {\n                log.error(&quot;IdleVerifyLoop: exiting, there are still conflicts after 2 minutes of waiting&quot;);\n                return;\n            }\n            sleep(10_000);\n        }\n\n        Duration duration = Duration.between(start, Instant.now());\n        log.info(&quot;IdleVerifyLoop: finished in {}&quot;, duration);\n    }\n\n    private static void sleep(long millis) {\n        try {\n            Thread.sleep(millis);\n        } catch (InterruptedException e) {\n            log.error(e.toString(), e);\n            Thread.interrupted();\n            throw new RuntimeException(e);\n        }\n    }\n}\n</code></pre>\n<h2>PreloadRunnable.java</h2>\n<pre class=\"lang-java prettyprint-override\"><code>public class PreloadRunnable implements IgniteRunnable {\n    private static final long serialVersionUID = 1L;\n\n    private final int jobId;\n    private final Random random = new Random();\n\n    public PreloadRunnable(int jobId) {\n        this.jobId = jobId;\n    }\n\n    @Override\n    public void run() {\n        try (IgniteDataStreamer&lt;String, Integer&gt; streamer = Ignition\n                .ignite(IgniteServerMain.IGNITE_INSTANCE_NAME)\n                .dataStreamer(IgniteServerMain.CACHE_NAME)) {\n            \n            for (int v = 0; v &lt; 10_000; v++) {\n                char randomLetter = (char) ('A' + random.nextInt(26));\n                String k = randomLetter + &quot;-&quot; + String.format(&quot;%06d&quot;, jobId) + &quot;-&quot; + String.format(&quot;%06d&quot;, v);\n                streamer.addData(k, v);\n            }\n        }\n    }\n}\n</code></pre>\n<p>A few scenarios:</p>\n<ol>\n<li>I observe the <code>WARN</code> logging whenever I start <code>Node 1</code> <em>before</em> <code>Node 2</code>. In this scenario, Node 1 starts streaming data into the cache soon after it detects that <code>Node 2</code> has joined the cluster, and the data streaming interleaves with a Partition Map Exchange.</li>\n<li>If I change the pause at the end of <code>waitForServerNodesToBeAvailable</code> from 100 milliseconds to 10 seconds, then I do <em>not</em> see the <code>WARN</code> logging. This gives the initial PME time to finish prior to <code>loadCache</code>.</li>\n<li>Similarly, if I set the pause back to 100 ms and start Node 1 <em>after</em> <code>Node 2</code>, then Node 1's <code>Ignition.start</code> method doesn't return until <em>after</em> the initial PME has finished, and so I also do <em>not</em> see the WARN logging in this scenario.</li>\n</ol>\n<p>In my sample code, I've added a loop after <code>loadCache</code> which programmatically calls Ignite's <code>IdleVerify</code> JMX method every 10 seconds. In scenario 1, the first <code>IdleVerify</code> call confirms that update counters are inconsistent. A bit later, I typically see logging which indicates that the PME has finished. And then, when the second <code>IdleVerify</code> call runs, it reports that no conflicts have been found. So it seems that these partition update counters are <em>eventually consistent</em>. I also observed that if I increase the amount of data that <code>loadCache</code> streams into the cache, then it takes <em>longer</em> for the initial PME to finish, and as a result it can take many iterations of my <code>IdleVerify</code> loop before it reports that there are no conflicts.</p>\n<p>Questions:</p>\n<ol>\n<li>What is going on here? The <a href=\"https://ignite.apache.org/releases/latest/javadoc/org/apache/ignite/IgniteDataStreamer.html\" rel=\"nofollow noreferrer\">IgniteDataStreamer API</a> states that &quot;data streamer doesnâ€™t guarantee [...] data consistency until successfully finished&quot; but this seems to contradict the inconsistent update counters I'm seeing.</li>\n<li>What is the significance of these inconsistent update counters? Is there a possibility of data loss when we're in this state? And is it correct that these update counters will always be eventually consistent?</li>\n<li>Is there a better way for <code>Node 1</code> to wait for the other node(s) to join and ensure that the initial PME finishes before it starts streaming data into the cache? Assuming that the topology is stable (this is possibly a bad assumption), this seems to avoid the whole issue with inconsistent update counters.</li>\n</ol>\n",
    "tags" : [ "java", "ignite", "apacheignite" ],
    "owner" : {
      "account_id" : 8648861,
      "reputation" : 13,
      "user_id" : 6475253,
      "user_type" : "registered",
      "profile_image" : "https://www.gravatar.com/avatar/27714d04cd1c05c0a5c7d32dc41244dd?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name" : "jvfullam",
      "link" : "https://stackoverflow.com/users/6475253/jvfullam"
    },
    "is_answered" : false,
    "view_count" : 86,
    "answer_count" : 0,
    "score" : 0,
    "last_activity_date" : 1746215124,
    "creation_date" : 1746025405,
    "link" : "https://stackoverflow.com/questions/79600594/ignitedatastreamer-resulting-in-partition-states-validation-has-failed-partition",
    "content_license" : "CC BY-SA 4.0"
  },
  "answers" : [ ],
  "question_comments" : [ ],
  "answer_comments" : { }
}