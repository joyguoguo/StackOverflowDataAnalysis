{
  "question" : {
    "question_id" : 79586452,
    "title" : "Why are thread pools so significantly slower than parallelStream for this scenario?",
    "body" : "<p>This is for Java 23, but I've replicated this on Java 21 and 17 as well.</p>\n<p>I recently ran a benchmark to compare performance differences between threadpools and parallelstreams for some simple calculations.</p>\n<p>I've set up JMH to compare the following:</p>\n<ul>\n<li><code>Executors.newFixedThreadPool</code></li>\n<li><code>Executors.newWorkStealingPool</code></li>\n<li>A manually generated <code>ForkJoinPool</code></li>\n<li><code>ForkJoinPool.commonPool</code></li>\n<li>A <code>parallelStream()</code> implementation, and</li>\n<li>A sequential loop to act as a base case</li>\n</ul>\n<p>And this is the calculation I'm benchmarking:</p>\n<pre class=\"lang-java prettyprint-override\"><code>    @State(Scope.Benchmark)\n    private static class StateData{\n        public static final List&lt;ContainerClass&gt; models = IntStream.range(1,1000_001)\n                .mapToObj(x-&gt;{\n                    double length = 10.0; // meters\n                    // ContainerClass is a record with two double type args\n                    return new ContainerClass(beamLength, x);\n                }).toList();\n    }\n\n    Callable&lt;Double&gt; getCallable(ContainerClass x){\n        return ()-&gt; x.length()*x.load()/2.0;\n    }\n</code></pre>\n<p>The benchmark code is below:</p>\n<pre class=\"lang-java prettyprint-override\"><code>    @Benchmark\n    @Fork(value = 1)\n    @Warmup(iterations = 5)\n    @Measurement(iterations = 5)\n    public void testingParallelStream_toList(Blackhole bh){\n        var midMoments = StateData.models.parallelStream()\n                .unordered()\n                .map(x-&gt; x.load()*x.length()/2.0).toList();\n\n        bh.consume(midMoments);\n    }\n\n    @Benchmark\n    @Fork(value = 1)\n    @Warmup(iterations = 5)\n    @Measurement(iterations = 5)\n    public void testingSequential(Blackhole bh) {\n        List&lt;Double&gt; results = new ArrayList&lt;&gt;();\n        for(var x: StateData.models){\n            var result = x.load()*x.length()/2.0;\n            results.add(result);\n        }\n        bh.consume(results);\n    }\n\n    @Benchmark\n    @Fork(value = 1)\n    @Warmup(iterations = 5)\n    @Measurement(iterations = 5)\n    public void testingExecutorService_FixedThreadPool(Blackhole bh) throws InterruptedException {\n        var pool = Executors.newFixedThreadPool(Runtime.getRuntime().availableProcessors());\n        runPool(bh, pool);\n    }\n\n    @Benchmark\n    @Fork(value = 1)\n    @Warmup(iterations = 5)\n    @Measurement(iterations = 5)\n    public void testingExecutorService_WorkStealingPool(Blackhole bh) throws InterruptedException {\n        var pool = Executors.newWorkStealingPool(Runtime.getRuntime().availableProcessors());\n        runPool(bh, pool);\n    }\n\n    @Benchmark\n    @Fork(value = 1)\n    @Warmup(iterations = 5)\n    @Measurement(iterations = 5)\n    public void testingManualFJPool(Blackhole bh) throws InterruptedException, ExecutionException {\n        var pool = new ForkJoinPool();\n        runPool(bh, pool);\n    }\n\n    @Benchmark\n    @Fork(value = 1)\n    @Warmup(iterations = 5)\n    @Measurement(iterations = 5)\n    public void testingCommonFJPool(Blackhole bh) throws InterruptedException, ExecutionException {\n        var pool = ForkJoinPool.commonPool();\n        List&lt;Double&gt; results = new ArrayList&lt;&gt;();\n\n        List&lt;Callable&lt;Double&gt;&gt; callables = new ArrayList&lt;&gt;();\n\n        for(var m: StateData.models){\n            callables.add(getCallable(m));\n        }\n\n        var futures = pool.invokeAll(callables);\n        boolean isFinished = pool.awaitQuiescence(60L, TimeUnit.MINUTES);\n        if(!isFinished){\n            throw new IllegalArgumentException(&quot;Timeout&quot;);\n        } else {\n            for(var future: futures){\n                results.add(future.get());\n            }\n        }\n        bh.consume(results);\n    }\n    private void runPool(Blackhole bh, ExecutorService pool) throws InterruptedException {\n        List&lt;Double&gt; results = new ArrayList&lt;&gt;();\n\n        List&lt;Callable&lt;Double&gt;&gt; callables = new ArrayList&lt;&gt;();\n\n        for(var m: StateData.models){\n            callables.add(getCallable(m));\n        }\n\n        var futures = pool.invokeAll(callables);\n        // wait for thread runs\n        pool.shutdown();\n        try{\n            boolean isFinished = pool.awaitTermination(60L, TimeUnit.MINUTES);\n            if(isFinished){\n                for(var future: futures){\n                    results.add(future.get());\n                }\n                bh.consume(results);\n            } else {\n                throw new IllegalArgumentException(&quot;Timeout occurred before all threads could finish&quot;);\n            }\n        } catch(Exception e){\n            throw new IllegalArgumentException(e);\n        }\n    }\n</code></pre>\n<p>I expected the threadpools to be slower than parallelstream due to better task splitting in the latter, but the results are a bit surprising:</p>\n<pre class=\"lang-none prettyprint-override\"><code>Benchmark                                                                  Mode  Cnt   Score   Error  Units\nFunctionalVsImperative.PerfTests.testingCommonFJPool                      thrpt    5  10.859 ± 0.937  ops/s\nFunctionalVsImperative.PerfTests.testingExecutorService_FixedThreadPool   thrpt    5   5.605 ± 0.907  ops/s\nFunctionalVsImperative.PerfTests.testingExecutorService_WorkStealingPool  thrpt    5  10.278 ± 0.430  ops/s\nFunctionalVsImperative.PerfTests.testingManualFJPool                      thrpt    5   9.875 ± 1.709  ops/s\nFunctionalVsImperative.PerfTests.testingParallelStream_toList             thrpt    5  74.648 ± 4.755  ops/s\nFunctionalVsImperative.PerfTests.testingSequential                        thrpt    5  46.895 ± 6.828  ops/s\n</code></pre>\n<p>Not only are threadpools much slower than <code>parallelStream</code>, they're even slower than the base sequential loop! I've tested this across 1000, 10000 and 100000 tasks/iterations and the trend remains the same.</p>\n<p>(I'll note that for smaller iterations, the sequential version is faster than <code>parallelStream</code> and that's to be expected. However, for this specific case (and for larger iterations), <code>parallelStream</code> is faster than sequential, so presumedly there are enough computations that the overhead of creating threads is smaller than the benefit of parallel runs)</p>\n<p>Does anyone have an idea as to why this benchmark is behaving this way ? If it helps, I'm running this on a Core i7 10850H system with 12 available CPUs (hexcore + hyperthreading).</p>\n<p><strong>EDIT (23/04/25)</strong> : following some discussion in the comments (see below) I modified my <code>State</code> class like so:</p>\n<pre class=\"lang-java prettyprint-override\"><code>    @State(Scope.Benchmark)\n    private static class StateData{\n        public static final List&lt;ContainerClass&gt; models = IntStream.range(1,1000_001)\n                .mapToObj(x-&gt;{\n                    double beamLength = 10.0; // meters\n                    return new ContainerClass(beamLength, x);\n                }).toList();\n\n        public static final List&lt;Callable&lt;Double&gt;&gt; getCallables(){\n            List&lt;Callable&lt;Double&gt;&gt; callables = new ArrayList&lt;&gt;();\n            for(var m: StateData.models){\n                callables.add(StateData.getCallable(m));\n            }\n            return callables;\n        }\n\n        public static Callable&lt;Double&gt; getCallable(ContainerClass x){\n            return ()-&gt; x.length()*x.load()/2.0;\n        }\n    }\n</code></pre>\n<p>The intent was to remove any overhead incurred from instantiating one million <code>Callable</code> objects. The <code>runPool()</code> method was also modified:</p>\n<pre><code>private void runPool(Blackhole bh, ExecutorService pool) throws InterruptedException {\n        List&lt;Double&gt; results = new ArrayList&lt;&gt;();\n        var futures = pool.invokeAll(StateData.getCallables());\n        // ... rest of the code\n}\n</code></pre>\n<p>Unfortunately, this did absolutely nothing to improve the benchmark, the results of which remain very similar to what I was seeing before (which is why I haven't added them here)</p>\n",
    "tags" : [ "java", "threadpool", "executorservice", "parallelstream" ],
    "owner" : {
      "account_id" : 11512926,
      "reputation" : 123,
      "user_id" : 10560184,
      "user_type" : "registered",
      "profile_image" : "https://www.gravatar.com/avatar/3399de3e1ec229ab54f7a9926b6a35b8?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name" : "Andorrax",
      "link" : "https://stackoverflow.com/users/10560184/andorrax"
    },
    "is_answered" : false,
    "view_count" : 138,
    "answer_count" : 0,
    "score" : 3,
    "last_activity_date" : 1745405939,
    "creation_date" : 1745324601,
    "link" : "https://stackoverflow.com/questions/79586452/why-are-thread-pools-so-significantly-slower-than-parallelstream-for-this-scenar",
    "content_license" : "CC BY-SA 4.0"
  },
  "answers" : [ ],
  "question_comments" : [ {
    "comment_id" : 140361894,
    "post_id" : 79586452,
    "body" : "Thanks guys. All of the overheads mentioned being the main culprits for slowness seems logical...though I&#39;d be keen to do some more profiling to see if they indeed are. I&#39;ll try some other stuff on the side and see if I can&#39;t nail it down further. Thanks again !",
    "score" : 0,
    "owner" : {
      "account_id" : 11512926,
      "reputation" : 123,
      "user_id" : 10560184,
      "user_type" : "registered",
      "profile_image" : "https://www.gravatar.com/avatar/3399de3e1ec229ab54f7a9926b6a35b8?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name" : "Andorrax",
      "link" : "https://stackoverflow.com/users/10560184/andorrax"
    },
    "creation_date" : 1745419635,
    "content_license" : "CC BY-SA 4.0"
  }, {
    "comment_id" : 140361795,
    "post_id" : 79586452,
    "body" : "@Andorrax, the <i>creation</i> of those Callables is only one of the things that is costly for your thread pool approaches.  It&#39;s probably not the most costly one.  Again, thread pools (and multithreading in general) are not well suited to large numbers of tiny tasks.  If I were to guess (dangerous when discussing performance) I would say that the most costly thing is probably all the synchronization overhead surrounding enqueuing and dequeuing the tasks.",
    "score" : 0,
    "owner" : {
      "account_id" : 2792262,
      "reputation" : 190832,
      "user_id" : 2402272,
      "user_type" : "registered",
      "accept_rate" : 85,
      "profile_image" : "https://www.gravatar.com/avatar/1182b1d5518a596d4e8cfe0567a65c4d?s=256&d=identicon&r=PG",
      "display_name" : "John Bollinger",
      "link" : "https://stackoverflow.com/users/2402272/john-bollinger"
    },
    "creation_date" : 1745418077,
    "content_license" : "CC BY-SA 4.0"
  }, {
    "comment_id" : 140361775,
    "post_id" : 79586452,
    "body" : "I agree the callable instantiations are probably not the cause of the slowness, but probably everything else mentioned here.",
    "score" : 0,
    "owner" : {
      "account_id" : 19118721,
      "reputation" : 12469,
      "user_id" : 13963086,
      "user_type" : "registered",
      "profile_image" : "https://www.gravatar.com/avatar/9df2c3c4c87d8050b8fd6a59c88c7133?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name" : "k314159",
      "link" : "https://stackoverflow.com/users/13963086/k314159"
    },
    "creation_date" : 1745417832,
    "content_license" : "CC BY-SA 4.0"
  }, {
    "comment_id" : 140361756,
    "post_id" : 79586452,
    "body" : "Apologies if I sound like a broken record. I&#39;m mostly just curious about what specifically is causing the slowness (callable instantiation was noted as a probable contributor, but if my understanding of JMH is correct then it seems it might not be)",
    "score" : 0,
    "owner" : {
      "account_id" : 11512926,
      "reputation" : 123,
      "user_id" : 10560184,
      "user_type" : "registered",
      "profile_image" : "https://www.gravatar.com/avatar/3399de3e1ec229ab54f7a9926b6a35b8?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name" : "Andorrax",
      "link" : "https://stackoverflow.com/users/10560184/andorrax"
    },
    "creation_date" : 1745417431,
    "content_license" : "CC BY-SA 4.0"
  }, {
    "comment_id" : 140361747,
    "post_id" : 79586452,
    "body" : "@JohnBollinger I see. One question though - in moving the <code>getCallable()</code> method over to the <code>StateData</code> class, my understanding is that JMH would create all the callable objects before running the benchmark - thereby removing at least one potential bottleneck. I agree that the enqueuing and dequeuing ops, along with synchronization overhead will slow things down, but I would have expected at least a small speed up from the move...unless each <code>Callable</code> is created only after being passed into <code>invokeAll()</code>? Or maybe the slowness is mostly due to the queuing and synchro ?",
    "score" : 0,
    "owner" : {
      "account_id" : 11512926,
      "reputation" : 123,
      "user_id" : 10560184,
      "user_type" : "registered",
      "profile_image" : "https://www.gravatar.com/avatar/3399de3e1ec229ab54f7a9926b6a35b8?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name" : "Andorrax",
      "link" : "https://stackoverflow.com/users/10560184/andorrax"
    },
    "creation_date" : 1745417270,
    "content_license" : "CC BY-SA 4.0"
  }, {
    "comment_id" : 140361615,
    "post_id" : 79586452,
    "body" : "[...] Parallel streams work better for you because as far as their use of threads goes, they break up the work into a small number of much larger tasks.  Additionally in this case, with a parallel stream, the same one object represents the per item work for every item, as opposed to there being a separate object for each item to represent the work to be done for that item.",
    "score" : 1,
    "owner" : {
      "account_id" : 2792262,
      "reputation" : 190832,
      "user_id" : 2402272,
      "user_type" : "registered",
      "accept_rate" : 85,
      "profile_image" : "https://www.gravatar.com/avatar/1182b1d5518a596d4e8cfe0567a65c4d?s=256&d=identicon&r=PG",
      "display_name" : "John Bollinger",
      "link" : "https://stackoverflow.com/users/2402272/john-bollinger"
    },
    "creation_date" : 1745415420,
    "content_license" : "CC BY-SA 4.0"
  }, {
    "comment_id" : 140361605,
    "post_id" : 79586452,
    "body" : "@Andorrax, I don&#39;t think you&#39;ve really understood what we&#39;re trying to tell you.  In particular, moving <code>getCallable()</code> to a different class does not change the fact that you are generating a separate <code>Callable</code> for every single task, and that you are both enqueueing and dequeueing each one, and that you have synchronization overhead associated with each one.  Thread pools simply are not an efficient choice for large numbers of tiny tasks. [...]",
    "score" : 1,
    "owner" : {
      "account_id" : 2792262,
      "reputation" : 190832,
      "user_id" : 2402272,
      "user_type" : "registered",
      "accept_rate" : 85,
      "profile_image" : "https://www.gravatar.com/avatar/1182b1d5518a596d4e8cfe0567a65c4d?s=256&d=identicon&r=PG",
      "display_name" : "John Bollinger",
      "link" : "https://stackoverflow.com/users/2402272/john-bollinger"
    },
    "creation_date" : 1745415230,
    "content_license" : "CC BY-SA 4.0"
  }, {
    "comment_id" : 140361066,
    "post_id" : 79586452,
    "body" : "@AndrewS - interesting thought. I&#39;ll give it a try later today",
    "score" : 0,
    "owner" : {
      "account_id" : 11512926,
      "reputation" : 123,
      "user_id" : 10560184,
      "user_type" : "registered",
      "profile_image" : "https://www.gravatar.com/avatar/3399de3e1ec229ab54f7a9926b6a35b8?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name" : "Andorrax",
      "link" : "https://stackoverflow.com/users/10560184/andorrax"
    },
    "creation_date" : 1745405743,
    "content_license" : "CC BY-SA 4.0"
  }, {
    "comment_id" : 140361065,
    "post_id" : 79586452,
    "body" : "Thanks for the great discussion points all !  @JohnBollinger, @k314159 and @rzwitserloot - I tried moving the <code>getCallable()</code> method - along with its accompaniments - over to the <code>State</code> class to remove any overhead from instantiating the <code>Callable</code> objects...Unfortunately no dice. I&#39;m seeing very nearly the same JMH results as I was before.  So unless I&#39;ve done something very horribly wrong in my code, it seems the cause of the slowness lies elsewhere ?",
    "score" : 0,
    "owner" : {
      "account_id" : 11512926,
      "reputation" : 123,
      "user_id" : 10560184,
      "user_type" : "registered",
      "profile_image" : "https://www.gravatar.com/avatar/3399de3e1ec229ab54f7a9926b6a35b8?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name" : "Andorrax",
      "link" : "https://stackoverflow.com/users/10560184/andorrax"
    },
    "creation_date" : 1745405714,
    "content_license" : "CC BY-SA 4.0"
  }, {
    "comment_id" : 140358226,
    "post_id" : 79586452,
    "body" : "Side note: the work-stealing function of a work-stealing pool requires the participation of the tasks being run.  Such a pool works like an ordinary fork-join pool for other tasks, such as the ones you&#39;re giving yours.",
    "score" : 0,
    "owner" : {
      "account_id" : 2792262,
      "reputation" : 190832,
      "user_id" : 2402272,
      "user_type" : "registered",
      "accept_rate" : 85,
      "profile_image" : "https://www.gravatar.com/avatar/1182b1d5518a596d4e8cfe0567a65c4d?s=256&d=identicon&r=PG",
      "display_name" : "John Bollinger",
      "link" : "https://stackoverflow.com/users/2402272/john-bollinger"
    },
    "creation_date" : 1745334645,
    "content_license" : "CC BY-SA 4.0"
  }, {
    "comment_id" : 140358192,
    "post_id" : 79586452,
    "body" : "... and each of those callables must initially be enqueued and subsequently be individually dequeued.  With the dequeueing contested by the pooled threads at least some of the time.  This is yet another place where the thread pool approaches tested have significant overhead.  Parallel streams take a more efficient approach to splitting the work among the supporting threads.",
    "score" : 2,
    "owner" : {
      "account_id" : 2792262,
      "reputation" : 190832,
      "user_id" : 2402272,
      "user_type" : "registered",
      "accept_rate" : 85,
      "profile_image" : "https://www.gravatar.com/avatar/1182b1d5518a596d4e8cfe0567a65c4d?s=256&d=identicon&r=PG",
      "display_name" : "John Bollinger",
      "link" : "https://stackoverflow.com/users/2402272/john-bollinger"
    },
    "creation_date" : 1745334225,
    "content_license" : "CC BY-SA 4.0"
  }, {
    "comment_id" : 140358134,
    "post_id" : 79586452,
    "body" : "With ParallelStream, you only have one <code>Callable</code> instance, and you are using that single instance to calculate a result for each <code>ContainerClass</code> element of the list. By contrast, with the thread pools, you are creating 1 million callables, and measuring the time it takes to create them in addition to the calculation.",
    "score" : 1,
    "owner" : {
      "account_id" : 19118721,
      "reputation" : 12469,
      "user_id" : 13963086,
      "user_type" : "registered",
      "profile_image" : "https://www.gravatar.com/avatar/9df2c3c4c87d8050b8fd6a59c88c7133?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name" : "k314159",
      "link" : "https://stackoverflow.com/users/13963086/k314159"
    },
    "creation_date" : 1745333338,
    "content_license" : "CC BY-SA 4.0"
  }, {
    "comment_id" : 140358063,
    "post_id" : 79586452,
    "body" : "@rzwitserloot yes, I agree, but if I knew for certain if it indeed was allocating on the heap and why the lambda in ParallelStream didn&#39;t have the same problem, I would have written an answer instead of a comment.",
    "score" : 0,
    "owner" : {
      "account_id" : 19118721,
      "reputation" : 12469,
      "user_id" : 13963086,
      "user_type" : "registered",
      "profile_image" : "https://www.gravatar.com/avatar/9df2c3c4c87d8050b8fd6a59c88c7133?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name" : "k314159",
      "link" : "https://stackoverflow.com/users/13963086/k314159"
    },
    "creation_date" : 1745332087,
    "content_license" : "CC BY-SA 4.0"
  }, {
    "comment_id" : 140357933,
    "post_id" : 79586452,
    "body" : "@k314159 That presupposes, I guess, that the <code>getCallable</code> method actually &quot;allocates things on the heap&quot;. I was under the impression that this isn&#39;t necessarily the case (presumably the JMH results are strongly indicative that it <i>is</i>, but, then, it requires some explanation as to <i>why</i> that is. And why the lambda in the ParallelStream_toList example <i>does not</i> do that).",
    "score" : 0,
    "owner" : {
      "account_id" : 401843,
      "reputation" : 107186,
      "user_id" : 768644,
      "user_type" : "registered",
      "profile_image" : "https://www.gravatar.com/avatar/b13bedc5215730fbce5edff6c130988a?s=256&d=identicon&r=PG",
      "display_name" : "rzwitserloot",
      "link" : "https://stackoverflow.com/users/768644/rzwitserloot"
    },
    "creation_date" : 1745329929,
    "content_license" : "CC BY-SA 4.0"
  }, {
    "comment_id" : 140357915,
    "post_id" : 79586452,
    "body" : "The time it takes to create a <code>Callable</code> (which involves memory allocation on the heap) and the time it takes to add it to the <code>callables</code> list is <i>far</i> greater than the time it takes for the <code>Callable</code> to do its calculation. You are therefore measuring the wrong thing.",
    "score" : 1,
    "owner" : {
      "account_id" : 19118721,
      "reputation" : 12469,
      "user_id" : 13963086,
      "user_type" : "registered",
      "profile_image" : "https://www.gravatar.com/avatar/9df2c3c4c87d8050b8fd6a59c88c7133?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name" : "k314159",
      "link" : "https://stackoverflow.com/users/13963086/k314159"
    },
    "creation_date" : 1745329580,
    "content_license" : "CC BY-SA 4.0"
  }, {
    "comment_id" : 140357722,
    "post_id" : 79586452,
    "body" : "Speculation: there is overhead to schedule and switch threads, and the work each thread is doing is CPU bound, where the CPU cycles used by an active thread is less than the CPU cycles to schedule/switch to that thread.  Consider: try a test where each thread works on a 1/12 subset of StateData.models (instead of a single element).",
    "score" : 1,
    "owner" : {
      "account_id" : 8909811,
      "reputation" : 2861,
      "user_id" : 6650475,
      "user_type" : "registered",
      "profile_image" : "https://www.gravatar.com/avatar/fe85994d05ad7aaaa1c47d638b37bc45?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name" : "Andrew S",
      "link" : "https://stackoverflow.com/users/6650475/andrew-s"
    },
    "creation_date" : 1745326512,
    "content_license" : "CC BY-SA 4.0"
  } ],
  "answer_comments" : { }
}