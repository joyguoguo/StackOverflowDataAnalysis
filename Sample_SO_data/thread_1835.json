{
  "question" : {
    "question_id" : 79671409,
    "title" : "Unable to load org.apache.spark.sql.delta classes from JVM pyspark",
    "body" : "<p>I’m working on Databricks with a cluster running Runtime 16.4, which includes Spark 3.5.2 and Scala 2.12.</p>\n<p>For a specific need, I want to implement my own custom way of writing to Delta tables by manually managing Delta transactions from PySpark. To do this, I want to access the Delta Lake transactional engine via the JVM embedded in the Spark session, specifically by using the class:</p>\n<pre><code>org.apache.spark.sql.delta.DeltaLog\n</code></pre>\n<p><strong>Issue</strong></p>\n<p>When I try to use classes from the package org.apache.spark.sql.delta directly from PySpark (through spark._jvm), the classes are not found if I don’t have the Delta Core package installed explicitly on the cluster.</p>\n<p>When I install the Delta Core Python package to gain access, I encounter the following Python import error:</p>\n<pre><code>ModuleNotFoundError: No module named 'delta.exceptions.captured'; 'delta.exceptions' is not a package\n</code></pre>\n<p>Without the Delta Core package installed, accessing DeltaLog simply returns a generic JavaPackage object that is unusable.</p>\n<p>What I want to do\nAccess the Delta transaction log API (DeltaLog) from PySpark via JVM.</p>\n<p>Be able to start transactions and commit manually to implement custom write behavior.</p>\n<p>Work within the Databricks Runtime 16.4 environment without conflicts or missing dependencies.</p>\n<p><strong>Questions</strong></p>\n<p>How can I correctly access and use <code>org.apache.spark.sql.delta.DeltaLog</code> from PySpark on Databricks Runtime 16.4?</p>\n<p>Is there a supported way to manually manage Delta transactions through the JVM in this environment?</p>\n<p>What is the correct setup or package dependency to avoid the <code>ModuleNotFoundError</code> when installing the Delta Core Python package?</p>\n<p>Are there any alternatives or recommended patterns to achieve manual Delta commits programmatically on Databricks?</p>\n",
    "tags" : [ "java", "scala", "apache-spark", "pyspark", "databricks" ],
    "owner" : {
      "account_id" : 17598288,
      "reputation" : 3,
      "user_id" : 12768801,
      "user_type" : "registered",
      "profile_image" : "https://i.sstatic.net/5TPN9.jpg?s=256",
      "display_name" : "Paul Douane",
      "link" : "https://stackoverflow.com/users/12768801/paul-douane"
    },
    "is_answered" : false,
    "view_count" : 73,
    "answer_count" : 0,
    "score" : 0,
    "last_activity_date" : 1750291290,
    "creation_date" : 1750291290,
    "link" : "https://stackoverflow.com/questions/79671409/unable-to-load-org-apache-spark-sql-delta-classes-from-jvm-pyspark",
    "content_license" : "CC BY-SA 4.0"
  },
  "answers" : [ ],
  "question_comments" : [ {
    "comment_id" : 140528866,
    "post_id" : 79671409,
    "body" : "I just installed the delta-core from the Maven interface on my cluster",
    "score" : 0,
    "owner" : {
      "account_id" : 17598288,
      "reputation" : 3,
      "user_id" : 12768801,
      "user_type" : "registered",
      "profile_image" : "https://i.sstatic.net/5TPN9.jpg?s=256",
      "display_name" : "Paul Douane",
      "link" : "https://stackoverflow.com/users/12768801/paul-douane"
    },
    "creation_date" : 1750407824,
    "content_license" : "CC BY-SA 4.0"
  }, {
    "comment_id" : 140528542,
    "post_id" : 79671409,
    "body" : "Haven&#39;t used Databricks runtime before, but how did installed your Delta Core package? Did you add the jar name in your Spark config, or you download the jar from repo (e.g. Maven) and put it into a specific location when you launch a Spark App?",
    "score" : 0,
    "owner" : {
      "account_id" : 14460373,
      "reputation" : 2343,
      "user_id" : 10445333,
      "user_type" : "registered",
      "profile_image" : "https://www.gravatar.com/avatar/c104efe81ce2ca3aed393584864a024d?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name" : "Jonathan",
      "link" : "https://stackoverflow.com/users/10445333/jonathan"
    },
    "creation_date" : 1750397293,
    "content_license" : "CC BY-SA 4.0"
  }, {
    "comment_id" : 140525590,
    "post_id" : 79671409,
    "body" : "I think it would help you get answers quickly  if you post this question in Databricks community as well. <a href=\"https://community.databricks.com/\" rel=\"nofollow noreferrer\">community.databricks.com</a>",
    "score" : 0,
    "owner" : {
      "account_id" : 1383279,
      "reputation" : 1511,
      "user_id" : 1315833,
      "user_type" : "registered",
      "accept_rate" : 80,
      "profile_image" : "https://www.gravatar.com/avatar/00485e9b12c3818ec67071c2bb197a10?s=256&d=identicon&r=PG",
      "display_name" : "Vindhya G",
      "link" : "https://stackoverflow.com/users/1315833/vindhya-g"
    },
    "creation_date" : 1750308282,
    "content_license" : "CC BY-SA 4.0"
  } ],
  "answer_comments" : { }
}