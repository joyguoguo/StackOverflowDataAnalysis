{
  "question" : {
    "question_id" : 79636267,
    "title" : "Spring Boot JPA Insert taking too much time for 800 records",
    "body" : "<p>I'm trying to store up to 5000 records.</p>\n<p>I'm using the normal JPA Repository (<code>abcDAO.saveAllAndFlush()</code>) with <strong><code>Spring Transaction</code></strong> to insert all my records.</p>\n<p>I'm calling this insert controller as a Rest API.</p>\n<p>When data has more than 800 records <code>abcDAO.saveAllAndFlush()</code> is taking more time which violates our 60 seconds of gateway time out for all rest calls, causing exception, however data inserted fine after some time.</p>\n<p>Below is the model JPA entity I'm trying to insert,</p>\n<pre class=\"lang-java prettyprint-override\"><code>package com.abc.def.xyz.entity;\n\nimport org.hibernate.annotations.RowId;\n\nimport jakarta.persistence.Column;\nimport jakarta.persistence.Entity;\nimport jakarta.persistence.Id;\nimport jakarta.persistence.Table;\nimport lombok.Data;\nimport lombok.NoArgsConstructor;\n\n@Data\n@NoArgsConstructor\n@Entity(name = &quot;TABLE_NAME&quot;)\n@RowId\n@Table(name = &quot;TABLE_NAME&quot;, schema = &quot;SCHEMA_NAME&quot;)\npublic class ABCData{\n\n    @Id\n    @Column(name = &quot;unique_id&quot;)\n    private String uniqueId;\n\n    @Column(name = &quot;name&quot;)\n    private String name;\n\n    @Column(name = &quot;age&quot;)\n    private Integer age;\n\n    @Column(name = &quot;lastUpdatedOn&quot;)\n    private java.sql.Timestamp lastUpdatedOn;\n\n    @Column(name = &quot;lastUpdatedBy&quot;)\n    private String lastUpdatedBy;\n\n    @Column(name = &quot;comments&quot;)\n    private String comments;\n    \n\n}\n</code></pre>\n<p>This model entity does not have a primary key which is generated sequentially or any way by the Oracle database, the <strong><code>Id</code> is generated by a third party</strong> (13 to 15 digit length number converted to string) and there is no possibility to add another column with sequence.</p>\n<p>Please find below the <strong>JPARepoConfig</strong> file I use for the repositories,</p>\n<p>How to enhance the <code>abcDAO.saveAllAndFlush()</code> to take less time and to insert more records with the above mentioned entity object?</p>\n<p>I tried adding the below properties to my <strong>JPARepoConfig</strong> but I see no improvement,</p>\n<pre><code>jpaPropMap.put(&quot;hibernate.order_updates&quot;,true);\n        jpaPropMap.put(&quot;hibernate.order_inserts&quot;,true);\n        jpaPropMap.put(&quot;hibernate.jdbc.batch_size&quot;,100);\n</code></pre>\n<p>Is there any other enhancements that can be done?</p>\n",
    "tags" : [ "java", "spring-boot", "performance", "jpa" ],
    "owner" : {
      "account_id" : 13988272,
      "reputation" : 15,
      "user_id" : 10102838,
      "user_type" : "registered",
      "profile_image" : "https://graph.facebook.com/1728996167190097/picture?type=large",
      "display_name" : "User1512",
      "link" : "https://stackoverflow.com/users/10102838/user1512"
    },
    "is_answered" : true,
    "view_count" : 244,
    "answer_count" : 2,
    "score" : 0,
    "last_activity_date" : 1749139202,
    "creation_date" : 1748039872,
    "link" : "https://stackoverflow.com/questions/79636267/spring-boot-jpa-insert-taking-too-much-time-for-800-records",
    "content_license" : "CC BY-SA 4.0"
  },
  "answers" : [ {
    "answer_id" : 79642944,
    "question_id" : 79636267,
    "body" : "<p><code>saveAllAndFlush()</code> uses <code>save()</code> under the hood. Refer the image attached:<a href=\"https://i.sstatic.net/yZ6F890w.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/yZ6F890w.png\" alt=\"enter image description here\" /></a></p>\n<p>It will not insert in batches until you explicitly define batch inserts. You may need to define batch inserts in <code>application.properties</code></p>\n<pre class=\"lang-yaml prettyprint-override\"><code>spring.jpa.properties.hibernate.jdbc.batch_size=100\nspring.jpa.properties.hibernate.order_inserts=true\nspring.jpa.properties.hibernate.order_updates=true\nspring.jpa.properties.hibernate.generate_statistics=true\nspring.jpa.show-sql=false\n</code></pre>\n<p>and then insert in batches in your code</p>\n<pre><code>public void saveInBatches(List&lt;ABCData&gt; dataList) {\n    final int batchSize = 100; //same as in application.properties\n    for (int i = 0; i &lt; dataList.size(); i += batchSize) {\n        List&lt;ABCData&gt; batch = dataList.subList(i, Math.min(i + batchSize, dataList.size()));\n        abcDAO.saveAllAndFlush(batch); \n    }\n}\n</code></pre>\n<p>Flushing thousands of objects at once may introduce performance overhead, that is why it is better to chunk data into smaller pieces, insert and flush.</p>\n",
    "score" : 2,
    "is_accepted" : false,
    "owner" : {
      "account_id" : 16860149,
      "reputation" : 181,
      "user_id" : 12191150,
      "user_type" : "registered",
      "profile_image" : "https://www.gravatar.com/avatar/fec9a6154254eb5fd901f4a0045c0b1f?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name" : "Yasin Ahmed",
      "link" : "https://stackoverflow.com/users/12191150/yasin-ahmed"
    },
    "creation_date" : 1748463243,
    "last_activity_date" : 1748463243,
    "content_license" : "CC BY-SA 4.0"
  }, {
    "answer_id" : 79654824,
    "question_id" : 79636267,
    "body" : "<p>Disable SQL logs in production:</p>\n<pre class=\"lang-ini prettyprint-override\"><code>spring.jpa.show-sql=false \nspring.jpa.properties.hibernate.format_sql=false\n</code></pre>\n<p>Oracle JDBC Driver must enable batching: If you're using Oracle 12c+:</p>\n<pre><code>spring.datasource.driver-class-name=oracle.jdbc.OracleDriver\nspring.datasource.hikari.data-source-properties.oracle.jdbc.batchPerformanceWorkaround=true\n</code></pre>\n<p>or HikariConfig :</p>\n<pre><code>dataSource.setConnectionProperties(&quot;oracle.jdbc.batchPerformanceWorkaround=true&quot;);\n</code></pre>\n<p>Instead of saveAllAndFlush use it <code>save()</code> in batches with <code>flush()</code> and <code>clear()</code></p>\n<pre><code>@Transactional\npublic void insertInChunks(List&lt;ABCData&gt; dataList) {\n    int batchSize = 100;\n    for (int i = 0; i &lt; dataList.size(); i++) {\n        abcDAO.save(dataList.get(i));\n        if (i % batchSize == 0 &amp;&amp; i &gt; 0) {\n            abcDAO.flush(); // flush to DB\n            entityManager.clear(); // clear persistence context to free memory\n        }\n    }\n    abcDAO.flush(); // final flush\n}\n</code></pre>\n",
    "score" : 0,
    "is_accepted" : false,
    "owner" : {
      "account_id" : 28700800,
      "reputation" : 9,
      "user_id" : 21978012,
      "user_type" : "registered",
      "profile_image" : "https://i.sstatic.net/USfGU.jpg?s=256",
      "display_name" : "vimukthi jayasanka",
      "link" : "https://stackoverflow.com/users/21978012/vimukthi-jayasanka"
    },
    "creation_date" : 1749139202,
    "last_activity_date" : 1749139202,
    "content_license" : "CC BY-SA 4.0"
  } ],
  "question_comments" : [ {
    "comment_id" : 140459860,
    "post_id" : 79636267,
    "body" : "If you have other concurrent processes, and time is an issue, maybe use smaller more manageable batch sizes instead of 800. Your processes will be forced to lock the table, so slow each other down, so that they can guarantee the constraints aren&#39;t violated. Save/flush at the end and handle the failures (but go quicker), or use smaller batches that can be committed more periodically to allow releasing the locks quicker. Otherwise, show us the actual batch processing.",
    "score" : 0,
    "owner" : {
      "account_id" : 231721,
      "reputation" : 21335,
      "user_id" : 496099,
      "user_type" : "registered",
      "profile_image" : "https://www.gravatar.com/avatar/efa7510cf47d7bd79e80486246e7e39e?s=256&d=identicon&r=PG",
      "display_name" : "Chris",
      "link" : "https://stackoverflow.com/users/496099/chris"
    },
    "creation_date" : 1748268739,
    "content_license" : "CC BY-SA 4.0"
  }, {
    "comment_id" : 140457049,
    "post_id" : 79636267,
    "body" : "@User1512 You are in a transaction and saveAllAndFlush() will not see what others are doing.",
    "score" : 0,
    "owner" : {
      "account_id" : 6389739,
      "reputation" : 5431,
      "user_id" : 4956336,
      "user_type" : "registered",
      "profile_image" : "https://i.sstatic.net/5b2Ij.png?s=256",
      "display_name" : "p3consulting",
      "link" : "https://stackoverflow.com/users/4956336/p3consulting"
    },
    "creation_date" : 1748176036,
    "content_license" : "CC BY-SA 4.0"
  }, {
    "comment_id" : 140455953,
    "post_id" : 79636267,
    "body" : "@p3consulting in my case, we have another application that also inserts and updates records into this table, there could be duplicates or some updates getting missed so flush is required :(",
    "score" : 0,
    "owner" : {
      "account_id" : 13988272,
      "reputation" : 15,
      "user_id" : 10102838,
      "user_type" : "registered",
      "profile_image" : "https://graph.facebook.com/1728996167190097/picture?type=large",
      "display_name" : "User1512",
      "link" : "https://stackoverflow.com/users/10102838/user1512"
    },
    "creation_date" : 1748113300,
    "content_license" : "CC BY-SA 4.0"
  }, {
    "comment_id" : 140455950,
    "post_id" : 79636267,
    "body" : "@TevinS yes the unique_id is a primary key to the table in the database",
    "score" : 0,
    "owner" : {
      "account_id" : 13988272,
      "reputation" : 15,
      "user_id" : 10102838,
      "user_type" : "registered",
      "profile_image" : "https://graph.facebook.com/1728996167190097/picture?type=large",
      "display_name" : "User1512",
      "link" : "https://stackoverflow.com/users/10102838/user1512"
    },
    "creation_date" : 1748113204,
    "content_license" : "CC BY-SA 4.0"
  }, {
    "comment_id" : 140455696,
    "post_id" : 79636267,
    "body" : "A call of the flush method forces the JPA implementation to perform a dirty check on all managed entity objects which is pointless if you import new data.",
    "score" : 0,
    "owner" : {
      "account_id" : 6389739,
      "reputation" : 5431,
      "user_id" : 4956336,
      "user_type" : "registered",
      "profile_image" : "https://i.sstatic.net/5b2Ij.png?s=256",
      "display_name" : "p3consulting",
      "link" : "https://stackoverflow.com/users/4956336/p3consulting"
    },
    "creation_date" : 1748105146,
    "content_license" : "CC BY-SA 4.0"
  }, {
    "comment_id" : 140455434,
    "post_id" : 79636267,
    "body" : "It&#39;s a random 13 digit number created converted back to string in the database",
    "score" : 0,
    "owner" : {
      "account_id" : 13988272,
      "reputation" : 15,
      "user_id" : 10102838,
      "user_type" : "registered",
      "profile_image" : "https://graph.facebook.com/1728996167190097/picture?type=large",
      "display_name" : "User1512",
      "link" : "https://stackoverflow.com/users/10102838/user1512"
    },
    "creation_date" : 1748096624,
    "content_license" : "CC BY-SA 4.0"
  } ],
  "answer_comments" : {
    "79642944" : [ {
      "comment_id" : 140490743,
      "post_id" : 79642944,
      "body" : "The delay was identified, the culprit was my Entity class and saveAllAndFlush() method from JPA. I used saveAll() and flush() separately in between batches and it improved the performance at a good level.",
      "score" : 1,
      "owner" : {
        "account_id" : 13988272,
        "reputation" : 15,
        "user_id" : 10102838,
        "user_type" : "registered",
        "profile_image" : "https://graph.facebook.com/1728996167190097/picture?type=large",
        "display_name" : "User1512",
        "link" : "https://stackoverflow.com/users/10102838/user1512"
      },
      "creation_date" : 1749136379,
      "content_license" : "CC BY-SA 4.0"
    }, {
      "comment_id" : 140484546,
      "post_id" : 79642944,
      "body" : "Is it possible to provide a skeleton or any links for the code which enables async insertion of JPA Repository in a Transaction?",
      "score" : 0,
      "owner" : {
        "account_id" : 13988272,
        "reputation" : 15,
        "user_id" : 10102838,
        "user_type" : "registered",
        "profile_image" : "https://graph.facebook.com/1728996167190097/picture?type=large",
        "display_name" : "User1512",
        "link" : "https://stackoverflow.com/users/10102838/user1512"
      },
      "creation_date" : 1748976795,
      "content_license" : "CC BY-SA 4.0"
    }, {
      "comment_id" : 140484471,
      "post_id" : 79642944,
      "body" : "Please show code, it depends how are you calling async. Additionally batch updates/inserts are for same tables. For e.g you can add 100 records in same table at once but not in two different tables.",
      "score" : 0,
      "owner" : {
        "account_id" : 16860149,
        "reputation" : 181,
        "user_id" : 12191150,
        "user_type" : "registered",
        "profile_image" : "https://www.gravatar.com/avatar/fec9a6154254eb5fd901f4a0045c0b1f?s=256&d=identicon&r=PG&f=y&so-version=2",
        "display_name" : "Yasin Ahmed",
        "link" : "https://stackoverflow.com/users/12191150/yasin-ahmed"
      },
      "creation_date" : 1748975582,
      "content_license" : "CC BY-SA 4.0"
    }, {
      "comment_id" : 140484387,
      "post_id" : 79642944,
      "body" : "I have multiple tables to insert or update records, and I&#39;m calling each table in a separate async method, with the help of generate statistics, I found that all my db calls to different tables are happening sequentially eventhough I call them from async method in a Transactional annotated service. Is there any possibility to make these DB calls async in JPA?",
      "score" : 0,
      "owner" : {
        "account_id" : 13988272,
        "reputation" : 15,
        "user_id" : 10102838,
        "user_type" : "registered",
        "profile_image" : "https://graph.facebook.com/1728996167190097/picture?type=large",
        "display_name" : "User1512",
        "link" : "https://stackoverflow.com/users/10102838/user1512"
      },
      "creation_date" : 1748973632,
      "content_license" : "CC BY-SA 4.0"
    }, {
      "comment_id" : 140484128,
      "post_id" : 79642944,
      "body" : "Try to increase batch size, and if there is any difference b/w performance that means you are on right track",
      "score" : 0,
      "owner" : {
        "account_id" : 16860149,
        "reputation" : 181,
        "user_id" : 12191150,
        "user_type" : "registered",
        "profile_image" : "https://www.gravatar.com/avatar/fec9a6154254eb5fd901f4a0045c0b1f?s=256&d=identicon&r=PG&f=y&so-version=2",
        "display_name" : "Yasin Ahmed",
        "link" : "https://stackoverflow.com/users/12191150/yasin-ahmed"
      },
      "creation_date" : 1748967532,
      "content_license" : "CC BY-SA 4.0"
    }, {
      "comment_id" : 140483266,
      "post_id" : 79642944,
      "body" : "Hi @Yasin it didn&#39;t help in the overall performance, using the statistics I see the queries were running in batches but still I didn&#39;t see an improvement in overall.",
      "score" : 0,
      "owner" : {
        "account_id" : 13988272,
        "reputation" : 15,
        "user_id" : 10102838,
        "user_type" : "registered",
        "profile_image" : "https://graph.facebook.com/1728996167190097/picture?type=large",
        "display_name" : "User1512",
        "link" : "https://stackoverflow.com/users/10102838/user1512"
      },
      "creation_date" : 1748953700,
      "content_license" : "CC BY-SA 4.0"
    } ]
  }
}