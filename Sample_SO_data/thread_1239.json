{
  "question" : {
    "question_id" : 79727681,
    "title" : "Combining two huge text files and removing duplicates",
    "body" : "<h2>The Specific Problem</h2>\n<p>I am starting with two gigantic input text files containing lots of lines with a single string value on each line. I'd like to write a Java program to combine the contents of those two files into one output text file, while also removing any duplicates. <strong>What method would be best for performance?</strong></p>\n<h2>What data sizes are we talking about?</h2>\n<p>The combined file might be <strong>250 GB, maybe larger</strong>. I did a back-of-the-napkin calculation using a smaller example file and the 250 GB file may have around <strong>27 billion lines</strong> (very rough) if it's proportional.</p>\n<p>So we're definitely running into the territory of max Integer value in Java (2,147,483,647). We can imagine that the value on each line is a max of 20 characters, otherwise it will be discarded.</p>\n<p>And let's assume the input files and output file (and likely any intermediate data structure used for duplicate checks) will be too large for memory. I will have the use of a computer with a large amount of memory, but it may not be enough.</p>\n<p>Let me also note that the average length of a line is probably 7-10 characters. And I will be discarding any lines that are longer than 25 characters. I mention this because hashing was brought up as a method for duplicate checking. It seems the hash for these values would take up more space than the actual value.</p>\n<h2>Planning</h2>\n<p>Initially I was thinking of how to do the duplicate check to reduce processing time, as I'm sure I will want to make it run as fast as possible. However I realize the more immediate problem is how to deal with the large sizes and whatever I use to check for duplicates or sort will not fit in memory. Possibly sort the input files first, then merge together and discard duplicates in one pass. The output file can be sorted, doesn't need to maintain original order.</p>\n<h2>I've also been doing my research.</h2>\n<p>I came across this previous question-\n<a href=\"https://stackoverflow.com/questions/52315045/removing-duplicate-strings-from-huge-text-files\">Removing duplicate strings from huge text files</a></p>\n<p>The only answer mostly deals with data sizes that would fit into memory. But at the end it mentions a possible way to do this with a large file (just one file) that doesn't fit into RAM. I'm not sure I fully understand it, and it would seem that the duplicate checking mechanism itself would also be very large and not fit into memory contrary to what the author intended.</p>\n<p>And I read up on External Sorting.\n<a href=\"https://en.wikipedia.org/wiki/External_sorting\" rel=\"nofollow noreferrer\">https://en.wikipedia.org/wiki/External_sorting</a></p>\n<p>That could definitely be something I try if I implement it with sorting first.</p>\n<p>What method do you suggest? Many thanks for input!</p>\n",
    "tags" : [ "java", "memory", "external-sorting" ],
    "owner" : {
      "account_id" : 709632,
      "reputation" : 1053,
      "user_id" : 620054,
      "user_type" : "registered",
      "accept_rate" : 70,
      "profile_image" : "https://www.gravatar.com/avatar/b9980ecabf192a4b53ff1d541a7b5229?s=256&d=identicon&r=PG",
      "display_name" : "Michael K",
      "link" : "https://stackoverflow.com/users/620054/michael-k"
    },
    "is_answered" : true,
    "view_count" : 256,
    "answer_count" : 2,
    "score" : 0,
    "last_activity_date" : 1756911788,
    "creation_date" : 1754503098,
    "link" : "https://stackoverflow.com/questions/79727681/combining-two-huge-text-files-and-removing-duplicates",
    "content_license" : "CC BY-SA 4.0"
  },
  "answers" : [ {
    "answer_id" : 79736315,
    "question_id" : 79727681,
    "body" : "<p>There is a very simple and straightforward way to implement this functionality using an SQL database. Most databases are designed to be friendly to large amounts of data, and all you need to do is:</p>\n<ol>\n<li>Create a database and table (you can add an index if needed)</li>\n<li>Read small amounts of data from the file each time and write them to the database</li>\n<li>Read small amounts of data from the database each time and write them to the file</li>\n</ol>\n<p>In the end, you will get the deduplicated results. This solution is very simple and easy to implement.</p>\n<p>There's another approach that doesn't require third-party tools or libraries. It involves bucketing strings using string identifiers to ensure potentially duplicate strings are placed in the same bucket. If you're not sensitive to performance, you can implement it in a simple way:</p>\n<ol>\n<li>Read small amounts of data from the file each time</li>\n<li>Calculate a hash value for each string (md5, sha, etc. - your choice)</li>\n<li>Write the string to a <code>&lt;hashcode&gt;.txt</code> file (be careful not to overwrite existing content)</li>\n<li>Deduplicate strings within each <code>&lt;hashcode&gt;.txt</code> file</li>\n<li>Merge these files</li>\n</ol>\n<p>In the end, you will get the deduplicated results. If you are sensitive to performance, you'll need to design more details more carefully.</p>\n<p>If you're not sensitive to time, both solutions are viable options. If time is a concern, I recommend the first solution, which can leverage advanced database optimizations without you having to pull out too much of your hair.</p>\n",
    "score" : 1,
    "is_accepted" : false,
    "owner" : {
      "account_id" : 36772059,
      "reputation" : 59,
      "user_id" : 27909701,
      "user_type" : "registered",
      "profile_image" : "https://www.gravatar.com/avatar/cb67853486307b4c0d8b90401c1c78d9?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name" : "user27909701",
      "link" : "https://stackoverflow.com/users/27909701/user27909701"
    },
    "creation_date" : 1755251999,
    "last_activity_date" : 1755251999,
    "content_license" : "CC BY-SA 4.0"
  }, {
    "answer_id" : 79741668,
    "question_id" : 79727681,
    "body" : "<p>Here is my solution</p>\n<h2>Duplicate checking via database</h2>\n<p>Based on suggestions here and elsewhere, I decided to go with a database table to do the work of removing duplicates. The table is defined with UNIQUE. When entries are added, any that are duplicates to those already in the table are discarded.</p>\n<p>I used an SQLite database, and the <a href=\"https://github.com/xerial/sqlite-jdbc\" rel=\"nofollow noreferrer\">SQLite JDBC library by xerial</a>. I don't know if it's fastest among database solutions for this purpose, but it is easy to deal with. Not requiring a server, and just dealing with files, will surely be an advantage too.</p>\n<p>I created the SQLite database in <a href=\"https://sqlite.org/\" rel=\"nofollow noreferrer\">SQLite</a> like this-</p>\n<pre><code>CREATE TABLE lines (\n    content TEXT UNIQUE\n);\n</code></pre>\n<p>It's a table with one column of text type, and unique specified to prevent duplicates. An attempt to insert a duplicate entry would give an exception.</p>\n<h2>Initial counting of lines</h2>\n<p>Initially my app runs through all the input files specified and counts the lines in each. I was able to do this in a multithreaded way (for each input file) and that improved speed.</p>\n<pre><code>Arrays.stream(inputFileArray)\n            .parallel()\n            .forEach(inputFile -&gt; {\n                inputFile.countLines();\n                inputFile.resetToBeginning();\n            });\n</code></pre>\n<p>This inputFile and the countLines() method ultimately use BufferedInputStream to read the data. It looks for the newline character in every byte. It does use a read size of 4 KB since that seemed fast in my testing. If the last line in the file is a newline, I don't count that. But every other newline character is counted, and that ends up being the number of lines. (It's possible BufferedInputStream is quicker than BufferedReader in this instance, though I'm not sure. BufferedReader requires converting from bytes to String/character data.) This counting of lines ends up just requiring a negligible amount of time compared to the core processing.</p>\n<h2>Main processing of lines</h2>\n<p>I have a basic loop through the input files, and a nested loop reading the lines, and acting on them. This is the core processing lines portion of the app. This reading actually uses BufferedReader, as it's the perfect fit for reading lines, and I do need the lines as character data, so there is no benefit to leaving them as bytes.</p>\n<p>For each line, I do an up-front rules check which can rule out some lines. I can do different things here like just discard every line that's longer/shorter than a certain length, or discard any lines that contain non-ASCII characters.</p>\n<p>Next an attempt is made to insert the line content to the database.</p>\n<p><strong>Batching</strong></p>\n<p>I used batching for the SQL inserts to improve speed. This was big! I have a batch size of 4 million entries right now. Rather than doing preparedStatementInsert.executeUpdate(), I do PreparedStatementInsert.addBatch(). And then I check for the count and compare to the batch size (modulus), and then do preparedStatementInsert.executeBatch().</p>\n<pre><code>psInsert.addBatch();\n\n[...]\n\nif (insertsCount % INSERTS_BATCH_SIZE == 0) {\n    psInsert.executeBatch();\n}\n</code></pre>\n<p><strong>Transactions &amp; commits</strong></p>\n<p>And I'm also using transactions + commits (disabled auto-commit). It's a similar idea. Before the SQL inserts, I <code>conn.setAutoCommit(false)</code>. Then within the loop over the lines, I do a <code>conn.commit()</code>, also based on a modulus of a chosen transaction size. I have that at 16 million right now. I'll play with these sizes more when I'm doing bigger runs.</p>\n<p>After the loop through the lines is complete, I do one extra <code>psInsert.executeBatch()</code> and <code>conn.commit()</code> to make sure everything is executed to the database.</p>\n<p>Initially I was counting the number of duplicates discarded while in progress by catching the exception on inserts, and checking if it contained the message about (just checked for &quot;UNIQUE&quot; basically). With small files that worked. But with batching, and large files, I had errors trying to catch the exceptions and count them. I was getting the following error -</p>\n<p><code>[SQLITE_CONSTRAINT_UNIQUE] A UNIQUE constraint failed (UNIQUE constraint failed: lines.content)</code></p>\n<p>It was throwing the exception even when I was trying to catch it.</p>\n<p>I ended up using INSERT OR IGNORE in my insert statements. This silently fails on duplicates, or other violations. That means I can't count duplicates by catching exceptions - and now I don't track that total while in progress so it can be reported to the user in periodic progress reports.</p>\n<p><code>INSERT OR IGNORE INTO lines (content) VALUES (?)</code></p>\n<p>I will probably revisit that. It's possible I need to catch the exception differently. Though I think it's throwing out the whole batch if there is one exception within the batch, and that's no good.</p>\n<p>The alternative would be to do a SELECT on the database and count how many were added and compare to how many it processed. That could be done after every insert, or periodically after so many lines processed like the batch execution. And of course that would have a performance hit. I decided to just do that at the very end for performance reasons.</p>\n<h2>Writing to output file</h2>\n<p>At this point, the database has all of the entries across all input files, and they are deduplicated.</p>\n<p>To write to output, first the app does a simple SELECT on all entries in the database table. An ORDER BY can be added here to sort alphabetically.</p>\n<p><code>SELECT content FROM lines ORDER BY content ASC</code></p>\n<p>Then it loops through the ResultSet of that, and writes lines to the output file. I did try making a local write buffer to batch up a bunch of lines to write, and then write them in the batches. But I found I got no performance gain from that. It's already using a buffer and I guess it does just fine. This stage of the execution is quite short compared to the core processing, but with a huge file it can still take many hours of course.</p>\n<pre><code>\nString currLineForOutput;\nwhile (rsLinesContent.next()) {\n    currLineForOutput = rsLinesContent.getString(&quot;content&quot;);                                \n    outputFile.writeLine(currLineForOutput);\n    numLinesWrittenToOutput++;\n    if (numLinesWrittenToOutput % REPORTING_INTERVAL_NUM_LINES_WRITTEN == 0){\n        //report lines\n    }                \n}\nrsLinesContent.close();\npsLinesContent.close();\n</code></pre>\n<h2>Database file notes</h2>\n<p>Then I optionally do some database cleanup here. Initially I emptied the database, and then added a VACUUM so the database file shrank, otherwise it would remain large (which is for performance reasons to reduce file re-sizing). But then the use-case was adjusted and the user wanted to run this to add entries from a new file, potentially a bunch of times. With the previous setup and an empty database, the insertions (duplicate checks) would be repeating work. To reduce processing time, especially for huge files, my app now leaves the database file as it is at the end of processing. This way the user does not need to include all previous input files, or a &quot;master&quot; input file, in the command. They only need to include the new files in the command. The database retains the cumulative deduplicated content from previous runs, and significant time is saved. And actually now my app allows the user to specify the database file to use, so the database can be moved to a different drive due to large sizes, for possible performance improvement, or to keep an older database copy for some reason.</p>\n<h2>Parallelization</h2>\n<p>I did make one attempt to parallelize the processing of lines, and inserting them. But I ran into SQLite errors due to contention: <code>[SQLITE_BUSY] The database file is locked (database is locked)</code></p>\n<p>I'll have another go at it soon.</p>\n<h2>Run times</h2>\n<p>Previous versions have been run on some quite large files. For anyone curious, I'll list out some times. I don't have times for the newest versions, but I think it would be a little quicker.</p>\n<p>A single 262.2 GB file with 23.1 billion lines</p>\n<ul>\n<li><p>Duration: 11 hours, 39 minutes</p>\n</li>\n<li><p>Duration counting lines in input: 4 minutes</p>\n</li>\n<li><p>Duration processing lines: 6 hours, 42 minutes</p>\n</li>\n<li><p>Duration writing lines to output: 4 hours, 51 minutes</p>\n</li>\n</ul>\n<p>4 input files - 250.9 GB, 9.0 GB, 15.0 GB, 28.3 GB (303 GB, 27.7 billion lines total)</p>\n<ul>\n<li><p>Duration: 1 day, 17 hours</p>\n</li>\n<li><p>Duration counting lines in input: 4 minutes</p>\n</li>\n<li><p>Duration processing lines: 1 day, 9 hours</p>\n</li>\n<li><p>Duration writing lines to output: 7 hours, 4 minutes</p>\n</li>\n</ul>\n<h2>Operating system sort command</h2>\n<p>OldDogProgrammer had mentioned the sort command in Windows, and Toby Speight mentioned sort commands as well. These may cover part of the functionality. So I tried out the <a href=\"https://learn.microsoft.com/en-us/windows-server/administration/windows-commands/sort\" rel=\"nofollow noreferrer\">Windows sort command</a> myself in my smaller tests. To remove duplicates, the unique option has to be used.</p>\n<p>I did see it was slightly faster. I'll have to run a new comparison at some point since my app is faster than it was. But my app also is designed for the data structures to be in &quot;disk&quot; storage, not memory (due to huge sizes), and it's possible the sort command keeps them in memory. I haven't tried it on huge size files yet. However, the sort command had some disadvantages, and actually it's not useful for this purpose.</p>\n<p><strong>Disadvantages of sort command:</strong></p>\n<ul>\n<li><p>&quot;The sort command doesn't distinguish between uppercase and lowercase letters&quot;</p>\n</li>\n<li><p>No progress reporting - no idea how it's doing if it's really large dataset. If it's running for potentially days, this is a problem.</p>\n</li>\n<li><p>It doesn't allow for any extra rules to remove entries (up-front rules)</p>\n</li>\n<li><p>It requires sorting, can't keep original sort order if desired</p>\n</li>\n</ul>\n<p>The lack of distinguishing between uppercase and lowercase letters basically kills its usefulness for my purposes here. Those need to be seen as distinct, and not lead to discarding as duplicates. I have not yet looked into the sort command for Unix/Linux/MacOS.</p>\n<h2>Update</h2>\n<p>I reworked my code to use a producer and consumer model. The producer reads lines from the input file(s), does the up-front rules check on each line, and puts the lines into a queue. The consumer did all the database inserts, pulling lines out of the queue and inserting them. So these two were decoupled a bit.</p>\n<p>With this new setup, I made a second attempt at parallelizing the code that inserts the lines into the database. However, that ran into the same issue. Even with just 2 threads and small batches and transactions, I had that error. The one time I only had one error, and it seemed like it only skipped a very minimal amount of data judging by the output size... however the execution time was much longer, due to the small batch and transaction sizes. So I abandoned that.</p>\n<p>But I also reworked the parallelization of the producer - reading lines from input and performing the up-front rules check. That seemed to help a little, but not that much.</p>\n<p>Overall it is slightly improved in performance with these new changes. But it seems I can't meaningfully improve the writing to the SQLite database. This isn't unexpected based on what I've read out there, but I had to give it a try. And that portion is the bulk of the run time. Any further attempt to use parallelization for speed improvement would require another method, maybe a different database system or some different way of checking for duplicates, likely with intermediate sorting like External Sorting.</p>\n",
    "score" : 0,
    "is_accepted" : true,
    "owner" : {
      "account_id" : 709632,
      "reputation" : 1053,
      "user_id" : 620054,
      "user_type" : "registered",
      "accept_rate" : 70,
      "profile_image" : "https://www.gravatar.com/avatar/b9980ecabf192a4b53ff1d541a7b5229?s=256&d=identicon&r=PG",
      "display_name" : "Michael K",
      "link" : "https://stackoverflow.com/users/620054/michael-k"
    },
    "creation_date" : 1755730712,
    "last_activity_date" : 1756911788,
    "content_license" : "CC BY-SA 4.0"
  } ],
  "question_comments" : [ {
    "comment_id" : 140669307,
    "post_id" : 79727681,
    "body" : "You&#39;re right. Those are other shortcoming of Windows <code>sort</code> command.",
    "score" : 1,
    "owner" : {
      "account_id" : 6606677,
      "reputation" : 1251,
      "user_id" : 5103317,
      "user_type" : "registered",
      "profile_image" : "https://www.gravatar.com/avatar/c81d92487b054d7290b4f36562c0ee02?s=256&d=identicon&r=PG",
      "display_name" : "Old Dog Programmer",
      "link" : "https://stackoverflow.com/users/5103317/old-dog-programmer"
    },
    "creation_date" : 1755310445,
    "content_license" : "CC BY-SA 4.0"
  }, {
    "comment_id" : 140668800,
    "post_id" : 79727681,
    "body" : "@OldDogProgrammer Thanks! I just tried the Windows <code>sort</code> command, using the unique option. I got smaller output. I think it&#39;s because &quot;The sort command doesn&#39;t distinguish between uppercase and lowercase letters&quot;. I found it did indeed remove entries that were the same but with different case. That rules out using that. Though it was slightly faster. It also doesn&#39;t report any progress, which wouldn&#39;t be ideal if this runs for potentially days on a huge dataset. And it requires sorting, though that&#39;s not a problem for me. And yeah, it lacks multiple input files.",
    "score" : 1,
    "owner" : {
      "account_id" : 709632,
      "reputation" : 1053,
      "user_id" : 620054,
      "user_type" : "registered",
      "accept_rate" : 70,
      "profile_image" : "https://www.gravatar.com/avatar/b9980ecabf192a4b53ff1d541a7b5229?s=256&d=identicon&r=PG",
      "display_name" : "Michael K",
      "link" : "https://stackoverflow.com/users/620054/michael-k"
    },
    "creation_date" : 1755285228,
    "content_license" : "CC BY-SA 4.0"
  }, {
    "comment_id" : 140664230,
    "post_id" : 79727681,
    "body" : "Here is a page about the  Windows <code>sort</code> command. <a href=\"https://learn.microsoft.com/en-us/windows-server/administration/windows-commands/sort\" rel=\"nofollow noreferrer\">learn.microsoft.com/en-us/windows-server/administration/&hellip;</a>  I&#39;m surprised I don&#39;t see a provision for having more than one input file, i.e., merge, while sorting.",
    "score" : 1,
    "owner" : {
      "account_id" : 6606677,
      "reputation" : 1251,
      "user_id" : 5103317,
      "user_type" : "registered",
      "profile_image" : "https://www.gravatar.com/avatar/c81d92487b054d7290b4f36562c0ee02?s=256&d=identicon&r=PG",
      "display_name" : "Old Dog Programmer",
      "link" : "https://stackoverflow.com/users/5103317/old-dog-programmer"
    },
    "creation_date" : 1755136402,
    "content_license" : "CC BY-SA 4.0"
  }, {
    "comment_id" : 140663854,
    "post_id" : 79727681,
    "body" : "@TobySpeight I chose Java because of the lower barrier to entry for me. I already have a lot of file processing tools I can leverage in Java. And since I&#39;m writing it, I have control over how it works. I&#39;ll post an answer here soon. (Thankfully this question has been reopened!) I have this working pretty well. I&#39;m guessing the sort command is something available on Linux or MacOS, right? Maybe I&#39;ll try running that as well and compare.",
    "score" : 0,
    "owner" : {
      "account_id" : 709632,
      "reputation" : 1053,
      "user_id" : 620054,
      "user_type" : "registered",
      "accept_rate" : 70,
      "profile_image" : "https://www.gravatar.com/avatar/b9980ecabf192a4b53ff1d541a7b5229?s=256&d=identicon&r=PG",
      "display_name" : "Michael K",
      "link" : "https://stackoverflow.com/users/620054/michael-k"
    },
    "creation_date" : 1755116894,
    "content_license" : "CC BY-SA 4.0"
  }, {
    "comment_id" : 140659603,
    "post_id" : 79727681,
    "body" : "Why do you want to do this <i>using Java</i>?  Why not use an external command such as <code>sort</code>? If the inputs are already sorted, you can use <code>sort -mu</code> to merge and uniquify.",
    "score" : 1,
    "owner" : {
      "account_id" : 6229027,
      "reputation" : 32424,
      "user_id" : 4850040,
      "user_type" : "registered",
      "accept_rate" : 62,
      "profile_image" : "https://i.sstatic.net/acYd0.png?s=256",
      "display_name" : "Toby Speight",
      "link" : "https://stackoverflow.com/users/4850040/toby-speight"
    },
    "creation_date" : 1754992406,
    "content_license" : "CC BY-SA 4.0"
  }, {
    "comment_id" : 140657611,
    "post_id" : 79727681,
    "body" : "@OldDogProgrammer Not that I&#39;m aware of. I&#39;m using Windows.",
    "score" : 0,
    "owner" : {
      "account_id" : 709632,
      "reputation" : 1053,
      "user_id" : 620054,
      "user_type" : "registered",
      "accept_rate" : 70,
      "profile_image" : "https://www.gravatar.com/avatar/b9980ecabf192a4b53ff1d541a7b5229?s=256&d=identicon&r=PG",
      "display_name" : "Michael K",
      "link" : "https://stackoverflow.com/users/620054/michael-k"
    },
    "creation_date" : 1754923259,
    "content_license" : "CC BY-SA 4.0"
  }, {
    "comment_id" : 140647766,
    "post_id" : 79727681,
    "body" : "Does your operating system have a <code>sort</code> command ?",
    "score" : 2,
    "owner" : {
      "account_id" : 6606677,
      "reputation" : 1251,
      "user_id" : 5103317,
      "user_type" : "registered",
      "profile_image" : "https://www.gravatar.com/avatar/c81d92487b054d7290b4f36562c0ee02?s=256&d=identicon&r=PG",
      "display_name" : "Old Dog Programmer",
      "link" : "https://stackoverflow.com/users/5103317/old-dog-programmer"
    },
    "creation_date" : 1754530155,
    "content_license" : "CC BY-SA 4.0"
  }, {
    "comment_id" : 140647624,
    "post_id" : 79727681,
    "body" : "@Robert Thanks for the link to the Unix question/answer. Yes that&#39;s similar to what the last step would be I think. Actually that part seems straightforward to write, to go through and merge things when there are multiple already-sorted files. Yes you have the idea. These would be generic lists of potential passwords, and this would allow merging of password dictionaries. That would be used to help clients recover lost access to something. This strategy would be used along with a strategy focused on variations on password ideas from the client to have our best shot.",
    "score" : 0,
    "owner" : {
      "account_id" : 709632,
      "reputation" : 1053,
      "user_id" : 620054,
      "user_type" : "registered",
      "accept_rate" : 70,
      "profile_image" : "https://www.gravatar.com/avatar/b9980ecabf192a4b53ff1d541a7b5229?s=256&d=identicon&r=PG",
      "display_name" : "Michael K",
      "link" : "https://stackoverflow.com/users/620054/michael-k"
    },
    "creation_date" : 1754520158,
    "content_license" : "CC BY-SA 4.0"
  }, {
    "comment_id" : 140647556,
    "post_id" : 79727681,
    "body" : "How about splitting each file up, depending on the first character of the string?  So you have firstfile-a.txt, firstfile-b.txt, firstfile-c.txt, and also have secondfile-a.txt, secondfile-b.txt and so on.  Use one of the methods from the other questions you&#39;ve found, to merge and deduplicate firstfile-a.txt with secondfile-a.txt; then similarly for the other initial characters.  Then take all of these merged deduplicated files and merge them together into a single output file.",
    "score" : 1,
    "owner" : {
      "account_id" : 1085259,
      "reputation" : 80183,
      "user_id" : 1081110,
      "user_type" : "registered",
      "profile_image" : "https://i.sstatic.net/Fp4Pm.jpg?s=256",
      "display_name" : "Dawood ibn Kareem",
      "link" : "https://stackoverflow.com/users/1081110/dawood-ibn-kareem"
    },
    "creation_date" : 1754517140,
    "content_license" : "CC BY-SA 4.0"
  }, {
    "comment_id" : 140647526,
    "post_id" : 79727681,
    "body" : "Let me guess you process password leaks? Sqlite is a database but serverless. To my experience real databases with server usually perform better as they have more advanced memory management functions and functions for partitioning tables and indexes. But as you don&#39;t want to keep the data in a database I would use the file partitioning approach. Merging sorted files in the end is a doable job even for very large data <a href=\"https://unix.stackexchange.com/q/485638/8020\">unix.stackexchange.com/q/485638/8020</a>",
    "score" : 2,
    "owner" : {
      "account_id" : 50585,
      "reputation" : 43459,
      "user_id" : 150978,
      "user_type" : "registered",
      "accept_rate" : 78,
      "profile_image" : "https://www.gravatar.com/avatar/feadc214792e2581c3c750140e3eb2c7?s=256&d=identicon&r=PG",
      "display_name" : "Robert",
      "link" : "https://stackoverflow.com/users/150978/robert"
    },
    "creation_date" : 1754515589,
    "content_license" : "CC BY-SA 4.0"
  }, {
    "comment_id" : 140647434,
    "post_id" : 79727681,
    "body" : "@SedJ601 Thank you for the other method as well! I may have to try the SQLite database method to see how it performs. It would be another requirement for a user, to install and run SQLite database. But it&#39;s very intriguing. I see what you are doing with the multiple files with the starting letter, etc., to chunk the data. However, I think the External Sorting would ultimately work similarly, and it would not vary along with commonality of different letters.",
    "score" : 1,
    "owner" : {
      "account_id" : 709632,
      "reputation" : 1053,
      "user_id" : 620054,
      "user_type" : "registered",
      "accept_rate" : 70,
      "profile_image" : "https://www.gravatar.com/avatar/b9980ecabf192a4b53ff1d541a7b5229?s=256&d=identicon&r=PG",
      "display_name" : "Michael K",
      "link" : "https://stackoverflow.com/users/620054/michael-k"
    },
    "creation_date" : 1754512353,
    "content_license" : "CC BY-SA 4.0"
  }, {
    "comment_id" : 140647334,
    "post_id" : 79727681,
    "body" : "You can use some of the ideas to create tables in a database for the first approach. That would probably speed things up.",
    "score" : 1,
    "owner" : {
      "account_id" : 2819940,
      "reputation" : 14002,
      "user_id" : 2423906,
      "user_type" : "registered",
      "accept_rate" : 100,
      "profile_image" : "https://i.sstatic.net/rkpYv.jpg?s=256",
      "display_name" : "SedJ601",
      "link" : "https://stackoverflow.com/users/2423906/sedj601"
    },
    "creation_date" : 1754509867,
    "content_license" : "CC BY-SA 4.0"
  }, {
    "comment_id" : 140647333,
    "post_id" : 79727681,
    "body" : "Here is how I would tackle that without a database. I would go through the first file and write all the A&#39;s to one file, B&#39;s to one file ... Z&#39;s to one file. You may have to be more detailed with some letters. Letters that naturally start more words, I would break up as AA-AC has one file, AD-AG has one file. From there, I would then do the same for the second file. I would write to the same exact files. I would then go through each file and sort it, and remove duplicates. Lastly, I would write all the files back to one big file.",
    "score" : 2,
    "owner" : {
      "account_id" : 2819940,
      "reputation" : 14002,
      "user_id" : 2423906,
      "user_type" : "registered",
      "accept_rate" : 100,
      "profile_image" : "https://i.sstatic.net/rkpYv.jpg?s=256",
      "display_name" : "SedJ601",
      "link" : "https://stackoverflow.com/users/2423906/sedj601"
    },
    "creation_date" : 1754509863,
    "content_license" : "CC BY-SA 4.0"
  }, {
    "comment_id" : 140647320,
    "post_id" : 79727681,
    "body" : "@Robert Good question. I did see crypto hashing mentioned in another question for duplicate checks. These lines will mostly be &quot;short&quot;. In my example file, the line average is 7.6 characters (at least for Excel&#39;s max rows). I will likely set a max of 20-25 characters, and discard longer entries. MD5 hashes would be longer than all the entries. That would be larger than the input, unless there were a high percentage of duplicates.",
    "score" : 0,
    "owner" : {
      "account_id" : 709632,
      "reputation" : 1053,
      "user_id" : 620054,
      "user_type" : "registered",
      "accept_rate" : 70,
      "profile_image" : "https://www.gravatar.com/avatar/b9980ecabf192a4b53ff1d541a7b5229?s=256&d=identicon&r=PG",
      "display_name" : "Michael K",
      "link" : "https://stackoverflow.com/users/620054/michael-k"
    },
    "creation_date" : 1754509507,
    "content_license" : "CC BY-SA 4.0"
  }, {
    "comment_id" : 140647289,
    "post_id" : 79727681,
    "body" : "@SedJ601 Interesting. I guess that would make me curious how a database such as SQLite actually does a duplicate check. It may be similar. But that method sounds like it may be less code. Thanks!",
    "score" : 1,
    "owner" : {
      "account_id" : 709632,
      "reputation" : 1053,
      "user_id" : 620054,
      "user_type" : "registered",
      "accept_rate" : 70,
      "profile_image" : "https://www.gravatar.com/avatar/b9980ecabf192a4b53ff1d541a7b5229?s=256&d=identicon&r=PG",
      "display_name" : "Michael K",
      "link" : "https://stackoverflow.com/users/620054/michael-k"
    },
    "creation_date" : 1754508925,
    "content_license" : "CC BY-SA 4.0"
  }, {
    "comment_id" : 140647280,
    "post_id" : 79727681,
    "body" : "How long is each line in average? For saving if you have already added one line you don&#39;t need to save the whole line, just it&#39;s hash, e.g. md5 or sha1. Depending on the original line length that can save you enough memory so you can simply process the files line by line directly writing the out file and saving in memory the hashes of already written lines.",
    "score" : 2,
    "owner" : {
      "account_id" : 50585,
      "reputation" : 43459,
      "user_id" : 150978,
      "user_type" : "registered",
      "accept_rate" : 78,
      "profile_image" : "https://www.gravatar.com/avatar/feadc214792e2581c3c750140e3eb2c7?s=256&d=identicon&r=PG",
      "display_name" : "Robert",
      "link" : "https://stackoverflow.com/users/150978/robert"
    },
    "creation_date" : 1754508730,
    "content_license" : "CC BY-SA 4.0"
  }, {
    "comment_id" : 140647258,
    "post_id" : 79727681,
    "body" : "@LMC It is possible each input file has duplicates within the same file. Removing these duplicates may be a lower priority than duplicates between files, so this can be flexible if there is a strong benefit to leaving them. Yes, the data is human readable.",
    "score" : 0,
    "owner" : {
      "account_id" : 709632,
      "reputation" : 1053,
      "user_id" : 620054,
      "user_type" : "registered",
      "accept_rate" : 70,
      "profile_image" : "https://www.gravatar.com/avatar/b9980ecabf192a4b53ff1d541a7b5229?s=256&d=identicon&r=PG",
      "display_name" : "Michael K",
      "link" : "https://stackoverflow.com/users/620054/michael-k"
    },
    "creation_date" : 1754508331,
    "content_license" : "CC BY-SA 4.0"
  }, {
    "comment_id" : 140647184,
    "post_id" : 79727681,
    "body" : "Does each file contain duplicates? Is the data human readable? Divide an conquer is probably worth a try.",
    "score" : 3,
    "owner" : {
      "account_id" : 3377022,
      "reputation" : 14393,
      "user_id" : 2834978,
      "user_type" : "registered",
      "profile_image" : "https://www.gravatar.com/avatar/ff9af10001f37bc0de566ca2caf2f558?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name" : "LMC",
      "link" : "https://stackoverflow.com/users/2834978/lmc"
    },
    "creation_date" : 1754506430,
    "content_license" : "CC BY-SA 4.0"
  }, {
    "comment_id" : 140647183,
    "post_id" : 79727681,
    "body" : "Add one file to a database table. Add the next file to the same database table. Don&#39;t allow duplicates in the database table. Once that&#39;s done, write the database back to a file. I would use something like SQLite. I have never tried anything that big, so I don&#39;t know if it will work. I am just throwing ideas out.",
    "score" : 2,
    "owner" : {
      "account_id" : 2819940,
      "reputation" : 14002,
      "user_id" : 2423906,
      "user_type" : "registered",
      "accept_rate" : 100,
      "profile_image" : "https://i.sstatic.net/rkpYv.jpg?s=256",
      "display_name" : "SedJ601",
      "link" : "https://stackoverflow.com/users/2423906/sedj601"
    },
    "creation_date" : 1754506430,
    "content_license" : "CC BY-SA 4.0"
  } ],
  "answer_comments" : { }
}