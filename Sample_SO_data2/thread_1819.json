{
  "question" : {
    "question_id" : 79675115,
    "title" : "How to dynamically load JARs in a SparkApplication from PySpark",
    "body" : "<p>I am writing a framework on top of PySpark 3.5.5.</p>\n<p>My code tries to determine during runtime which JARs are required for certain operations and then load them when creating SparkSessions.\nWhile this all works nicely on a local driver, it fails when submitting the job via a SparkApplication to the Kubeflow Spark Operator (I'm running on Minikube).\nI have created my own Docker image base on the standard &quot;spark:3.5.5-java17&quot; that adds my framework code and all the required JARs into a custom location (adding them to /opt/spark/jars causes all of them to be loaded, which is bad for me even though all my test code works when I do).</p>\n<p>I have tried everything I could find to make this work but failed thus far.</p>\n<p>When creating the SparkSession from my Python code I configure &quot;spark.jars&quot;, &quot;spark.driver.extraClassPath&quot;, &quot;spark.executor.extraClassPath&quot; and even tried using 'addJar' via the Java RPC but while the log shows that the driver picks up on the JARs and loads them, the executors report ClassNotFoundException for classes coming from the JARs.</p>\n<p>I tried setting the above with absolute paths to the JARS, with &quot;local://&quot; prefix added, using ',' or ':' as a separator and any combination of these but still can't get it to work.</p>\n<p>I know I can add the list of JARs to the props for the spec of the SparkApplication CRD but I'm looking for a solution that would allow my code within the SparkApplication to dynamically determine what is required and load only the relevant JARs.</p>\n<p>Is there anything else I can try?</p>\n<p>Thanks!</p>\n",
    "tags" : [ "java", "apache-spark", "pyspark" ],
    "owner" : {
      "account_id" : 42666495,
      "reputation" : 1,
      "user_id" : 30863656,
      "user_type" : "registered",
      "profile_image" : "https://www.gravatar.com/avatar/0cacab613b115b87db2b6b2e6d821bbf?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name" : "Ettieane Ciscov",
      "link" : "https://stackoverflow.com/users/30863656/ettieane-ciscov"
    },
    "is_answered" : false,
    "view_count" : 83,
    "answer_count" : 0,
    "score" : 0,
    "last_activity_date" : 1750592635,
    "creation_date" : 1750592635,
    "link" : "https://stackoverflow.com/questions/79675115/how-to-dynamically-load-jars-in-a-sparkapplication-from-pyspark",
    "content_license" : "CC BY-SA 4.0"
  },
  "answers" : [ ],
  "question_comments" : [ {
    "comment_id" : 140534813,
    "post_id" : 79675115,
    "body" : "This sounds like a Docker/K8s mount issue not a Spark issue. If it works locally, you likely need to mount the PVC correctly.",
    "score" : 0,
    "owner" : {
      "account_id" : 12987761,
      "reputation" : 636,
      "user_id" : 9388056,
      "user_type" : "registered",
      "profile_image" : "https://i.sstatic.net/lwIgN.jpg?s=256",
      "display_name" : "Frank",
      "link" : "https://stackoverflow.com/users/9388056/frank"
    },
    "creation_date" : 1750675247,
    "content_license" : "CC BY-SA 4.0"
  }, {
    "comment_id" : 140533246,
    "post_id" : 79675115,
    "body" : "Only one SparkContext can exist at a time, so what do you mean by dynamically add?",
    "score" : 0,
    "owner" : {
      "account_id" : 2671330,
      "reputation" : 193048,
      "user_id" : 2308683,
      "user_type" : "registered",
      "accept_rate" : 90,
      "profile_image" : "https://www.gravatar.com/avatar/fb8f5877d244f223b4b6d29e0afb3a4e?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name" : "OneCricketeer",
      "link" : "https://stackoverflow.com/users/2308683/onecricketeer"
    },
    "creation_date" : 1750610362,
    "content_license" : "CC BY-SA 4.0"
  } ],
  "answer_comments" : { }
}