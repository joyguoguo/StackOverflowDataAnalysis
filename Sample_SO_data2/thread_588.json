{
  "question" : {
    "question_id" : 79794432,
    "title" : "dataproc serverless slow consume kafka topic",
    "body" : "<p>I use dataproc serverless with java API to read kafka topic. The topic has only 2 partitions.\nThe topic receives 200 msg/sec.\nAfter reading messages I repartition to 100, transform the data then write to BQ.\nSince I repartition I see two stages one with 2 tasks to read data and a second one with 10 that transforms and writes the data.\nWhen having\nstreaming.max.offsets.per.trigger=500\nstreaming.min.offsets.per.trigger=100\nThe task of reading is varying a lot in time between 7sec and 1min50.\nWhile the task that transforms an dwrites data takes around 10 sec.\nAny idea about why it's taking too much time to read data and how to optimize the code ?</p>\n<pre><code>Dataset&lt;Row&gt; dfr = spark\n                .readStream()\n                .format(&quot;org.apache.spark.sql.kafka010.KafkaSourceProvider&quot;)\n                .option(&quot;kafka.bootstrap.servers&quot;, kafkaServers)\n                .option(&quot;kafka.sasl.kerberos.service.name&quot;, &quot;kafka&quot;)\n                .option(&quot;kafka.sasl.mechanism&quot;, &quot;GSSAPI&quot;)\n                .option(&quot;kafka.security.protocol&quot;, &quot;SASL_SSL&quot;)\n                .option(&quot;kafka.ssl.truststore.location&quot;, trustStoreName)\n                .option(&quot;kafka.ssl.truststore.password&quot;, truststorePassword)\n                .option(&quot;kafka.ssl.truststore.type&quot;, &quot;JKS&quot;)\n                .option(&quot;startingOffsets&quot;, &quot;latest&quot;)\n                .option(&quot;kafka.max.partition.fetch.bytes&quot;, &quot;209715200&quot;)  // 200MB per partition\n                .option(&quot;kafka.fetch.max.bytes&quot;, &quot;1048576000&quot;)            // 1000MB total\n                .option(&quot;subscribe&quot;, kafkaTopic)\n                .option(&quot;maxOffsetsPerTrigger&quot;, maxOffsets)\n                .option(&quot;minOffsetsPerTrigger&quot;, minOffsets)\n                .option(&quot;failOnDataLoss&quot;, &quot;false&quot;)\n                .option(&quot;kafka.request.timeout.ms&quot;, 300000)\n                .option(&quot;kafka.session.timeout.ms&quot;, 60000)\n                .load();\n\n        Dataset&lt;Row&gt; dfr2 = dfr.selectExpr(\n                &quot;CAST(topic as STRING) as topic&quot;, &quot;CAST(key AS STRING) AS key&quot;,\n                &quot;CAST(value AS STRING) AS xml&quot;,\n                &quot;timestamp&quot;, &quot;partition&quot;, &quot;offset&quot;).repartition(10);\n\n        StructType outSchema = new StructType()\n                .add(&quot;key&quot;, DataTypes.StringType)\n                .add(&quot;topic&quot;, DataTypes.StringType)\n                .add(&quot;partition&quot;, DataTypes.IntegerType)\n                .add(&quot;offset&quot;, DataTypes.LongType)\n                .add(&quot;JSON_COL&quot;, DataTypes.StringType)\n                .add(&quot;DAT_MAJ_DWH&quot;, DataTypes.StringType);\n\n        // Create proper encoder - cast the result to Encoder&lt;Row&gt;\n        Encoder&lt;Row&gt; encoder = Encoders.row(outSchema);\n\n        Dataset&lt;Row&gt; jsonified = dfr2.mapPartitions(\n                (MapPartitionsFunction&lt;Row, Row&gt;) (Iterator&lt;Row&gt; it) -&gt; {\n                    List&lt;Row&gt; out = new ArrayList&lt;&gt;();\n                    DocumentBuilder builder = XML_BUILDER.get();\n                    while (it.hasNext()) {\n                        Row r = it.next();\n                        String topic = r.getString(0);\n                        String key = r.getString(1);\n                        String xml = r.getString(2);\n                        int part = r.getInt(4);\n                        long offset = r.getLong(5);\n                        String json = null;\n                        try {\n                            builder.reset();\n                            // Pass the XML string, not the Document\n                            json = BusMessageXmlJson.toJson(xml);\n                        } catch (Exception ex) {\n                            System.err.println(&quot;XML parse error partition=&quot; + part +\n                                                       &quot; offset=&quot; + offset + &quot; msg=&quot; + ex.getMessage());\n                        }\n                        String ts = r.getTimestamp(3).toInstant().toString();\n                        out.add(RowFactory.create(key, topic, part, offset, json, ts));\n                    }\n                    return out.iterator();\n                },\n                encoder\n        ).withColumn(\n                &quot;DAT_MAJ_DWH&quot;,\n                date_format(to_timestamp(col(&quot;DAT_MAJ_DWH&quot;)), &quot;yyyy-MM-dd'T'HH:mm:ss.SSSSSS&quot;)\n        ).select(&quot;key&quot;,&quot;topic&quot;,&quot;partition&quot;,&quot;offset&quot;,&quot;JSON_COL&quot;,&quot;DAT_MAJ_DWH&quot;);\n\n\n        StreamingQuery query = jsonified\n                .writeStream()\n                .queryName(&quot;spark-sdh-ndc-streaming-query&quot;)\n                .foreachBatch((batchDF, batchId) -&gt; {\n\n                    // Write this batch to BigQuery using batch API\n                    batchDF.select(&quot;JSON_COL&quot;, &quot;DAT_MAJ_DWH&quot;).write()\n                            .format(&quot;bigquery&quot;)\n                            .option(&quot;temporaryGcsBucket&quot;, tempBucket)\n                            .option(&quot;table&quot;, bigQueryTable)\n                            .option(&quot;createDisposition&quot;, &quot;CREATE_IF_NEEDED&quot;)\n                            .option(&quot;intermediateFormat&quot;, &quot;avro&quot;)\n                            .option(&quot;writeMethod&quot;, &quot;indirect&quot;)\n                            .option(&quot;allowFieldAddition&quot;, &quot;true&quot;)\n                            .option(&quot;allowFieldRelaxation&quot;, &quot;true&quot;)\n                            .mode(SaveMode.Append)\n                            .save();\n\n                    // 2. Commit offsets to Kafka AFTER successful write\n                    commitOffsetsToKafka(batchDF, kafkaServers, trustStoreName, truststorePassword, consumerGroup);\n\n                    System.out.println(&quot;Batch &quot; + batchId + &quot; written successfully&quot;);\n                })\n                .option(&quot;checkpointLocation&quot;, checkpointPath)\n                .trigger(Trigger.ProcessingTime(triggerInterval))\n                .start();\n        System.out.println(&quot;Streaming query started successfully!&quot;);\n        System.out.println(&quot;Query ID: &quot; + query.id());\n        System.out.println(&quot;Waiting for termination... (Ctrl+C to stop)&quot;);\n\n        query.awaitTermination();\n</code></pre>\n<p><a href=\"https://i.sstatic.net/eSgG6zvI.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/eSgG6zvI.png\" alt=\"enter image description here\" /></a></p>\n",
    "tags" : [ "java", "apache-spark", "spark-structured-streaming", "dataproc" ],
    "owner" : {
      "account_id" : 7999012,
      "reputation" : 926,
      "user_id" : 6034446,
      "user_type" : "registered",
      "profile_image" : "https://www.gravatar.com/avatar/eee81eb4f64c8211d5a40f2a8ff968ca?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name" : "firsni",
      "link" : "https://stackoverflow.com/users/6034446/firsni"
    },
    "is_answered" : false,
    "view_count" : 85,
    "answer_count" : 1,
    "score" : 0,
    "last_activity_date" : 1762251013,
    "creation_date" : 1760898609,
    "link" : "https://stackoverflow.com/questions/79794432/dataproc-serverless-slow-consume-kafka-topic",
    "content_license" : "CC BY-SA 4.0"
  },
  "answers" : [ {
    "answer_id" : 79807550,
    "question_id" : 79794432,
    "body" : "<p>I think removing <code>streaming.min.offsets.per.trigger=100</code> or keeping it less than 80 since you have 80msg/sec would ensure tasks do not wait to reach 100msg to read per batch and read right at next batch time. Also, you can try <code>minPartitions</code> (you can play around with the value. Giving around 1.5-2 times the no.of partitions to start with) config  which can help create more parallel tasks to read per partition.</p>\n",
    "score" : 0,
    "is_accepted" : false,
    "owner" : {
      "account_id" : 1383279,
      "reputation" : 1511,
      "user_id" : 1315833,
      "user_type" : "registered",
      "accept_rate" : 80,
      "profile_image" : "https://www.gravatar.com/avatar/00485e9b12c3818ec67071c2bb197a10?s=256&d=identicon&r=PG",
      "display_name" : "Vindhya G",
      "link" : "https://stackoverflow.com/users/1315833/vindhya-g"
    },
    "creation_date" : 1762145926,
    "last_activity_date" : 1762145926,
    "content_license" : "CC BY-SA 4.0"
  } ],
  "question_comments" : [ ],
  "answer_comments" : {
    "79807550" : [ {
      "comment_id" : 140855214,
      "post_id" : 79807550,
      "body" : "okay so the time 7 sec to 1 min 50 sec is without repartition?",
      "score" : 0,
      "owner" : {
        "account_id" : 1383279,
        "reputation" : 1511,
        "user_id" : 1315833,
        "user_type" : "registered",
        "accept_rate" : 80,
        "profile_image" : "https://www.gravatar.com/avatar/00485e9b12c3818ec67071c2bb197a10?s=256&d=identicon&r=PG",
        "display_name" : "Vindhya G",
        "link" : "https://stackoverflow.com/users/1315833/vindhya-g"
      },
      "creation_date" : 1763179313,
      "content_license" : "CC BY-SA 4.0"
    }, {
      "comment_id" : 140850006,
      "post_id" : 79807550,
      "body" : "I did the repartition in order to split the stage of reading from transformation so that I figure out which is taking time. But I plan to remove it in prod.",
      "score" : 0,
      "owner" : {
        "account_id" : 7999012,
        "reputation" : 926,
        "user_id" : 6034446,
        "user_type" : "registered",
        "profile_image" : "https://www.gravatar.com/avatar/eee81eb4f64c8211d5a40f2a8ff968ca?s=256&d=identicon&r=PG&f=y&so-version=2",
        "display_name" : "firsni",
        "link" : "https://stackoverflow.com/users/6034446/firsni"
      },
      "creation_date" : 1762939152,
      "content_license" : "CC BY-SA 4.0"
    }, {
      "comment_id" : 140837036,
      "post_id" : 79807550,
      "body" : "As far as I have seen bottlenecks are mostly in dataproc than kafka unless kafka is overloaded with less partitions &amp; storage. I see you do repartitions right after reading which are handled by the same tasks that reads from kafka to do shuffle write.  It does some aggregations within the partitions before it ends. So repartition could be the bottleneck. You can try removing it and see if the read becomes faster. Usually Spark handles partitions well on its own. Is there any particular reason you are using repartition? Also Spark UI can help. Timelines of tasks scheduled, streaming metrics etc",
      "score" : 0,
      "owner" : {
        "account_id" : 1383279,
        "reputation" : 1511,
        "user_id" : 1315833,
        "user_type" : "registered",
        "accept_rate" : 80,
        "profile_image" : "https://www.gravatar.com/avatar/00485e9b12c3818ec67071c2bb197a10?s=256&d=identicon&r=PG",
        "display_name" : "Vindhya G",
        "link" : "https://stackoverflow.com/users/1315833/vindhya-g"
      },
      "creation_date" : 1762313766,
      "content_license" : "CC BY-SA 4.0"
    }, {
      "comment_id" : 140836205,
      "post_id" : 79807550,
      "body" : "Thx for your response actually it&#39;s more than 80 msg/sec one topic is 200 msg/sec and second 2000 msg/sec. I tried the minPartitions it made things better but not enough. how do you think I can figure out if the problem is on kafka side or dataproc side ?",
      "score" : 0,
      "owner" : {
        "account_id" : 7999012,
        "reputation" : 926,
        "user_id" : 6034446,
        "user_type" : "registered",
        "profile_image" : "https://www.gravatar.com/avatar/eee81eb4f64c8211d5a40f2a8ff968ca?s=256&d=identicon&r=PG&f=y&so-version=2",
        "display_name" : "firsni",
        "link" : "https://stackoverflow.com/users/6034446/firsni"
      },
      "creation_date" : 1762275355,
      "content_license" : "CC BY-SA 4.0"
    } ]
  }
}