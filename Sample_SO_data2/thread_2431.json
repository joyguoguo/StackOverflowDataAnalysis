{
  "question" : {
    "question_id" : 79623158,
    "title" : "Unpredictable exception while querying clickhouse database via JDBC driver",
    "body" : "<p>I'm facing a problem that doesn't always occur, and therefore, it is difficult to reproduce and resolve.</p>\n<p>Here is some key information about the env:</p>\n<ul>\n<li>OS: Rocky Linux 9.2 (Blue Onyx)</li>\n<li>Language: Scala 2.13.13</li>\n</ul>\n<p>Main dependencies involved:</p>\n<pre class=\"lang-none prettyprint-override\"><code>&quot;com.clickhouse&quot; % &quot;clickhouse-jdbc&quot; % &quot;0.8.3&quot;\n&quot;org.apache.spark&quot; %% &quot;spark-XXX&quot; % &quot;3.5.1&quot;\n</code></pre>\n<p>For some queries (and not always the same ones) this exception occurs:</p>\n<pre class=\"lang-none prettyprint-override\"><code>java.sql.SQLException: Failed to close response\n    at com.clickhouse.jdbc.internal.ExceptionUtils.toSqlState(ExceptionUtils.java:66)\n    at com.clickhouse.jdbc.internal.ExceptionUtils.toSqlState(ExceptionUtils.java:39)\n    at com.clickhouse.jdbc.ResultSetImpl.close(ResultSetImpl.java:117)\n    at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD.close$1(JDBCRDD.scala:195)\n    at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD.$anonfun$compute$6(JDBCRDD.scala:225)\n    at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD.$anonfun$compute$6$adapted(JDBCRDD.scala:225)\n    at org.apache.spark.TaskContext$$anon$1.onTaskCompletion(TaskContext.scala:137)\n    at org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1(TaskContextImpl.scala:144)\n    at org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1$adapted(TaskContextImpl.scala:144)\n    at org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:199)\n    at org.apache.spark.TaskContextImpl.invokeTaskCompletionListeners(TaskContextImpl.scala:144)\n    at org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:137)\n    at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:177)\n    at org.apache.spark.scheduler.Task.run(Task.scala:141)\n    at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n    at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n    at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n    at java.lang.Thread.run(Thread.java:750)\nCaused by: com.clickhouse.client.api.ClientException: Failed to close response\n    at com.clickhouse.client.api.query.QueryResponse.close(QueryResponse.java:70)\n    at com.clickhouse.jdbc.ResultSetImpl.close(ResultSetImpl.java:106)\n    ... 19 more\nCaused by: org.apache.hc.core5.http.ConnectionClosedException: Premature end of chunk coded message body: closing chunk expected\n    at org.apache.hc.core5.http.impl.io.ChunkedInputStream.getChunkSize(ChunkedInputStream.java:263)\n    at org.apache.hc.core5.http.impl.io.ChunkedInputStream.nextChunk(ChunkedInputStream.java:222)\n    at org.apache.hc.core5.http.impl.io.ChunkedInputStream.read(ChunkedInputStream.java:147)\n    at org.apache.hc.core5.http.impl.io.ChunkedInputStream.close(ChunkedInputStream.java:314)\n    at org.apache.hc.core5.io.Closer.close(Closer.java:48)\n    at org.apache.hc.core5.http.impl.io.IncomingHttpEntity.close(IncomingHttpEntity.java:112)\n    at org.apache.hc.core5.http.io.entity.HttpEntityWrapper.close(HttpEntityWrapper.java:120)\n    at org.apache.hc.client5.http.impl.classic.ResponseEntityProxy.close(ResponseEntityProxy.java:180)\n    at com.clickhouse.client.api.internal.LZ4Entity.close(LZ4Entity.java:104)\n    at org.apache.hc.core5.io.Closer.close(Closer.java:48)\n    at org.apache.hc.core5.http.message.BasicClassicHttpResponse.close(BasicClassicHttpResponse.java:93)\n    at org.apache.hc.client5.http.impl.classic.CloseableHttpResponse.close(CloseableHttpResponse.java:200)\n    at com.clickhouse.client.api.query.QueryResponse.close(QueryResponse.java:68)\n    ... 20 more\n25/05/14 05:49:23 ERROR Executor: Exception in task 0.0 in stage 3684.0 (TID 119142)\njava.sql.SQLException: Failed to read value for column date\n    at com.clickhouse.jdbc.internal.ExceptionUtils.toSqlState(ExceptionUtils.java:66)\n    at com.clickhouse.jdbc.internal.ExceptionUtils.toSqlState(ExceptionUtils.java:39)\n    at com.clickhouse.jdbc.ResultSetImpl.next(ResultSetImpl.java:83)\n    at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anon$1.getNext(JdbcUtils.scala:354)\n    at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anon$1.getNext(JdbcUtils.scala:340)\n    at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n    at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n    at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n    at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n    at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n    at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n    at org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:88)\n    at org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:80)\n    at org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:290)\n    at org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:287)\n    at org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n    at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n    at org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\n    at org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n    at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n    at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n    at org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n    at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n    at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n    at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n    at org.apache.spark.scheduler.Task.run(Task.scala:141)\n    at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n    at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n    at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n    at java.lang.Thread.run(Thread.java:750)\nCaused by: com.clickhouse.client.api.ClientException: Failed to read value for column date\n    at com.clickhouse.client.api.data_formats.internal.BinaryStreamReader.readValue(BinaryStreamReader.java:241)\n    at com.clickhouse.client.api.data_formats.internal.BinaryStreamReader.readValue(BinaryStreamReader.java:82)\n    at com.clickhouse.client.api.data_formats.internal.AbstractBinaryFormatReader.readRecord(AbstractBinaryFormatReader.java:163)\n    at com.clickhouse.client.api.data_formats.internal.AbstractBinaryFormatReader.readNextRecord(AbstractBinaryFormatReader.java:208)\n    at com.clickhouse.client.api.data_formats.internal.AbstractBinaryFormatReader.next(AbstractBinaryFormatReader.java:229)\n    at com.clickhouse.jdbc.ResultSetImpl.next(ResultSetImpl.java:81)\n    ... 32 more\nCaused by: org.apache.hc.core5.http.ConnectionClosedException: Premature end of chunk coded message body: closing chunk expected\n    at org.apache.hc.core5.http.impl.io.ChunkedInputStream.getChunkSize(ChunkedInputStream.java:263)\n    at org.apache.hc.core5.http.impl.io.ChunkedInputStream.nextChunk(ChunkedInputStream.java:222)\n    at org.apache.hc.core5.http.impl.io.ChunkedInputStream.read(ChunkedInputStream.java:183)\n    at org.apache.hc.core5.http.io.EofSensorInputStream.read(EofSensorInputStream.java:135)\n    at com.clickhouse.client.api.internal.ClickHouseLZ4InputStream.readFully(ClickHouseLZ4InputStream.java:81)\n    at com.clickhouse.client.api.internal.ClickHouseLZ4InputStream.refill(ClickHouseLZ4InputStream.java:120)\n    at com.clickhouse.client.api.internal.ClickHouseLZ4InputStream.read(ClickHouseLZ4InputStream.java:62)\n    at com.clickhouse.client.api.data_formats.internal.BinaryStreamReader.readNBytes(BinaryStreamReader.java:541)\n    at com.clickhouse.client.api.data_formats.internal.BinaryStreamReader.readShortLE(BinaryStreamReader.java:328)\n    at com.clickhouse.client.api.data_formats.internal.BinaryStreamReader.readUnsignedShortLE(BinaryStreamReader.java:421)\n    at com.clickhouse.client.api.data_formats.internal.BinaryStreamReader.readDate(BinaryStreamReader.java:883)\n    at com.clickhouse.client.api.data_formats.internal.BinaryStreamReader.readDate(BinaryStreamReader.java:871)\n    at com.clickhouse.client.api.data_formats.internal.BinaryStreamReader.readValue(BinaryStreamReader.java:173)\n    ... 37 more\n25/05/14 05:49:23 WARN TaskSetManager: Lost task 0.0 in stage 3684.0 (TID 119142) (EEE-CCCC.BBB.AAA.ZZZ.XXXXXX.YYY executor driver): java.sql.SQLException: Failed to read value for column date\n    at com.clickhouse.jdbc.internal.ExceptionUtils.toSqlState(ExceptionUtils.java:66)\n    at com.clickhouse.jdbc.internal.ExceptionUtils.toSqlState(ExceptionUtils.java:39)\n    at com.clickhouse.jdbc.ResultSetImpl.next(ResultSetImpl.java:83)\n    at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anon$1.getNext(JdbcUtils.scala:354)\n    at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anon$1.getNext(JdbcUtils.scala:340)\n    at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n    at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n    at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n    at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n    at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n    at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n    at org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:88)\n    at org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:80)\n    at org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:290)\n    at org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:287)\n    at org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n    at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n    at org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\n    at org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n    at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n    at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n    at org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n    at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n    at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n    at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n    at org.apache.spark.scheduler.Task.run(Task.scala:141)\n    at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n    at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n    at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n    at java.lang.Thread.run(Thread.java:750)\nCaused by: com.clickhouse.client.api.ClientException: Failed to read value for column date\n    at com.clickhouse.client.api.data_formats.internal.BinaryStreamReader.readValue(BinaryStreamReader.java:241)\n    at com.clickhouse.client.api.data_formats.internal.BinaryStreamReader.readValue(BinaryStreamReader.java:82)\n    at com.clickhouse.client.api.data_formats.internal.AbstractBinaryFormatReader.readRecord(AbstractBinaryFormatReader.java:163)\n    at com.clickhouse.client.api.data_formats.internal.AbstractBinaryFormatReader.readNextRecord(AbstractBinaryFormatReader.java:208)\n    at com.clickhouse.client.api.data_formats.internal.AbstractBinaryFormatReader.next(AbstractBinaryFormatReader.java:229)\n    at com.clickhouse.jdbc.ResultSetImpl.next(ResultSetImpl.java:81)\n    ... 32 more\nCaused by: org.apache.hc.core5.http.ConnectionClosedException: Premature end of chunk coded message body: closing chunk expected\n    at org.apache.hc.core5.http.impl.io.ChunkedInputStream.getChunkSize(ChunkedInputStream.java:263)\n    at org.apache.hc.core5.http.impl.io.ChunkedInputStream.nextChunk(ChunkedInputStream.java:222)\n    at org.apache.hc.core5.http.impl.io.ChunkedInputStream.read(ChunkedInputStream.java:183)\n    at org.apache.hc.core5.http.io.EofSensorInputStream.read(EofSensorInputStream.java:135)\n    at com.clickhouse.client.api.internal.ClickHouseLZ4InputStream.readFully(ClickHouseLZ4InputStream.java:81)\n    at com.clickhouse.client.api.internal.ClickHouseLZ4InputStream.refill(ClickHouseLZ4InputStream.java:120)\n    at com.clickhouse.client.api.internal.ClickHouseLZ4InputStream.read(ClickHouseLZ4InputStream.java:62)\n    at com.clickhouse.client.api.data_formats.internal.BinaryStreamReader.readNBytes(BinaryStreamReader.java:541)\n    at com.clickhouse.client.api.data_formats.internal.BinaryStreamReader.readShortLE(BinaryStreamReader.java:328)\n    at com.clickhouse.client.api.data_formats.internal.BinaryStreamReader.readUnsignedShortLE(BinaryStreamReader.java:421)\n    at com.clickhouse.client.api.data_formats.internal.BinaryStreamReader.readDate(BinaryStreamReader.java:883)\n    at com.clickhouse.client.api.data_formats.internal.BinaryStreamReader.readDate(BinaryStreamReader.java:871)\n    at com.clickhouse.client.api.data_formats.internal.BinaryStreamReader.readValue(BinaryStreamReader.java:173)\n    ... 37 more\n</code></pre>\n<p>Last time the exception occurred for a query with a result set of 100K rows circa (8 columns per row).</p>\n<p>It is often sufficient to repeat the query few times, but I would like a better solution to the problem.</p>\n<p>At the moment I do not have any parameter set at the JDBC level (i.e., in the JDBC URL), everything is set to default.</p>\n<p>Any recommendation?</p>\n",
    "tags" : [ "java", "scala", "apache-spark", "jdbc", "clickhouse" ],
    "owner" : {
      "account_id" : 8824481,
      "reputation" : 81,
      "user_id" : 6593151,
      "user_type" : "registered",
      "profile_image" : "https://i.sstatic.net/Vub1b.jpg?s=256",
      "display_name" : "ronnie",
      "link" : "https://stackoverflow.com/users/6593151/ronnie"
    },
    "is_answered" : false,
    "view_count" : 153,
    "answer_count" : 0,
    "score" : 0,
    "last_activity_date" : 1765732175,
    "creation_date" : 1747306375,
    "link" : "https://stackoverflow.com/questions/79623158/unpredictable-exception-while-querying-clickhouse-database-via-jdbc-driver",
    "content_license" : "CC BY-SA 4.0"
  },
  "answers" : [ ],
  "question_comments" : [ {
    "comment_id" : 140504274,
    "post_id" : 79623158,
    "body" : "I see multiple &quot;Failed to read value for column date&quot;. Is there something wrong with the date column?",
    "score" : 0,
    "owner" : {
      "account_id" : 1370666,
      "reputation" : 490,
      "user_id" : 1305959,
      "user_type" : "registered",
      "profile_image" : "https://www.gravatar.com/avatar/434b8f4a61d2d5a72ed8a83072023fc9?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name" : "Derek Chia",
      "link" : "https://stackoverflow.com/users/1305959/derek-chia"
    },
    "creation_date" : 1749613522,
    "content_license" : "CC BY-SA 4.0"
  } ],
  "answer_comments" : { }
}