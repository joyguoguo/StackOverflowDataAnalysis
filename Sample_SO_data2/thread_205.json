{
  "question" : {
    "question_id" : 79821921,
    "title" : "Data extraction from PDF",
    "body" : "<p>Dealing with large volumes of PDFs has been challenging lately. Our primary focus is accurately extracting data from PDFs and in some cases the volume can go up to 100,000 files in a single batch. Tesseract-based OCR is too slow for this use case, especially with mixed file sizes, image-heavy pages, and the need to extract data from specific regions of each document. On top of that, I often have to identify which pages need to be rearranged based on client instructions. I’m exploring faster alternatives and tools for high-volume OCR, page identification/reordering, and data extraction.</p>\n<p>If you’ve solved similar problems or have suggestions for better ways to handle this, I’d really appreciate your insights in the comments.</p>\n",
    "tags" : [ "python", "java", "artificial-intelligence" ],
    "owner" : {
      "account_id" : 26747337,
      "reputation" : 89,
      "user_id" : 20347223,
      "user_type" : "registered",
      "profile_image" : "https://www.gravatar.com/avatar/aaf01f1b26bbabec2c257ce75b2ed996?s=256&d=identicon&r=PG",
      "display_name" : "Ch Vamsi",
      "link" : "https://stackoverflow.com/users/20347223/ch-vamsi"
    },
    "is_answered" : true,
    "view_count" : 159,
    "answer_count" : 4,
    "score" : 1,
    "last_activity_date" : 1763476027,
    "creation_date" : 1763356009,
    "link" : "https://stackoverflow.com/questions/79821921/data-extraction-from-pdf",
    "content_license" : "CC BY-SA 4.0"
  },
  "answers" : [ {
    "answer_id" : 79822078,
    "question_id" : 79821921,
    "body" : "<p>How fast would you like to process those 100,000 files?</p>\n",
    "score" : 1,
    "is_accepted" : false,
    "owner" : {
      "account_id" : 1277793,
      "reputation" : 3556,
      "user_id" : 1232660,
      "user_type" : "registered",
      "accept_rate" : 75,
      "profile_image" : "https://www.gravatar.com/avatar/3843a2bbd046589aeba450315a996d8d?s=256&d=identicon&r=PG",
      "display_name" : "Jeyekomon",
      "link" : "https://stackoverflow.com/users/1232660/jeyekomon"
    },
    "creation_date" : 1763370931,
    "last_activity_date" : 1763370931,
    "content_license" : "CC BY-SA 4.0"
  }, {
    "answer_id" : 79822193,
    "question_id" : 79821921,
    "body" : "<p>how does your input look like? do you know in advance which the <strong>region</strong> of text?</p>\n",
    "score" : 1,
    "is_accepted" : false,
    "owner" : {
      "account_id" : 22225922,
      "reputation" : 5280,
      "user_id" : 16462878,
      "user_type" : "registered",
      "profile_image" : "https://www.gravatar.com/avatar/44635bb166b98fe3986b406543ef1ef6?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name" : "cards",
      "link" : "https://stackoverflow.com/users/16462878/cards"
    },
    "creation_date" : 1763376471,
    "last_activity_date" : 1763376471,
    "content_license" : "CC BY-SA 4.0"
  }, {
    "answer_id" : 79822208,
    "question_id" : 79821921,
    "body" : "<p>Hmm the core issue is number of humans since <a href=\"https://en.wikipedia.org/wiki/Optical_character_recognition\" rel=\"nofollow noreferrer\">OCR has been around for 110 years</a> and needs visual eyes on multiple times so if you have 100,000 documents you need a distributed system like Historic Census Verification or (The largest and most successful distributed computing project to date) <a href=\"https://seti.berkeley.edu/participate/\" rel=\"nofollow noreferrer\">SETI@Home</a>, where the job is done in one high speed instantaneous hit by 100,000 humans each doing one document. Alternatively, one human inspects the incoming and outgoing 100,000 time units. This always surprises those that think big data should be faster than little data without a data analyst knowledge of shortcuts. Identify the bottlenecks then rank how they might be overcome as Tesseract is as slow or as fast as OCR is.</p>\n<p>python myocr3.py 100images.pdf --pages 1-10 --max-procs 1<br />\nOCR done 10 pages in 21.13 seconds → 2.11 sec/page</p>\n<p>python myocr3.py 100images.pdf --pages 1-10 --max-procs 2<br />\nOCR done 10 pages in 15.20 seconds → 1.52 sec/page</p>\n<p>python myocr3.py 100images.pdf --pages 1-10 --max-procs 3<br />\nOCR done 10 pages in 11.87 seconds → 1.19 sec/page</p>\n<p>python myocr3.py 100images.pdf --pages 1-10 --max-procs 4<br />\nOCR done 10 pages in 11.08 seconds → 1.11 sec/page</p>\n<p>python myocr3.py 100images.pdf --pages 1-10 --max-procs 5<br />\nOCR done 10 pages in 8.29 seconds → 0.83 sec/page</p>\n<p>python myocr3.py 100images.pdf --pages 1-10 --max-procs 6<br />\nOCR done 10 pages in 8.79 seconds → 0.88 sec/page</p>\n<p>So, the more tessellar processors (OCR tile scanners) you buy either Human, Mechanical GPU or AI LLM the faster the task is completed.</p>\n<p>Disk I/O, memory BUS, or network speed can slow down even if you add more processors, Humans may be slower but more accurate. Hence large Human Distributed Models are used for more critical Big Data like Census Enumerations or Google Books etc.</p>\n<p>For the computing part Amdahl’s Law applies the more you can mechanise, the greater the return so for a task 50%=2 times 95%=20 times gain. But this task is not driven by mechanics more by desires.\nMachines can scan millions of pages, but humans must select areas, correct results, and place the tiles according to task priorities.</p>\n",
    "score" : 0,
    "is_accepted" : false,
    "owner" : {
      "account_id" : 14313923,
      "reputation" : 12621,
      "user_id" : 10802527,
      "user_type" : "registered",
      "profile_image" : "https://www.gravatar.com/avatar/09ba828ca91501c7202a014fd0388bc4?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name" : "K J",
      "link" : "https://stackoverflow.com/users/10802527/k-j"
    },
    "creation_date" : 1763377032,
    "last_activity_date" : 1763476027,
    "content_license" : "CC BY-SA 4.0"
  }, {
    "answer_id" : 79823457,
    "question_id" : 79821921,
    "body" : "<p>Usually I am anti cloud but I at this bulk and if you need it relatively fast. The best option is to use a cloud hosting provider to deploy 10-100 instances of the conversion program, then split the dataset evenly across each of the instances to convert it.</p>\n",
    "score" : 0,
    "is_accepted" : false,
    "owner" : {
      "account_id" : 29512571,
      "reputation" : 11,
      "user_id" : 22617290,
      "user_type" : "registered",
      "profile_image" : "https://i.sstatic.net/USl2x.png?s=256",
      "display_name" : "ocueye guy",
      "link" : "https://stackoverflow.com/users/22617290/ocueye-guy"
    },
    "creation_date" : 1763475807,
    "last_activity_date" : 1763475807,
    "content_license" : "CC BY-SA 4.0"
  } ],
  "question_comments" : [ ],
  "answer_comments" : { }
}