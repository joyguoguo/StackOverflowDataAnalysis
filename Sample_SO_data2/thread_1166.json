{
  "question" : {
    "question_id" : 79740213,
    "title" : "Kafka Manual commit scenario Springboot",
    "body" : "<p>We are using Spring Boot Kafka listener to consume messages from Kafka. Currently, we have the auto commit offset enabled and so the offset commits happen every 5 seconds (default auto commit interval). We are polling Kafka for a batch of 100 records at a time and take about 300 milliseconds to process the messages. In a regular scenario where there will be multiple polls of messages and one commit every 5 seconds as they get processed fairly quickly. Even if the service pod goes down before the next offset commit, the code is idempotent so any messages not committed yet will be republished/reprocessed in the event of this issue.</p>\n<p>However, I am thinking of a way to handle a remote scenario of the batch processing taking more than the default window of 5 seconds in which case the offset would be committed before the processing is completed. If the processing is completed fine, then there is no issue. but if there is any error with processing these messages which are already marked consumed, we will lose those messages.</p>\n<p>I am looking for ways to have this handled without losing the messages. I was thinking of changing the ack mode to manual and commit after every batch no matter how long it takes (of course less than the max.poll.interval) but this will increase the # of offset commits by 10 fold. I also thought of having an internal timer to acknowledge when the timer elapses say 5 seconds since the last poll. This way the commit won't happen when the current poll processing takes longer than 5 seconds and will be done only later.</p>\n<p>However, I am wondering if this leaves a possibility of we receiving a bunch of messages and they will be waiting to be committed until the next poll and if this is the last set of messages, then they will be waiting for a long time. I have also tried different ack modes like COUNT/COUNT_TIME but none of that are solving the use case here. Not sure how this can be handled.</p>\n<p>Any thoughts on this?</p>\n",
    "tags" : [ "java", "spring-boot", "apache-kafka", "spring-kafka", "kafka-consumer-api" ],
    "owner" : {
      "account_id" : 18044566,
      "reputation" : 325,
      "user_id" : 13115896,
      "user_type" : "registered",
      "profile_image" : "https://www.gravatar.com/avatar/6d35ea24f31161f110fed75caf9d2d04?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name" : "sg2000",
      "link" : "https://stackoverflow.com/users/13115896/sg2000"
    },
    "is_answered" : true,
    "view_count" : 160,
    "answer_count" : 2,
    "score" : 0,
    "last_activity_date" : 1756237834,
    "creation_date" : 1755623157,
    "link" : "https://stackoverflow.com/questions/79740213/kafka-manual-commit-scenario-springboot",
    "content_license" : "CC BY-SA 4.0"
  },
  "answers" : [ {
    "answer_id" : 79740387,
    "question_id" : 79740213,
    "body" : "<p>If you're using Spring Kafka containers (e.g. @KafkaListener as a basis for your stuff in Spring Boot) - you don't need manual acknowledgement.</p>\n<p>Just set AckMode in your Container Properties to RECORD - and be happy.<br />\nContainer would do that lower-level Kafka API Consumer manual ack for you.</p>\n<p>P.S. On a side note - the default 5 sec is way, way, WAY too long in a nowadays nanosecond-fine world. For one, the default for &quot;native&quot; Kafka API is 100ms, to my recollection.<br />\nI don't even know what these Spring Kafka guys were thinking when they set it (although it goes along with a messy quality of the package itself).</p>\n",
    "score" : 1,
    "is_accepted" : true,
    "owner" : {
      "account_id" : 7692774,
      "reputation" : 1282,
      "user_id" : 5828464,
      "user_type" : "registered",
      "profile_image" : "https://www.gravatar.com/avatar/de54db63fab1e3c3d00d0dbef1a0a0f1?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name" : "Yuri G",
      "link" : "https://stackoverflow.com/users/5828464/yuri-g"
    },
    "creation_date" : 1755636018,
    "last_activity_date" : 1755636018,
    "content_license" : "CC BY-SA 4.0"
  }, {
    "answer_id" : 79747274,
    "question_id" : 79740213,
    "body" : "<p>You are on the right track here but with a slight misunderstanding. Not sure how you think it would increase it by 10 fold..</p>\n<blockquote>\n<p>I was thinking of changing the ack mode to manual and commit after every batch no matter how long it takes (of course less than the max.poll.interval) but this will increase the # of offset commits by 10 fold</p>\n</blockquote>\n<p>You don't have to commit all the messages in the batch. you only need to commit the last offset of the batch. Which means 1 commit per batch. If the way you process messages makes it lose the order, then you could retrieve the &quot;max&quot; offset of a batch and commit that at the end of batch processing. This approach is what I have used and works well.</p>\n",
    "score" : 1,
    "is_accepted" : false,
    "owner" : {
      "account_id" : 10705914,
      "reputation" : 24,
      "user_id" : 7879727,
      "user_type" : "registered",
      "profile_image" : "https://www.gravatar.com/avatar/942002e95e1bbae4db6bc38d6f066d66?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name" : "Koditect",
      "link" : "https://stackoverflow.com/users/7879727/koditect"
    },
    "creation_date" : 1756237834,
    "last_activity_date" : 1756237834,
    "content_license" : "CC BY-SA 4.0"
  } ],
  "question_comments" : [ ],
  "answer_comments" : {
    "79740387" : [ {
      "comment_id" : 140679887,
      "post_id" : 79740387,
      "body" : "Yeah, every single one - and what&#39;s wrong with it? &quot;Too many commits&quot; is not a thing, I absolutely can&#39;t get what&#39;s your concern with it? You have any evidence it impacts anything? Especially, it impacts anything more than potential re-consumption of the whole batch 100 or 200 batch given your processing time per message is quite substantial (whole 300ms)?",
      "score" : 0,
      "owner" : {
        "account_id" : 7692774,
        "reputation" : 1282,
        "user_id" : 5828464,
        "user_type" : "registered",
        "profile_image" : "https://www.gravatar.com/avatar/de54db63fab1e3c3d00d0dbef1a0a0f1?s=256&d=identicon&r=PG&f=y&so-version=2",
        "display_name" : "Yuri G",
        "link" : "https://stackoverflow.com/users/5828464/yuri-g"
      },
      "creation_date" : 1755709482,
      "content_license" : "CC BY-SA 4.0"
    }, {
      "comment_id" : 140677901,
      "post_id" : 79740387,
      "body" : "RECORD will commit on every record no? What i am trying to do is poll for batch of messages (100 or 200) and then commit them at regular intervals. I was thinking of just committing them at the end of every batch manually but that will be too many commits to Kafka compared to now once every 5 seconds. Already we are seeing some issues with Kafka stability where 6  pods are polling messages in batches of 100 which i feel is not a lot so wondering if there would be any performance impact of doing this for every batch immediatedly.",
      "score" : 0,
      "owner" : {
        "account_id" : 18044566,
        "reputation" : 325,
        "user_id" : 13115896,
        "user_type" : "registered",
        "profile_image" : "https://www.gravatar.com/avatar/6d35ea24f31161f110fed75caf9d2d04?s=256&d=identicon&r=PG&f=y&so-version=2",
        "display_name" : "sg2000",
        "link" : "https://stackoverflow.com/users/13115896/sg2000"
      },
      "creation_date" : 1755652927,
      "content_license" : "CC BY-SA 4.0"
    } ]
  }
}