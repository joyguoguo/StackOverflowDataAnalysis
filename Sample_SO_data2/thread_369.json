{
  "question" : {
    "question_id" : 79818098,
    "title" : "Flink Job Manager Direct Buffer Memory gets exhausted when checkpointing enabled",
    "body" : "<p>Issue:</p>\n<ul>\n<li><p>Flink application throws Thread 'jobmanager-io-thread-25' produced an uncaught exception. java.lang.OutOfMemoryError: Direct buffer memory and terminates after running for 2-3 days.</p>\n</li>\n<li><p>No matter how much direct buffer memory is increased it gets exhausted over time (it just stays longer but eventually terminates), tried max of 16GB till now.</p>\n</li>\n<li><p>When checkpointing is disabled, buffer memory doesn't grows and application works well.</p>\n</li>\n<li><p>Tried all sorts of optimizations or tuning suggested in documentation such as incrementalCheckpointing, stateChangelog, directBuffer increase etc.. but none worked.</p>\n</li>\n</ul>\n<p>Stack Trace:</p>\n<pre><code>2025-11-09 17:06:56,442 ERROR org.apache.flink.util.FatalExitExceptionHandler              [] - FATAL: Thread 'jobmanager-io-thread-25' produced an uncaught exception. Stopping the process...\njava.lang.OutOfMemoryError: Direct buffer memory\n    at java.base/java.nio.Bits.reserveMemory(Bits.java:175)\n    at java.base/java.nio.DirectByteBuffer.&lt;init&gt;(DirectByteBuffer.java:118)\n    at java.base/java.nio.ByteBuffer.allocateDirect(ByteBuffer.java:317)\n    at java.base/sun.nio.ch.Util.getTemporaryDirectBuffer(Util.java:242)\n    at java.base/sun.nio.ch.IOUtil.write(IOUtil.java:71)\n    at java.base/sun.nio.ch.FileChannelImpl.write(FileChannelImpl.java:280)\n    at java.base/java.nio.channels.Channels.writeFullyImpl(Channels.java:74)\n    at java.base/java.nio.channels.Channels.writeFully(Channels.java:97)\n    at java.base/java.nio.channels.Channels$1.write(Channels.java:172)\n    at org.apache.flink.core.fs.OffsetAwareOutputStream.write(OffsetAwareOutputStream.java:48)\n    at org.apache.flink.core.fs.RefCountedFileWithStream.write(RefCountedFileWithStream.java:54)\n    at org.apache.flink.core.fs.RefCountedBufferingFileStream.write(RefCountedBufferingFileStream.java:88)\n    at org.apache.flink.fs.s3.common.writer.S3RecoverableFsDataOutputStream.write(S3RecoverableFsDataOutputStream.java:112)\n    at org.apache.flink.runtime.state.filesystem.FsCheckpointMetadataOutputStream.write(FsCheckpointMetadataOutputStream.java:78)\n    at java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)\n    at java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:108)\n    at org.apache.flink.runtime.checkpoint.metadata.MetadataV2V3SerializerBase.serializeStreamStateHandle(MetadataV2V3SerializerBase.java:758)\n    at org.apache.flink.runtime.checkpoint.metadata.MetadataV3Serializer.serializeStreamStateHandle(MetadataV3Serializer.java:264)\n    at org.apache.flink.runtime.checkpoint.metadata.MetadataV3Serializer.serializeOperatorState(MetadataV3Serializer.java:109)\n    at org.apache.flink.runtime.checkpoint.metadata.MetadataV2V3SerializerBase.serializeMetadata(MetadataV2V3SerializerBase.java:165)\n    at org.apache.flink.runtime.checkpoint.metadata.MetadataV3Serializer.serialize(MetadataV3Serializer.java:83)\n    at org.apache.flink.runtime.checkpoint.metadata.MetadataV4Serializer.serialize(MetadataV4Serializer.java:56)\n    at org.apache.flink.runtime.checkpoint.Checkpoints.storeCheckpointMetadata(Checkpoints.java:101)\n    at org.apache.flink.runtime.checkpoint.Checkpoints.storeCheckpointMetadata(Checkpoints.java:88)\n    at org.apache.flink.runtime.checkpoint.Checkpoints.storeCheckpointMetadata(Checkpoints.java:83)\n    at org.apache.flink.runtime.checkpoint.PendingCheckpoint.finalizeCheckpoint(PendingCheckpoint.java:339)\n    at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.finalizeCheckpoint(CheckpointCoordinator.java:1624)\n    at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.completePendingCheckpoint(CheckpointCoordinator.java:1518)\n    at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.receiveAcknowledgeMessage(CheckpointCoordinator.java:1410)\n    at org.apache.flink.runtime.scheduler.ExecutionGraphHandler.lambda$acknowledgeCheckpoint$2(ExecutionGraphHandler.java:109)\n    at org.apache.flink.runtime.scheduler.ExecutionGraphHandler.lambda$processCheckpointCoordinatorMessage$4(ExecutionGraphHandler.java:139)\n</code></pre>\n<p>Checkpointing Config:\n<a href=\"https://i.sstatic.net/0kvmaaVC.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/0kvmaaVC.png\" alt=\"Checkpoint Config\" /></a></p>\n<p>Application Structure (High Level):</p>\n<ul>\n<li>Application is setup on Flink on AWS EMR. Using Flink 1.20, on EMR 7.10.0</li>\n<li>Has 2 S3 Filesources reads parquet files, both unbounded, continuous monitoring with discoveryInterval of 1min, new files get added every min in new paths yyyy/mm/dd/mi/files</li>\n<li>process and create objects from both streams</li>\n<li>Join them using keyed-coprocess function, with TTL of 30mins</li>\n<li>Perform some map, filter operations on joined stream</li>\n<li>Sink result stream to S3 again as parquet files.</li>\n</ul>\n<p>Help:</p>\n<ul>\n<li>Please help me understand why would checkpointing consume such large buffers gradually? Even then why aren't they getting released?</li>\n<li>What exactly is getting stored in this buffer memory by checkpoint co-ordinator?</li>\n<li>How can i handle this issue or apply tuning so that this wouldn't occur?</li>\n<li>What can be my next steps of action to try out and resolve this ?</li>\n</ul>\n<p>Direct Memory vs Checkpoint Size :\n<a href=\"https://i.sstatic.net/2fg7jmoM.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/2fg7jmoM.png\" alt=\"Direct Memory vs Checkpoint Size\" /></a></p>\n<p>Native Memory Leak - Stack Trace :\n<a href=\"https://i.sstatic.net/zV068Z5n.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/zV068Z5n.png\" alt=\"Native Memory Leak - Stack Trace\" /></a></p>\n",
    "tags" : [ "java", "apache-flink", "flink-streaming", "amazon-emr", "flink-checkpoint" ],
    "owner" : {
      "account_id" : 15862641,
      "reputation" : 1514,
      "user_id" : 11994158,
      "user_type" : "registered",
      "profile_image" : "https://lh3.googleusercontent.com/-9m6ElB_KVGY/AAAAAAAAAAI/AAAAAAAAAAc/gceWZ0TdWkw/s256-rj/photo.jpg",
      "display_name" : "Strange",
      "link" : "https://stackoverflow.com/users/11994158/strange"
    },
    "is_answered" : false,
    "view_count" : 157,
    "answer_count" : 1,
    "score" : -3,
    "last_activity_date" : 1763459908,
    "creation_date" : 1762971242,
    "link" : "https://stackoverflow.com/questions/79818098/flink-job-manager-direct-buffer-memory-gets-exhausted-when-checkpointing-enabled",
    "content_license" : "CC BY-SA 4.0"
  },
  "answers" : [ {
    "answer_id" : 79822476,
    "question_id" : 79818098,
    "body" : "<p>This feels an awful lot like a memory leak, but in my experience, that's not always the case. I'll try to throw out a few things to try based on the configurations that you provided. I think the key culprits here though may be related to one (or more) of: state changelog, checkpoint retention, or possibly some other configurations.</p>\n<p>I'll provide a few suggestions (feel free to try one or more):</p>\n<p><strong>Disabling State Changelog</strong><br />\nThis is a recommendation that likely could explain some of the situation as the use of changelog can add some additional memory pressure (it functions as a buffer for those changes as mentioned <a href=\"https://flink.apache.org/2022/05/30/improving-speed-and-stability-of-checkpointing-with-generic-log-based-incremental-checkpoints/?utm_source=chatgpt.com#high-level-overview\" rel=\"nofollow noreferrer\">in this older blog post</a>). I'd suspect that using it in conjunction with RocksDB could likely impact memory utilization:</p>\n<pre><code>state.backend.changelog.enabled: false\n</code></pre>\n<p><strong>Limiting Checkpoint Retention</strong><br />\nCurrently your job has checkpoint retention enabled which is fine, however you may want to consider limiting it to a specific number of those to retain (otherwise things could balloon) as well as cleaning them up to ensure too many don't stick around:</p>\n<pre><code>state.checkpoints.num-retained: 5\nstate.checkpoints.externalized.enable: true\nstate.checkpoints.externalized.delete-on-cancellation: true\n</code></pre>\n<p><strong>Gather Metrics</strong><br />\nOne thing that I would highly suggest as you monitor the job and these changes would be to implement and monitor some of <a href=\"https://nightlies.apache.org/flink/flink-docs-master/docs/ops/metrics/#system-metrics\" rel=\"nofollow noreferrer\">the built-in metrics</a> that Flink provides out of the box with regards to memory/JVM. Using these with some type of visualization tool (e.g., Prometheus, Grafana, etc.) would allow you to easily monitor things like the JobManager, Changelog, Checkpointing, etc.</p>\n<p>Definitely check out:</p>\n<ul>\n<li><p>Any/all memory-related metrics (or just JobManager in general too)</p>\n</li>\n<li><p>Any/all changelog-related metrics</p>\n</li>\n<li><p>Checkpointing sizes/durations</p>\n</li>\n<li><p>RocksDB metrics (these need to be enabled separately)</p>\n</li>\n</ul>\n<p><strong>Questions</strong><br />\nAs far as your questions go, I'll try to give a few possible explanations to help on those fronts too:</p>\n<blockquote>\n<p>Please help me understand why would checkpointing consume such large buffers gradually? Even then why aren't they getting released?</p>\n</blockquote>\n<p>tl;dr: there's a lot more moving pieces to the puzzle when checkpointing is enabled that can impact memory pressure (even gradually) in a consistently flowing system</p>\n<p>So there's a lot of things at play when checkpointing is enabled (vs. why things are rainbows and butterflies when it's disabled). Checkpointing is going to bring RocksDB into the picture which has a native memory impact with each checkpoint this in conjunction with the changelog could apply a quite a bit more pressure for the changelog-related segments as well.</p>\n<p>Many of these things can stick around much longer than expected if data is continually flowing at scale into the system and may require tuning. RocksDB, for example, does a decent job at cleanup during its compaction process, however if the job is busy 24/7 it may never have the opportunity to do so, especially with all of the checkpointing operations and state being interacted with.</p>\n<blockquote>\n<p>What exactly is getting stored in this buffer memory by checkpoint co-ordinator?</p>\n</blockquote>\n<p>Obviously there's things like just direct memory like Netty, checkpoint buffers for your filesystem, <em>tons</em> of RocksDB related things (e.g, cache, tables, changelog, etc.), and the changelog has its own series of content.</p>\n<blockquote>\n<p>How can i handle this issue or apply tuning so that this wouldn't occur?\nWhat can be my next steps of action to try out and resolve this?</p>\n</blockquote>\n<p>Combining these two as this is already <em>way</em> too long, but hopefully some of the configurations that I provided above can help relieve the issue. Checkpointing and OOM type errors can be really nasty to troubleshoot, even when you know the ins/outs of a given job, but I'll keep my fingers crossed for you.</p>\n",
    "score" : 0,
    "is_accepted" : false,
    "owner" : {
      "account_id" : 268602,
      "reputation" : 76920,
      "user_id" : 557445,
      "user_type" : "registered",
      "accept_rate" : 100,
      "profile_image" : "https://i.sstatic.net/M6iZ3z7p.jpg?s=256",
      "display_name" : "Rion Williams",
      "link" : "https://stackoverflow.com/users/557445/rion-williams"
    },
    "creation_date" : 1763393628,
    "last_activity_date" : 1763403801,
    "content_license" : "CC BY-SA 4.0"
  } ],
  "question_comments" : [ {
    "comment_id" : 140858708,
    "post_id" : 79818098,
    "body" : "Sorry to bother but difficult to tell without the code",
    "score" : 0,
    "owner" : {
      "account_id" : 173785,
      "reputation" : 1046,
      "user_id" : 402723,
      "user_type" : "registered",
      "profile_image" : "https://i.sstatic.net/Q8TPb.jpg?s=256",
      "display_name" : "Raul Lapeira Herrero",
      "link" : "https://stackoverflow.com/users/402723/raul-lapeira-herrero"
    },
    "creation_date" : 1763397844,
    "content_license" : "CC BY-SA 4.0"
  } ],
  "answer_comments" : {
    "79822476" : [ {
      "comment_id" : 140859986,
      "post_id" : 79822476,
      "body" : "I have added Direct Memory vs Checkpoint Size graph and async-profiler native memory leak flame graph.  Used below commands to get leak flame-graph   -  ./asprof -e nativemem -d 600 --live -f /tmp/native_mem.jfr 1600550   -  ./jfrconv --total --nativemem --leak /tmp/native_mem.jfr app-leak.html",
      "score" : 0,
      "owner" : {
        "account_id" : 15862641,
        "reputation" : 1514,
        "user_id" : 11994158,
        "user_type" : "registered",
        "profile_image" : "https://lh3.googleusercontent.com/-9m6ElB_KVGY/AAAAAAAAAAI/AAAAAAAAAAc/gceWZ0TdWkw/s256-rj/photo.jpg",
        "display_name" : "Strange",
        "link" : "https://stackoverflow.com/users/11994158/strange"
      },
      "creation_date" : 1763459977,
      "content_license" : "CC BY-SA 4.0"
    }, {
      "comment_id" : 140859981,
      "post_id" : 79822476,
      "body" : "I do have metrics being flushed to cloudwatch with statsd, so i can help providing with any metrics required for more information.",
      "score" : 0,
      "owner" : {
        "account_id" : 15862641,
        "reputation" : 1514,
        "user_id" : 11994158,
        "user_type" : "registered",
        "profile_image" : "https://lh3.googleusercontent.com/-9m6ElB_KVGY/AAAAAAAAAAI/AAAAAAAAAAc/gceWZ0TdWkw/s256-rj/photo.jpg",
        "display_name" : "Strange",
        "link" : "https://stackoverflow.com/users/11994158/strange"
      },
      "creation_date" : 1763459783,
      "content_license" : "CC BY-SA 4.0"
    }, {
      "comment_id" : 140859968,
      "post_id" : 79822476,
      "body" : "- Changelog enabled or disabled, scenario is same.    - I did not find the exact configuration you provided for checkpointing retention from <a href=\"https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/deployment/config/\" rel=\"nofollow noreferrer\">documentation</a>, but the default retention count is 1 and i need retain_on_cancellation, to basically start up from previous checkpoint.    - Apart from above, i&#39;ve also removed all intermediate operators like joins, flatmaps, maps, filters etc and had only filesource + no-op sink, yet i saw buffer memory increase. so it shouldn&#39;t be because of rocksdb",
      "score" : 0,
      "owner" : {
        "account_id" : 15862641,
        "reputation" : 1514,
        "user_id" : 11994158,
        "user_type" : "registered",
        "profile_image" : "https://lh3.googleusercontent.com/-9m6ElB_KVGY/AAAAAAAAAAI/AAAAAAAAAAc/gceWZ0TdWkw/s256-rj/photo.jpg",
        "display_name" : "Strange",
        "link" : "https://stackoverflow.com/users/11994158/strange"
      },
      "creation_date" : 1763459247,
      "content_license" : "CC BY-SA 4.0"
    } ]
  }
}