{
  "question" : {
    "question_id" : 79677459,
    "title" : "Files stuck as .inprogress, not rolling into final Parquet files",
    "body" : "<p>I'm running a Flink streaming job using the Table API, which reads from Kafka and writes to S3 (for now I'm using a local path to simulate S3). The job uses a filesystem connector to write data in Parquet format.</p>\n<p>Here is a simplified version of my code:</p>\n<pre><code>tableEnv.executeSql(\n    &quot;CREATE TABLE logs_source (&quot; +\n    &quot; `timestamp` STRING,&quot; +\n    &quot; user_id INT,&quot; +\n    &quot; message STRING&quot; +\n    &quot;) WITH (&quot; +\n    &quot; 'connector' = 'kafka',&quot; +\n    &quot; 'topic' = 'kafka-5s',&quot; +\n    &quot; 'properties.bootstrap.servers' = 'localhost:9092',&quot; +\n    &quot; 'properties.group.id' = 'kafka-5s-1',&quot; +\n    &quot; 'scan.startup.mode' = 'earliest-offset',&quot; +\n    &quot; 'format' = 'json',&quot; +\n    &quot; 'json.timestamp-format.standard' = 'ISO-8601'&quot; +\n    &quot;)&quot;\n);\n\ntableEnv.executeSql(\n    &quot;CREATE TABLE s3_sink (&quot; +\n    &quot; `timestamp` STRING,&quot; +\n    &quot; user_id INT,&quot; +\n    &quot; message STRING&quot; +\n    &quot;) WITH (&quot; +\n    &quot; 'connector' = 'filesystem',&quot; +\n    &quot; 'path' = 'file:///D://flink-data/data',&quot; +\n    &quot; 'format' = 'parquet',&quot; +\n    &quot; 'parquet.compression' = 'SNAPPY',&quot; +\n    &quot; 'sink.rolling-policy.rollover-interval' = '1000',&quot; +\n    &quot; 'sink.rolling-policy.file-size' = '10',&quot; +\n    &quot; 'sink.rolling-policy.inactivity-interval' = '1000'&quot; +\n    &quot;)&quot;\n);\n\ntableEnv.executeSql(&quot;INSERT INTO s3_sink SELECT `timestamp`, user_id, message FROM logs_source&quot;);\n</code></pre>\n<p><strong>Problem:</strong></p>\n<p>Flink does write .inprogress files in the target directory (e.g., part-0-0.inprogress), but these files never finalize into proper .parquet files.</p>\n<p><strong>What I’ve tried:</strong></p>\n<ul>\n<li><p>Setting short rollover-interval, file-size, and inactivity-interval values (1000 ms and 10 bytes) to trigger quick roll.</p>\n</li>\n<li><p>Enabled checkpointing with:</p>\n</li>\n</ul>\n<pre><code>        Configuration config = GlobalConfiguration.loadConfiguration(\n                &quot;D:\\\\Gitlab\\\\java-project\\\\src\\\\main\\\\resources\\\\&quot;\n        );\n\n        config.set(CheckpointingOptions.CHECKPOINT_STORAGE, &quot;filesystem&quot;);\n        config.set(CheckpointingOptions.CHECKPOINTS_DIRECTORY, &quot;file:///D://flink-data/checkpoints/&quot;);\n\n        // Print out S3 credentials for verification\n        System.out.println(&quot;fs.s3a.access.key: &quot; + config.getString(&quot;fs.s3a.access.key&quot;, &quot;NOT SET&quot;));\n        System.out.println(&quot;fs.s3a.secret.key: &quot; + config.getString(&quot;fs.s3a.secret.key&quot;, &quot;NOT SET&quot;));\n        System.out.println(&quot;fs.s3a.endpoint: &quot; + config.getString(&quot;fs.s3a.endpoint&quot;, &quot;NOT SET&quot;));\n\n        // Initialize the file system with the config\n        FileSystem.initialize(config, null);\n\n        // Create a streaming execution environment with checkpoints enabled\n        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(config);\n        env.enableCheckpointing(10000);\n        env.getCheckpointConfig().setMinPauseBetweenCheckpoints(5000);\n        env.getCheckpointConfig().setCheckpointTimeout(60000);\n        env.getCheckpointConfig().setMaxConcurrentCheckpoints(1);\n        env.configure(config);\n\n        // Create a TableEnvironment for streaming queries\n        StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env);\n</code></pre>\n<ul>\n<li><p>Verified that the Kafka topic (kafka-5s) is producing data continuously.</p>\n</li>\n<li><p>Used local path with file:/// scheme for easier testing before moving to actual S3.</p>\n</li>\n</ul>\n",
    "tags" : [ "java", "apache-flink", "parquet", "flink-sql" ],
    "owner" : {
      "account_id" : 21965267,
      "reputation" : 41,
      "user_id" : 16242616,
      "user_type" : "registered",
      "profile_image" : "https://www.gravatar.com/avatar/76e02cbbd7f1b6548d7445a2abcc6f81?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name" : "Tuan Duy",
      "link" : "https://stackoverflow.com/users/16242616/tuan-duy"
    },
    "is_answered" : false,
    "view_count" : 63,
    "answer_count" : 1,
    "score" : 2,
    "last_activity_date" : 1750870589,
    "creation_date" : 1750763081,
    "link" : "https://stackoverflow.com/questions/79677459/files-stuck-as-inprogress-not-rolling-into-final-parquet-files",
    "content_license" : "CC BY-SA 4.0"
  },
  "answers" : [ {
    "answer_id" : 79679402,
    "question_id" : 79677459,
    "body" : "<p>You're writing to Parquet, which is a bulk format. As such, files are only rolled on checkpoints, so you have to use CheckpointRollingPolicy. See <a href=\"https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/connectors/datastream/filesystem/#bulk-encoded-formats\" rel=\"nofollow noreferrer\">bulk encoded formats</a>, where it says</p>\n<blockquote>\n<p><strong>Important</strong> Bulk Formats can only have a rolling policy that extends the <code>CheckpointRollingPolicy</code>. The latter rolls on every checkpoint. A policy can roll additionally based on size or processing time.</p>\n</blockquote>\n",
    "score" : 0,
    "is_accepted" : false,
    "owner" : {
      "account_id" : 82425,
      "reputation" : 9560,
      "user_id" : 231762,
      "user_type" : "registered",
      "profile_image" : "https://www.gravatar.com/avatar/b5d195db362831baf779f0e4b491c68a?s=256&d=identicon&r=PG",
      "display_name" : "kkrugler",
      "link" : "https://stackoverflow.com/users/231762/kkrugler"
    },
    "creation_date" : 1750870589,
    "last_activity_date" : 1750870589,
    "content_license" : "CC BY-SA 4.0"
  } ],
  "question_comments" : [ {
    "comment_id" : 140538868,
    "post_id" : 79677459,
    "body" : "I think the problem is it cannot sink to parquet bc the temp file size is always 0KB :))",
    "score" : 1,
    "owner" : {
      "account_id" : 21965267,
      "reputation" : 41,
      "user_id" : 16242616,
      "user_type" : "registered",
      "profile_image" : "https://www.gravatar.com/avatar/76e02cbbd7f1b6548d7445a2abcc6f81?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name" : "Tuan Duy",
      "link" : "https://stackoverflow.com/users/16242616/tuan-duy"
    },
    "creation_date" : 1750782197,
    "content_license" : "CC BY-SA 4.0"
  }, {
    "comment_id" : 140538858,
    "post_id" : 79677459,
    "body" : "I think it is not the point cuz when I sink as json, everything is still smooth :&lt;",
    "score" : 0,
    "owner" : {
      "account_id" : 21965267,
      "reputation" : 41,
      "user_id" : 16242616,
      "user_type" : "registered",
      "profile_image" : "https://www.gravatar.com/avatar/76e02cbbd7f1b6548d7445a2abcc6f81?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name" : "Tuan Duy",
      "link" : "https://stackoverflow.com/users/16242616/tuan-duy"
    },
    "creation_date" : 1750781938,
    "content_license" : "CC BY-SA 4.0"
  }, {
    "comment_id" : 140538850,
    "post_id" : 79677459,
    "body" : "<a href=\"https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/fault-tolerance/checkpointing/\" rel=\"nofollow noreferrer\">nightlies.apache.org/flink/flink-docs-master/docs/dev/&hellip;</a> hope it helps anyway",
    "score" : 0,
    "owner" : {
      "account_id" : 2465829,
      "reputation" : 13627,
      "user_id" : 2148953,
      "user_type" : "registered",
      "accept_rate" : 96,
      "profile_image" : "https://i.sstatic.net/DVHoP84E.jpg?s=256",
      "display_name" : "aran",
      "link" : "https://stackoverflow.com/users/2148953/aran"
    },
    "creation_date" : 1750781842,
    "content_license" : "CC BY-SA 4.0"
  }, {
    "comment_id" : 140538841,
    "post_id" : 79677459,
    "body" : "Make Sure the TableEnvironment is using the same StreamExecutionEnvironment. If you&#39;re creating a StreamExecutionEnvironment with checkpointing, but not passing it to the StreamTableEnvironment, checkpoints won’t apply to your job.  <code>EnvironmentSettings settings = EnvironmentSettings.newInstance()     .inStreamingMode()     .build();</code>  <code>StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env, settings);</code> Make sure env here is the one with checkpointing enabled.",
    "score" : 0,
    "owner" : {
      "account_id" : 2465829,
      "reputation" : 13627,
      "user_id" : 2148953,
      "user_type" : "registered",
      "accept_rate" : 96,
      "profile_image" : "https://i.sstatic.net/DVHoP84E.jpg?s=256",
      "display_name" : "aran",
      "link" : "https://stackoverflow.com/users/2148953/aran"
    },
    "creation_date" : 1750781616,
    "content_license" : "CC BY-SA 4.0"
  }, {
    "comment_id" : 140538834,
    "post_id" : 79677459,
    "body" : "still not work. inprogress files are still produced, the checkpoint folder is created but no chk-0,1,2... folder appeared, and the temp data file is not rolled.",
    "score" : 0,
    "owner" : {
      "account_id" : 21965267,
      "reputation" : 41,
      "user_id" : 16242616,
      "user_type" : "registered",
      "profile_image" : "https://www.gravatar.com/avatar/76e02cbbd7f1b6548d7445a2abcc6f81?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name" : "Tuan Duy",
      "link" : "https://stackoverflow.com/users/16242616/tuan-duy"
    },
    "creation_date" : 1750781473,
    "content_license" : "CC BY-SA 4.0"
  } ],
  "answer_comments" : { }
}